{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed969536-d16d-479c-a716-6fc2c2c9e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "874de457-1a20-4399-9af2-2df745f3f6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['core-ontology.ipynb',\n",
       " '.ruff_cache',\n",
       " 'schema_output.jsonld',\n",
       " 'schema.txt',\n",
       " 'mypy.ini',\n",
       " 'schema.ttl',\n",
       " '.mypy_cache',\n",
       " 'output_ontology',\n",
       " '.ipynb_checkpoints',\n",
       " 'schema.jsonld']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6efd8f35-9498-4de6-8e58-21c6cd7d13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ad193dfc-7f26-47d3-83fb-c4a015cebe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_filepath = \"schema.ttl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98f6e3-c329-46c3-93ab-f147806176ee",
   "metadata": {},
   "source": [
    "# Step 1: Parsing from schema.org ttl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d578d-27b4-47f1-9b18-8fd5bf4fa245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8368c4dd-a07c-4f78-95fa-a0c71f2d7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the input schema file path (assuming schema.txt is in the same directory)\n",
    "# In a real application, this path would be configurable.\n",
    "SCHEMA_FILE = \"schema.txt\"\n",
    "SCHEMA_FORMAT = \"turtle\" # Explicitly state the format of schema.txt\n",
    "\n",
    "# Define output path for JSON-LD (optional, for inspection)\n",
    "JSON_LD_OUTPUT_FILE = \"schema_output.jsonld\"\n",
    "\n",
    "def parse_schema_to_graph(file_path: str, file_format: str) -> rdflib.Graph | None:\n",
    "    \"\"\"\n",
    "    Parses an RDF schema file into an rdflib Graph object.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the schema file.\n",
    "        file_format: The format of the RDF file (e.g., 'turtle', 'xml', 'json-ld').\n",
    "\n",
    "    Returns:\n",
    "        An rdflib.Graph object or None if parsing fails.\n",
    "    \"\"\"\n",
    "    g = rdflib.Graph()\n",
    "    try:\n",
    "        logging.info(f\"Attempting to parse schema file: {file_path} (format: {file_format})\")\n",
    "        g.parse(source=file_path, format=file_format)\n",
    "        logging.info(f\"Successfully parsed {len(g)} triples.\")\n",
    "        return g\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Schema file not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing schema file '{file_path}' with format '{file_format}': {e}\")\n",
    "        return None\n",
    "\n",
    "def serialize_graph_to_jsonld(graph: rdflib.Graph, output_file: str) -> bool:\n",
    "    \"\"\"\n",
    "    Serializes an rdflib Graph object to a JSON-LD file.\n",
    "\n",
    "    Args:\n",
    "        graph: The rdflib.Graph object.\n",
    "        output_file: Path to save the JSON-LD output file.\n",
    "\n",
    "    Returns:\n",
    "        True if serialization was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        logging.error(\"Cannot serialize: Graph object is None.\")\n",
    "        return False\n",
    "    try:\n",
    "        logging.info(f\"Attempting to serialize graph to JSON-LD: {output_file}\")\n",
    "        # Common context for schema.org can help make JSON-LD more readable\n",
    "        # Using a standard context URL\n",
    "        context = {\n",
    "            \"@vocab\": \"https://schema.org/\"\n",
    "            # Add other prefixes if needed, though @vocab covers schema.org terms\n",
    "        }\n",
    "        # Serialize to JSON-LD format\n",
    "        # Note: rdflib's json-ld serialization might produce a list of objects\n",
    "        json_ld_data = graph.serialize(format='json-ld', context=context, indent=2)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(json_ld_data)\n",
    "        logging.info(f\"Successfully serialized graph to {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error serializing graph to JSON-LD: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0b67fa75-681c-402d-ac70-5a998d667164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Attempting to serialize graph to JSON-LD: schema_output.jsonld\n",
      "INFO:root:Successfully serialized graph to schema_output.jsonld\n",
      "INFO:root:Graph parsed. Ready for Step 2: Schema Analysis (in next code chunk).\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution block for this step ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Parse the schema\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "\n",
    "    if schema_graph:\n",
    "        # Step 2 (as requested): Serialize to JSON-LD for inspection\n",
    "        serialize_graph_to_jsonld(schema_graph, JSON_LD_OUTPUT_FILE)\n",
    "\n",
    "        # Next steps (to be implemented in subsequent chunks) would involve\n",
    "        # analyzing 'schema_graph' to extract class/property info\n",
    "        # and then generating Pydantic models.\n",
    "        logging.info(\"Graph parsed. Ready for Step 2: Schema Analysis (in next code chunk).\")\n",
    "    else:\n",
    "        logging.error(\"Failed to parse schema graph. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d3c2f040-24ff-443b-a78c-f8c13b4380f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Na483d00186fa457fa32657c5fb34c138 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec7fdf-48a3-47c6-b906-7d882a206b48",
   "metadata": {},
   "source": [
    "# Step 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8a6eeb20-8748-4b1f-b59b-dce4098fbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD # Common RDF/RDFS/OWL namespaces\n",
    "from typing import List, Set, Dict, Optional, NamedTuple\n",
    "import logging\n",
    "\n",
    "# Assuming the 'parse_schema_to_graph' function from Chunk 1 exists\n",
    "# and 'schema_graph' is the rdflib.Graph object returned by it.\n",
    "\n",
    "# Define the schema.org namespace\n",
    "SCHEMA = rdflib.Namespace(\"https://schema.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9ae6eb5f-a7f7-4960-9b15-d491b5efffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple structures to hold extracted info\n",
    "class PropertyInfo(NamedTuple):\n",
    "    uri: rdflib.URIRef\n",
    "    label: Optional[str]\n",
    "    comment: Optional[str]\n",
    "    domains: Set[rdflib.URIRef] # Classes where this property applies\n",
    "    ranges: Set[rdflib.URIRef] # Expected types for this property's value\n",
    "\n",
    "class ClassInfo(NamedTuple):\n",
    "    uri: rdflib.URIRef\n",
    "    label: Optional[str]\n",
    "    comment: Optional[str]\n",
    "    superclasses: Set[rdflib.URIRef] # Direct parent classes\n",
    "    properties: Set[rdflib.URIRef] # Properties associated via domainIncludes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "28017987-ccf8-4120-bebf-abfd74b885fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(graph: rdflib.Graph, subject: rdflib.URIRef) -> Optional[str]:\n",
    "    \"\"\"Gets the rdfs:label for a subject.\"\"\"\n",
    "    label = graph.value(subject=subject, predicate=RDFS.label)\n",
    "    return str(label) if label else None\n",
    "\n",
    "def get_comment(graph: rdflib.Graph, subject: rdflib.URIRef) -> Optional[str]:\n",
    "    \"\"\"Gets the rdfs:comment for a subject.\"\"\"\n",
    "    comment = graph.value(subject=subject, predicate=RDFS.comment)\n",
    "    return str(comment) if comment else None\n",
    "\n",
    "def find_schema_classes(graph: rdflib.Graph) -> Set[rdflib.URIRef]:\n",
    "    \"\"\"Finds all subjects defined as RDFS Classes within the schema.org namespace.\"\"\"\n",
    "    classes = set()\n",
    "    # Find things explicitly declared as rdfs:Class or owl:Class\n",
    "    for class_type in [RDFS.Class, OWL.Class]:\n",
    "        for subject in graph.subjects(predicate=RDF.type, object=class_type):\n",
    "            # Filter to include only those within the schema.org namespace\n",
    "            if str(subject).startswith(str(SCHEMA)):\n",
    "                 # Also check if it's a schema.org DataType, treat those differently later\n",
    "                 is_datatype = (subject, RDF.type, SCHEMA.DataType) in graph\n",
    "                 if not is_datatype:\n",
    "                    classes.add(subject)\n",
    "    # Schema.org also defines types like schema:Person without explicitly stating\n",
    "    # rdf:type rdfs:Class in all serializations, but implies they are classes\n",
    "    # by using them in domain/range or subClassOf. A more robust approach\n",
    "    # might involve looking for usage in rdfs:subClassOf, :domainIncludes, :rangeIncludes\n",
    "    # For now, primarily rely on explicit declaration if present.\n",
    "    logging.info(f\"Found {len(classes)} potential schema.org classes.\")\n",
    "    return classes\n",
    "\n",
    "def find_schema_properties(graph: rdflib.Graph) -> Set[rdflib.URIRef]:\n",
    "    \"\"\"Finds all subjects defined as RDF Properties within the schema.org namespace.\"\"\"\n",
    "    properties = set()\n",
    "    for subject in graph.subjects(predicate=RDF.type, object=RDF.Property):\n",
    "         if str(subject).startswith(str(SCHEMA)):\n",
    "             properties.add(subject)\n",
    "    logging.info(f\"Found {len(properties)} potential schema.org properties.\")\n",
    "    return properties\n",
    "\n",
    "def get_property_details(graph: rdflib.Graph, prop_uri: rdflib.URIRef) -> PropertyInfo:\n",
    "    \"\"\"Extracts details for a given property URI.\"\"\"\n",
    "    label = get_label(graph, prop_uri)\n",
    "    comment = get_comment(graph, prop_uri)\n",
    "    domains = set(graph.objects(subject=prop_uri, predicate=SCHEMA.domainIncludes))\n",
    "    ranges = set(graph.objects(subject=prop_uri, predicate=SCHEMA.rangeIncludes))\n",
    "    return PropertyInfo(uri=prop_uri, label=label, comment=comment, domains=domains, ranges=ranges)\n",
    "\n",
    "def get_class_details(graph: rdflib.Graph, class_uri: rdflib.URIRef, all_properties: Dict[rdflib.URIRef, PropertyInfo]) -> ClassInfo:\n",
    "    \"\"\"Extracts details for a given class URI.\"\"\"\n",
    "    label = get_label(graph, class_uri)\n",
    "    comment = get_comment(graph, class_uri)\n",
    "    superclasses = set(graph.objects(subject=class_uri, predicate=RDFS.subClassOf))\n",
    "    # Find properties where this class is listed in the domain\n",
    "    associated_properties = set()\n",
    "    for prop_uri, prop_info in all_properties.items():\n",
    "        if class_uri in prop_info.domains:\n",
    "            associated_properties.add(prop_uri)\n",
    "\n",
    "    # Also consider properties inherited from superclasses (requires recursive lookup - omitted for v0.1 simplicity)\n",
    "    # For v0.1, we primarily care about properties directly associated via domainIncludes\n",
    "\n",
    "    return ClassInfo(uri=class_uri, label=label, comment=comment, superclasses=superclasses, properties=associated_properties)\n",
    "\n",
    "def analyze_schema_graph(graph: rdflib.Graph) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Analyzes the RDF graph to extract structured info about classes and properties.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing 'classes' and 'properties' information.\n",
    "    \"\"\"\n",
    "    if not graph:\n",
    "        logging.error(\"Analysis failed: Graph is empty or None.\")\n",
    "        return {\"classes\": {}, \"properties\": {}}\n",
    "\n",
    "    schema_classes_uris = find_schema_classes(graph)\n",
    "    schema_property_uris = find_schema_properties(graph)\n",
    "\n",
    "    properties_info = {}\n",
    "    for prop_uri in schema_property_uris:\n",
    "        properties_info[prop_uri] = get_property_details(graph, prop_uri)\n",
    "\n",
    "    classes_info = {}\n",
    "    for class_uri in schema_classes_uris:\n",
    "        # Skip RDFS/OWL base classes if they somehow got included\n",
    "        if class_uri in [RDFS.Resource, OWL.Thing, RDFS.Class]:\n",
    "             continue\n",
    "        classes_info[class_uri] = get_class_details(graph, class_uri, properties_info)\n",
    "\n",
    "    logging.info(f\"Analyzed {len(classes_info)} classes and {len(properties_info)} properties.\")\n",
    "    return {\"classes\": classes_info, \"properties\": properties_info}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "d51bc8bb-9332-464b-9a4b-5f8ce8984adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Found 628 potential schema.org classes.\n",
      "INFO:root:Found 921 potential schema.org properties.\n",
      "INFO:root:Analyzed 628 classes and 921 properties.\n",
      "INFO:root:Schema analysis complete. Ready for Step 3: Mapping Logic Definition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analysis for schema:Person ---\n",
      "ClassInfo(uri=rdflib.term.URIRef('https://schema.org/Person'), label='Person', comment='A person (alive, dead, undead, or fictional).', superclasses={rdflib.term.URIRef('https://schema.org/Thing')}, properties={rdflib.term.URIRef('https://schema.org/owns'), rdflib.term.URIRef('https://schema.org/skills'), rdflib.term.URIRef('https://schema.org/contactPoint'), rdflib.term.URIRef('https://schema.org/award'), rdflib.term.URIRef('https://schema.org/memberOf'), rdflib.term.URIRef('https://schema.org/jobTitle'), rdflib.term.URIRef('https://schema.org/spouse'), rdflib.term.URIRef('https://schema.org/seeks'), rdflib.term.URIRef('https://schema.org/birthPlace'), rdflib.term.URIRef('https://schema.org/address'), rdflib.term.URIRef('https://schema.org/naics'), rdflib.term.URIRef('https://schema.org/netWorth'), rdflib.term.URIRef('https://schema.org/vatID'), rdflib.term.URIRef('https://schema.org/makesOffer'), rdflib.term.URIRef('https://schema.org/additionalName'), rdflib.term.URIRef('https://schema.org/familyName'), rdflib.term.URIRef('https://schema.org/children'), rdflib.term.URIRef('https://schema.org/funder'), rdflib.term.URIRef('https://schema.org/height'), rdflib.term.URIRef('https://schema.org/email'), rdflib.term.URIRef('https://schema.org/contactPoints'), rdflib.term.URIRef('https://schema.org/deathDate'), rdflib.term.URIRef('https://schema.org/sponsor'), rdflib.term.URIRef('https://schema.org/hasOccupation'), rdflib.term.URIRef('https://schema.org/taxID'), rdflib.term.URIRef('https://schema.org/globalLocationNumber'), rdflib.term.URIRef('https://schema.org/siblings'), rdflib.term.URIRef('https://schema.org/awards'), rdflib.term.URIRef('https://schema.org/homeLocation'), rdflib.term.URIRef('https://schema.org/honorificPrefix'), rdflib.term.URIRef('https://schema.org/affiliation'), rdflib.term.URIRef('https://schema.org/brand'), rdflib.term.URIRef('https://schema.org/follows'), rdflib.term.URIRef('https://schema.org/sibling'), rdflib.term.URIRef('https://schema.org/publishingPrinciples'), rdflib.term.URIRef('https://schema.org/gender'), rdflib.term.URIRef('https://schema.org/isicV4'), rdflib.term.URIRef('https://schema.org/performerIn'), rdflib.term.URIRef('https://schema.org/weight'), rdflib.term.URIRef('https://schema.org/relatedTo'), rdflib.term.URIRef('https://schema.org/alumniOf'), rdflib.term.URIRef('https://schema.org/givenName'), rdflib.term.URIRef('https://schema.org/duns'), rdflib.term.URIRef('https://schema.org/honorificSuffix'), rdflib.term.URIRef('https://schema.org/colleague'), rdflib.term.URIRef('https://schema.org/nationality'), rdflib.term.URIRef('https://schema.org/parent'), rdflib.term.URIRef('https://schema.org/deathPlace'), rdflib.term.URIRef('https://schema.org/colleagues'), rdflib.term.URIRef('https://schema.org/workLocation'), rdflib.term.URIRef('https://schema.org/hasOfferCatalog'), rdflib.term.URIRef('https://schema.org/hasPOS'), rdflib.term.URIRef('https://schema.org/birthDate'), rdflib.term.URIRef('https://schema.org/faxNumber'), rdflib.term.URIRef('https://schema.org/telephone'), rdflib.term.URIRef('https://schema.org/worksFor'), rdflib.term.URIRef('https://schema.org/parents'), rdflib.term.URIRef('https://schema.org/knows')})\n",
      "------------------------------\n",
      "\n",
      "--- Analysis for schema:address ---\n",
      "PropertyInfo(uri=rdflib.term.URIRef('https://schema.org/address'), label='address', comment='Physical address of the item.', domains={rdflib.term.URIRef('https://schema.org/Place'), rdflib.term.URIRef('https://schema.org/GeoShape'), rdflib.term.URIRef('https://schema.org/GeoCoordinates'), rdflib.term.URIRef('https://schema.org/Person'), rdflib.term.URIRef('https://schema.org/Organization')}, ranges={rdflib.term.URIRef('https://schema.org/Text'), rdflib.term.URIRef('https://schema.org/PostalAddress')})\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution block demonstrating analysis ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume schema_graph is loaded from Step 1 (re-parse for standalone demo)\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "\n",
    "    if schema_graph:\n",
    "        # Step 2: Analyze the graph\n",
    "        analyzed_schema = analyze_schema_graph(schema_graph)\n",
    "\n",
    "        # Example: Print info for schema:Person and schema:address\n",
    "        person_uri = SCHEMA.Person\n",
    "        address_prop_uri = SCHEMA.address\n",
    "\n",
    "        if person_uri in analyzed_schema[\"classes\"]:\n",
    "            print(\"\\n--- Analysis for schema:Person ---\")\n",
    "            print(analyzed_schema[\"classes\"][person_uri])\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        if address_prop_uri in analyzed_schema[\"properties\"]:\n",
    "            print(\"\\n--- Analysis for schema:address ---\")\n",
    "            print(analyzed_schema[\"properties\"][address_prop_uri])\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        # Output confirms we have structured data ready for mapping rules (Step 3)\n",
    "        # and code generation (Step 4) in the next chunks.\n",
    "        logging.info(\"Schema analysis complete. Ready for Step 3: Mapping Logic Definition.\")\n",
    "    else:\n",
    "        logging.error(\"Failed to parse schema graph for analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab5f8b-39b0-4e59-8cdb-5626ee71f2dc",
   "metadata": {},
   "source": [
    "# Step 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "fa21c84b-d194-4f87-adf0-8f729b465519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, ForwardRef, Any\n",
    "from pydantic import BaseModel, Field, EmailStr, AnyUrl\n",
    "from datetime import date, datetime, time\n",
    "import re\n",
    "import keyword\n",
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, ForwardRef, Any, cast\n",
    "# Added imports for richer types\n",
    "from pydantic import BaseModel, Field, EmailStr, AnyUrl, constr, conint, condecimal, field_validator, model_validator\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import isodate # Library to parse ISO 8601 durations\n",
    "import decimal\n",
    "import keyword\n",
    "import logging\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f1b53067-53c9-4d63-a084-b6598c76f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantity(BaseModel):\n",
    "    \"\"\"\n",
    "    Base model for quantitative values based on schema.org/Quantity.\n",
    "    Actual value and unit are often in subclasses or specific properties.\n",
    "    This primarily serves as a conceptual base.\n",
    "    \"\"\"\n",
    "    model_config = {'extra': 'allow'} # Allow extra fields as Quantity is generic\n",
    "\n",
    "class Distance(Quantity):\n",
    "    \"\"\"\n",
    "    Represents a distance based on schema.org/Distance.\n",
    "    Uses value and unit representation common in QuantitativeValue.\n",
    "    \"\"\"\n",
    "    # Based on properties commonly used with QuantitativeValue for distance\n",
    "    value: Optional[float] = Field(None, description=\"The numerical value of the distance.\")\n",
    "    unitCode: Optional[str] = Field(None, description=\"UN/CEFACT Common Code (3 characters) or URL for the unit of measurement. E.g., 'MTR' for meter, 'KM' for kilometer, 'FT' for foot, 'INH' for inch.\")\n",
    "    unitText: Optional[str] = Field(None, description=\"A string indicating the unit of measurement. Useful if unitCode is not applicable or needs clarification. E.g., 'meters', 'miles'.\")\n",
    "\n",
    "    model_config = {'extra': 'forbid'}\n",
    "\n",
    "    # Add validation if needed, e.g., check unitCode format\n",
    "\n",
    "class Duration(Quantity):\n",
    "    \"\"\"\n",
    "    Represents a duration based on schema.org/Duration.\n",
    "    Stores duration as datetime.timedelta, parsed from ISO 8601 duration format.\n",
    "    \"\"\"\n",
    "    # Pydantic doesn't have native ISO 8601 duration parsing, use validator\n",
    "    value_iso8601: Optional[str] = Field(None, alias=\"iso8601Duration\", description=\"Duration in ISO 8601 format (e.g., P1Y2M3DT4H5M6S).\")\n",
    "    value_timedelta: Optional[timedelta] = Field(None, exclude=True, description=\"Parsed timedelta value (internal).\") # Exclude from standard model dump\n",
    "\n",
    "    model_config = {'extra': 'forbid', 'populate_by_name': True} # Allow using alias on input\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def parse_duration(cls, data: Any) -> Any:\n",
    "        if isinstance(data, dict):\n",
    "            iso_duration_str = data.get(\"value_iso8601\") or data.get(\"iso8601Duration\")\n",
    "            if iso_duration_str and isinstance(iso_duration_str, str):\n",
    "                try:\n",
    "                    # Use isodate library to parse ISO 8601 duration\n",
    "                    td = isodate.parse_duration(iso_duration_str)\n",
    "                    data['value_timedelta'] = td\n",
    "                    # Keep original string too\n",
    "                    data['value_iso8601'] = iso_duration_str\n",
    "                except (isodate.ISO8601Error, ValueError) as e:\n",
    "                    # Or raise validation error depending on strictness needed\n",
    "                    logging.warning(f\"Could not parse ISO 8601 duration '{iso_duration_str}': {e}\")\n",
    "                    data['value_timedelta'] = None # Set internal value to None on error\n",
    "            # If timedelta is provided directly\n",
    "            elif data.get('value_timedelta') and isinstance(data.get('value_timedelta'), timedelta):\n",
    "                 # Optionally generate ISO string if needed, though complex\n",
    "                 pass\n",
    "        elif isinstance(data, str):\n",
    "             # Allow direct initialization from ISO string\n",
    "             try:\n",
    "                 td = isodate.parse_duration(data)\n",
    "                 return {'value_iso8601': data, 'value_timedelta': td}\n",
    "             except (isodate.ISO8601Error, ValueError) as e:\n",
    "                 logging.warning(f\"Could not parse ISO 8601 duration string '{data}': {e}\")\n",
    "                 return {'value_iso8601': data, 'value_timedelta': None} # Return original string, mark as failed parse\n",
    "\n",
    "        return data # Return dict for Pydantic processing\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return ISO 8601 string representation.\"\"\"\n",
    "        if self.value_timedelta is not None:\n",
    "             try:\n",
    "                 # isodate can also format, but requires careful handling of years/months\n",
    "                 # For simplicity, return original string if present, else standard timedelta str\n",
    "                 return self.value_iso8601 or str(self.value_timedelta)\n",
    "             except Exception:\n",
    "                 return str(self.value_timedelta) # Fallback\n",
    "        return self.value_iso8601 or \"Invalid Duration\"\n",
    "\n",
    "\n",
    "class DefinedTerm(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a term from a defined set based on schema.org/DefinedTerm.\n",
    "    \"\"\"\n",
    "    # Core properties often associated with DefinedTerm\n",
    "    termCode: Optional[str] = Field(None, description=\"A code that identifies this DefinedTerm within a DefinedTermSet.\")\n",
    "    name: Optional[str] = Field(None, description=\"The name of the item.\")\n",
    "    description: Optional[str] = Field(None, description=\"A description of the item.\")\n",
    "    # Allow referencing the set it belongs to, if known\n",
    "    inDefinedTermSet: Optional[AnyUrl] = Field(None, description=\"A DefinedTermSet Organization or DataCatalog that contains this term.\")\n",
    "\n",
    "    model_config = {'extra': 'allow'} # Allow potential other properties from schema.org or extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b8056-ef01-43aa-a4b7-f6a443869f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "5e29b50c-592c-453c-a504-576bf320e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def safe_python_identifier(name: str) -> str:\n",
    "#     \"\"\"Converts a name to a valid Python identifier, handling keywords.\"\"\"\n",
    "#     if keyword.iskeyword(name):\n",
    "#         return name + \"_\"\n",
    "#     # Basic check for valid identifier start/characters - can be improved\n",
    "#     if not name or not (name[0].isalpha() or name[0] == '_'):\n",
    "#         name = '_' + name # Ensure valid start if needed\n",
    "#     # Replace invalid characters (simplistic)\n",
    "#     name = re.sub(r'\\W|^(?=\\d)', '_', name)\n",
    "#     return name\n",
    "\n",
    "# def map_uri_to_classname(uri: rdflib.URIRef) -> str:\n",
    "#     \"\"\"Converts a schema.org URI to a Python CamelCase class name.\"\"\"\n",
    "#     if not str(uri).startswith(str(SCHEMA)):\n",
    "#         # Handle non-schema.org URIs if necessary, maybe return original or raise error\n",
    "#         return str(uri).split('/')[-1].split('#')[-1] # Best guess\n",
    "#     local_name = uri.replace(SCHEMA, \"\")\n",
    "#     # Basic check for upper camel case, assuming schema.org mostly uses this\n",
    "#     if local_name and local_name[0].isupper():\n",
    "#         return safe_python_identifier(local_name)\n",
    "#     else:\n",
    "#         # Attempt to convert potentially lowerCamelCase or other cases\n",
    "#         # This is a simple heuristic, might need refinement\n",
    "#         parts = re.split(r'[-_ ]', local_name)\n",
    "#         return safe_python_identifier(\"\".join(part.capitalize() for part in parts))\n",
    "\n",
    "\n",
    "# def map_uri_to_fieldname(uri: rdflib.URIRef) -> str:\n",
    "#     \"\"\"Converts a schema.org property URI to a Python snake_case field name.\"\"\"\n",
    "#     if not str(uri).startswith(str(SCHEMA)):\n",
    "#          return safe_python_identifier(str(uri).split('/')[-1].split('#')[-1].lower()) # Best guess\n",
    "#     local_name = uri.replace(SCHEMA, \"\")\n",
    "#     # Convert camelCase or PascalCase to snake_case\n",
    "#     s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', local_name)\n",
    "#     snake_case_name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "#     return safe_python_identifier(snake_case_name)\n",
    "\n",
    "# def map_range_to_typehint(\n",
    "#     ranges: Set[rdflib.URIRef],\n",
    "#     class_registry: Set[str], # Set of known generated class names\n",
    "#     default_optional: bool = True,\n",
    "#     use_list_for_multi: bool = True # Assume List for properties allowing multiple values in RDF? Risky default.\n",
    "#                                      # Schema.org generally doesn't use OWL cardinality.\n",
    "#                                      # A safer default might be Optional[Union[T1, T2, List[T1], List[T2]]]\n",
    "#                                      # For v0.1, let's keep it simpler: use Optional[Union[...]]\n",
    "# ) -> str:\n",
    "#     \"\"\"Maps a set of RDF range URIs to a Python type hint string.\"\"\"\n",
    "#     if not ranges:\n",
    "#         return \"typing.Any\" # No range specified\n",
    "\n",
    "#     mapped_types = set()\n",
    "#     for r_uri in ranges:\n",
    "#         if r_uri in TYPE_MAP:\n",
    "#             mapped_types.add(TYPE_MAP[r_uri])\n",
    "#         else:\n",
    "#             # Assume it's a class defined in our ontology\n",
    "#             class_name = map_uri_to_classname(r_uri)\n",
    "#             if class_name in class_registry:\n",
    "#                  # Use string literal for forward reference\n",
    "#                 mapped_types.add(f\"'{class_name}'\")\n",
    "#             elif str(r_uri) == str(SCHEMA.Thing) or str(r_uri) == str(RDFS.Resource):\n",
    "#                  mapped_types.add(\"typing.Any\") # Map generic Thing/Resource\n",
    "#             else:\n",
    "#                  # Unknown range URI - treat as Any or potentially raise error/warning\n",
    "#                  logging.warning(f\"Unknown range URI encountered: {r_uri}. Mapping to Any.\")\n",
    "#                  mapped_types.add(\"typing.Any\")\n",
    "\n",
    "#     # Remove duplicates and sort for consistent output\n",
    "#     unique_types = sorted(list(mapped_types))\n",
    "\n",
    "#     if not unique_types:\n",
    "#          return \"typing.Any\" # Should not happen if ranges is not empty, but safety check\n",
    "\n",
    "#     # Build the Union if multiple types\n",
    "#     type_hint_core = \"\"\n",
    "#     if len(unique_types) == 1:\n",
    "#         type_hint_core = unique_types[0]\n",
    "#     else:\n",
    "#         type_hint_core = f\"typing.Union[{', '.join(unique_types)}]\"\n",
    "\n",
    "#     # Handle Optionality (defaulting to Optional for v0.1 simplicity)\n",
    "#     # A more advanced version would check OWL cardinality if present\n",
    "#     if default_optional:\n",
    "#         # Check if None is effectively already included via Optional[...] in the union parts\n",
    "#         is_already_optional = any(t.startswith(\"typing.Optional[\") or t == 'None' for t in unique_types)\n",
    "#         if not is_already_optional:\n",
    "#              return f\"typing.Optional[{type_hint_core}]\"\n",
    "#         else:\n",
    "#              # If Optional is already part of a Union, just return the Union\n",
    "#              # e.g., Union[Optional['Thing'], str] is valid\n",
    "#              # This logic might need refinement based on desired strictness\n",
    "#              return type_hint_core\n",
    "#     else:\n",
    "#         return type_hint_core\n",
    "\n",
    "# def get_field_metadata(prop_info: PropertyInfo) -> Dict[str, Union[str, Dict]]:\n",
    "#     \"\"\"Generates arguments for pydantic.Field based on PropertyInfo.\"\"\"\n",
    "#     args = {}\n",
    "#     if prop_info.comment:\n",
    "#         # Basic cleaning of comment string\n",
    "#         clean_comment = ' '.join(prop_info.comment.split())\n",
    "#         args['description'] = clean_comment\n",
    "#     # Add aliases for common variations\n",
    "#     if prop_info.uri in PROPERTY_ALIAS_MAP:\n",
    "#         # Pydantic v2 alias handling might differ slightly, adjust as needed\n",
    "#         for alias_type, alias_value in PROPERTY_ALIAS_MAP[prop_info.uri].items():\n",
    "#              args[alias_type] = alias_value # e.g., validation_alias='dob'\n",
    "\n",
    "#     # Example: Add examples if available (assuming they could be parsed from RDF)\n",
    "#     # if prop_info.examples: args['examples'] = prop_info.examples\n",
    "\n",
    "#     # Default value is None for Optional fields, handled by type hint + Field(None)\n",
    "#     # Required fields would have no default in Field()\n",
    "#     # We default to Optional, so default is usually None\n",
    "#     default_value = None\n",
    "\n",
    "#     field_args_str = f\"default={default_value}\"\n",
    "#     if args:\n",
    "#         args_repr = ', '.join(f\"{k}={repr(v)}\" for k, v in args.items())\n",
    "#         field_args_str += f\", {args_repr}\"\n",
    "\n",
    "#     # Return structure suitable for formatting into Field(...) call\n",
    "#     # Returning dict for easier manipulation before final string formatting\n",
    "#     return {'default': default_value, **args}\n",
    "\n",
    "\n",
    "# # --- MRO Resolution Helper Functions ---\n",
    "\n",
    "# def get_all_ancestors(graph: rdflib.Graph, class_uri: rdflib.URIRef, known_classes_uris: Set[rdflib.URIRef]) -> Set[rdflib.URIRef]:\n",
    "#     \"\"\"Recursively find all superclass URIs for a given class URI within our known set.\"\"\"\n",
    "#     ancestors = set()\n",
    "#     parents = set(graph.objects(subject=class_uri, predicate=RDFS.subClassOf))\n",
    "#     for parent_uri in parents:\n",
    "#         if parent_uri in known_classes_uris and parent_uri not in [RDFS.Resource, OWL.Thing]:\n",
    "#             if parent_uri not in ancestors: # Avoid infinite loops\n",
    "#                 ancestors.add(parent_uri)\n",
    "#                 ancestors.update(get_all_ancestors(graph, parent_uri, known_classes_uris))\n",
    "#     return ancestors\n",
    "\n",
    "# def get_base_classes(\n",
    "#      class_info: ClassInfo,\n",
    "#      all_class_names: Set[str], # Not strictly needed here, using URIs mainly\n",
    "#      all_class_uris: Set[rdflib.URIRef],\n",
    "#      graph: rdflib.Graph\n",
    "#      ) -> List[str]:\n",
    "#     \"\"\"Determines the most specific base classes for a Pydantic model, pruning redundant ancestors.\"\"\"\n",
    "#     direct_superclass_uris = {sup for sup in class_info.superclasses if sup in all_class_uris and sup not in [RDFS.Resource, OWL.Thing]}\n",
    "#     if not direct_superclass_uris: return [\"BaseModel\"]\n",
    "\n",
    "#     ancestor_map = {sup: get_all_ancestors(graph, sup, all_class_uris) for sup in direct_superclass_uris}\n",
    "#     minimal_bases_uris = set()\n",
    "#     for potential_base in direct_superclass_uris:\n",
    "#         is_ancestor_of_another = False\n",
    "#         for other_base in direct_superclass_uris:\n",
    "#             if potential_base != other_base and potential_base in ancestor_map.get(other_base, set()):\n",
    "#                 is_ancestor_of_another = True; break\n",
    "#         if not is_ancestor_of_another: minimal_bases_uris.add(potential_base)\n",
    "\n",
    "#     if not minimal_bases_uris:\n",
    "#         # If pruning removed everything, it might mean all direct parents were ancestors of others.\n",
    "#         # Fallback cautiously to direct parents or just BaseModel\n",
    "#         logger.warning(f\"Could not determine minimal bases for {class_info.uri} from {direct_superclass_uris} after pruning. Check hierarchy. Defaulting to direct parents or BaseModel.\")\n",
    "#         # Use direct superclass names if pruning failed, otherwise BaseModel\n",
    "#         direct_base_names = sorted([map_uri_to_classname(uri) for uri in direct_superclass_uris])\n",
    "#         return direct_base_names if direct_base_names else [\"BaseModel\"]\n",
    "#     else:\n",
    "#         base_class_names = sorted([map_uri_to_classname(uri) for uri in minimal_bases_uris])\n",
    "#         return base_class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc165c-7a03-41ab-bfac-917f9d8cddff",
   "metadata": {},
   "source": [
    "# Step 4: Pydantic Class Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e556c05-ce73-4480-a24c-bc1470bd90c7",
   "metadata": {},
   "source": [
    "## Updated Step 4 plus some of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "de0461e7-8b31-4c29-80ca-67299c9402ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_python_identifier(name: str) -> str:\n",
    "    if keyword.iskeyword(name): return name + \"_\"\n",
    "    if not name or not (name[0].isalpha() or name[0] == '_'): name = '_' + name\n",
    "    name = re.sub(r'\\W|^(?=\\d)', '_', name)\n",
    "    return name\n",
    "\n",
    "def map_uri_to_classname(uri: rdflib.URIRef) -> str:\n",
    "    # Simplified version for brevity, assumes SCHEMA namespace primarily\n",
    "    local_name = uri.replace(SCHEMA, \"\")\n",
    "    if not local_name or ':' in local_name or '/' in local_name: # Handle non-schema or complex URIs crudely\n",
    "        local_name = str(uri).split('/')[-1].split('#')[-1]\n",
    "    # Convert to CamelCase if needed\n",
    "    if local_name and local_name[0].islower():\n",
    "        parts = re.split(r'[-_ ]', local_name)\n",
    "        local_name = \"\".join(part.capitalize() for part in parts if part)\n",
    "    return safe_python_identifier(local_name or \"_UnknownClass\")\n",
    "\n",
    "def map_uri_to_fieldname(uri: rdflib.URIRef) -> str:\n",
    "    local_name = uri.replace(SCHEMA, \"\")\n",
    "    if not local_name or ':' in local_name or '/' in local_name:\n",
    "        local_name = str(uri).split('/')[-1].split('#')[-1]\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', local_name)\n",
    "    snake_case_name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "    return safe_python_identifier(snake_case_name or \"_unknown_field\")\n",
    "\n",
    "def get_module_path_for_class(class_name: str) -> str:\n",
    "    \"\"\"Determines relative module path (snake_case) for a CamelCase class name.\"\"\"\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', class_name)\n",
    "    snake_case_name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "    module_name_part = safe_python_identifier(snake_case_name)\n",
    "    return f\".{module_name_part}\"\n",
    "\n",
    "def map_range_to_typehint(\n",
    "    ranges: Set[rdflib.URIRef],\n",
    "    all_class_names: Set[str],\n",
    "    base_type_names: Set[str]\n",
    ") -> str:\n",
    "    \"\"\"Maps RDF range URIs to a Python type hint string.\"\"\"\n",
    "    if not ranges: return \"Any\"\n",
    "    mapped_types = set()\n",
    "    for r_uri in ranges:\n",
    "        if r_uri in TYPE_MAP:\n",
    "            type_name = TYPE_MAP[r_uri]\n",
    "            if type_name in base_type_names:\n",
    "                 # Reference base types via the module where they will live\n",
    "                 mapped_types.add(f\"base_types.{type_name}\")\n",
    "            else:\n",
    "                 mapped_types.add(type_name)\n",
    "        else:\n",
    "            class_name = map_uri_to_classname(r_uri)\n",
    "            if class_name in all_class_names:\n",
    "                 mapped_types.add(f\"'{class_name}'\")\n",
    "            elif str(r_uri) == str(SCHEMA.Thing) or str(r_uri) == str(RDFS.Resource):\n",
    "                 mapped_types.add(\"Any\")\n",
    "            else:\n",
    "                 logger.warning(f\"Unknown range URI: {r_uri}. Mapping to Any.\")\n",
    "                 mapped_types.add(\"Any\")\n",
    "\n",
    "    unique_types = sorted([t for t in mapped_types if t != \"Any\"])\n",
    "    if not unique_types: return \"Any\"\n",
    "\n",
    "    type_hint_core = \"\"\n",
    "    if len(unique_types) == 1:\n",
    "        type_hint_core = unique_types[0]\n",
    "    else:\n",
    "        union_args = []\n",
    "        for t in unique_types:\n",
    "             # Clean potential prefixes only for final display in Union, imports handle resolution\n",
    "             # clean_t = t.replace(\"base_types.\", \"\") if t.startswith(\"base_types.\") else t\n",
    "             union_args.append(t) # clean_t\n",
    "        type_hint_core = f\"Union[{', '.join(union_args)}]\"\n",
    "    print(f\"type_hint_core: {type_hint_core}\")\n",
    "    # Default to Optional\n",
    "    is_already_optional = any(t.startswith(\"Optional[\") or t == 'None' for t in unique_types)\n",
    "    if not is_already_optional:\n",
    "         return f\"Optional[{type_hint_core}]\"\n",
    "    else:\n",
    "         return type_hint_core\n",
    "\n",
    "def get_field_metadata(prop_info: PropertyInfo) -> Dict:\n",
    "    \"\"\"Generates arguments for pydantic.Field based on PropertyInfo.\"\"\"\n",
    "    args = {}\n",
    "    if prop_info.comment:\n",
    "        clean_comment = ' '.join(prop_info.comment.split())\n",
    "        # Escape potential triple quotes in comments for docstrings/descriptions\n",
    "        clean_comment = clean_comment.replace('\"\"\"', '\\\\\"\\\\\"\\\\\"').replace(\"'''\", \"\\\\'\\\\'\\\\'\")\n",
    "        args['description'] = clean_comment\n",
    "    if prop_info.uri in PROPERTY_ALIAS_MAP:\n",
    "        for alias_type, alias_value in PROPERTY_ALIAS_MAP[prop_info.uri].items():\n",
    "             args[alias_type] = alias_value\n",
    "    return {'default': None, **args}\n",
    "\n",
    "# --- MRO Resolution Helper Functions ---\n",
    "\n",
    "def get_all_ancestors(graph: rdflib.Graph, class_uri: rdflib.URIRef, known_classes_uris: Set[rdflib.URIRef]) -> Set[rdflib.URIRef]:\n",
    "    \"\"\"Recursively find all superclass URIs for a given class URI within our known set.\"\"\"\n",
    "    ancestors = set()\n",
    "    parents = set(graph.objects(subject=class_uri, predicate=RDFS.subClassOf))\n",
    "    for parent_uri in parents:\n",
    "        if parent_uri in known_classes_uris and parent_uri not in [RDFS.Resource, OWL.Thing]:\n",
    "            if parent_uri not in ancestors: # Avoid infinite loops\n",
    "                ancestors.add(parent_uri)\n",
    "                ancestors.update(get_all_ancestors(graph, parent_uri, known_classes_uris))\n",
    "    return ancestors\n",
    "\n",
    "def get_base_classes(\n",
    "     class_info: ClassInfo,\n",
    "     all_class_names: Set[str], # Not strictly needed here, using URIs mainly\n",
    "     all_class_uris: Set[rdflib.URIRef],\n",
    "     graph: rdflib.Graph\n",
    "     ) -> List[str]:\n",
    "    \"\"\"Determines the most specific base classes for a Pydantic model, pruning redundant ancestors.\"\"\"\n",
    "    direct_superclass_uris = {sup for sup in class_info.superclasses if sup in all_class_uris and sup not in [RDFS.Resource, OWL.Thing]}\n",
    "    if not direct_superclass_uris: return [\"BaseModel\"]\n",
    "\n",
    "    ancestor_map = {sup: get_all_ancestors(graph, sup, all_class_uris) for sup in direct_superclass_uris}\n",
    "    minimal_bases_uris = set()\n",
    "    for potential_base in direct_superclass_uris:\n",
    "        is_ancestor_of_another = False\n",
    "        for other_base in direct_superclass_uris:\n",
    "            if potential_base != other_base and potential_base in ancestor_map.get(other_base, set()):\n",
    "                is_ancestor_of_another = True; break\n",
    "        if not is_ancestor_of_another: minimal_bases_uris.add(potential_base)\n",
    "\n",
    "    if not minimal_bases_uris:\n",
    "        # If pruning removed everything, it might mean all direct parents were ancestors of others.\n",
    "        # Fallback cautiously to direct parents or just BaseModel\n",
    "        logger.warning(f\"Could not determine minimal bases for {class_info.uri} from {direct_superclass_uris} after pruning. Check hierarchy. Defaulting to direct parents or BaseModel.\")\n",
    "        # Use direct superclass names if pruning failed, otherwise BaseModel\n",
    "        direct_base_names = sorted([map_uri_to_classname(uri) for uri in direct_superclass_uris])\n",
    "        return direct_base_names if direct_base_names else [\"BaseModel\"]\n",
    "    else:\n",
    "        base_class_names = sorted([map_uri_to_classname(uri) for uri in minimal_bases_uris])\n",
    "        return base_class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "70cd7313-3c02-410b-b636-6c7c3452ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, Any, cast, ForwardRef, TYPE_CHECKING\n",
    "from pydantic import (\n",
    "    BaseModel, Field, AnyUrl, field_validator,\n",
    "    model_validator, condecimal, constr, EmailStr\n",
    ")\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import decimal\n",
    "import isodate # Requires: pip install isodate\n",
    "import keyword\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import shlex\n",
    "import textwrap\n",
    "import importlib\n",
    "import pkgutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e3c3e5fc-4730-4f1d-a2b0-638769171430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Complete Step 4 Script: Schema.org to Pydantic Generator ---\n",
    "\n",
    "\n",
    "# --- Basic Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "SCHEMA_FILE = \"schema.txt\" # Assumes schema.txt is in the same directory\n",
    "SCHEMA_FORMAT = \"turtle\"\n",
    "OUTPUT_DIR = \"output_ontology\"\n",
    "MODELS_SUBDIR = \"models\"\n",
    "BASE_TYPES_FILENAME = \"base_types.py\" # Filename for base types\n",
    "\n",
    "SCHEMA = rdflib.Namespace(\"https://schema.org/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e3128e0e-a327-463d-a1f6-40a7f71540f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Code for base_types.py (Essential Context) ---\n",
    "BASE_TYPES_CODE = \"\"\"\n",
    "from __future__ import annotations # Keep first\n",
    "from pydantic import (\n",
    "    BaseModel, Field, AnyUrl, field_validator,\n",
    "    model_validator, condecimal, constr, EmailStr\n",
    ")\n",
    "from typing import Optional, List, Union, Any, Annotated\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import decimal\n",
    "import isodate # Requires: pip install isodate\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__) # Use logger for warnings\n",
    "\n",
    "class Quantity(BaseModel):\n",
    "    \\\"\\\"\\\"Base model for schema.org/Quantity.\\\"\"\"\\\"\\\"\n",
    "    model_config = {'extra': 'allow'}\n",
    "\n",
    "class Distance(Quantity):\n",
    "    \\\"\\\"\\\"Represents schema.org/Distance.\\\"\"\"\\\"\\\"\n",
    "    value: Optional[float] = Field(None, description=\"The numerical value of the distance.\")\n",
    "    unitCode: Optional[str] = Field(None, description=\"UN/CEFACT Common Code (3 characters) or URL. E.g., 'MTR', 'KM', 'FT'.\")\n",
    "    unitText: Optional[str] = Field(None, description=\"A string indicating the unit of measurement.\")\n",
    "    model_config = {'extra': 'forbid'}\n",
    "\n",
    "class Duration(Quantity):\n",
    "    \\\"\\\"\\\"Represents schema.org/Duration, parsing ISO 8601 string.\\\"\"\"\\\"\\\"\n",
    "    value_iso8601: Optional[str] = Field(None, validation_alias='iso8601Duration', serialization_alias='iso8601Duration', description=\"Duration in ISO 8601 format.\")\n",
    "    value_timedelta: Optional[timedelta] = Field(None, exclude=True, description=\"Parsed timedelta value (internal).\")\n",
    "    model_config = {'extra': 'forbid', 'populate_by_name': True}\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def parse_duration(cls, data: Any) -> Any:\n",
    "        if isinstance(data, dict):\n",
    "            iso_duration_str = data.get(\"value_iso8601\") or data.get(\"iso8601Duration\")\n",
    "            if iso_duration_str and isinstance(iso_duration_str, str) and 'value_timedelta' not in data:\n",
    "                try:\n",
    "                    td = isodate.parse_duration(iso_duration_str)\n",
    "                    data['value_timedelta'] = td\n",
    "                    data['value_iso8601'] = iso_duration_str\n",
    "                except (isodate.ISO8601Error, ValueError) as e:\n",
    "                    logger.warning(f\"Could not parse ISO 8601 duration '{iso_duration_str}': {e}\")\n",
    "                    data['value_timedelta'] = None\n",
    "                    data['value_iso8601'] = iso_duration_str\n",
    "            elif data.get('value_timedelta') and isinstance(data.get('value_timedelta'), timedelta):\n",
    "                pass\n",
    "        elif isinstance(data, str):\n",
    "             try: \n",
    "                 return {'value_iso8601': data, 'value_timedelta': isodate.parse_duration(data)}\n",
    "             except (isodate.ISO8601Error, ValueError) as e:\n",
    "                 logger.warning(f\"Could not parse ISO 8601 duration string '{data}': {e}\")\n",
    "                 return {'value_iso8601': data, 'value_timedelta': None}\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def timedelta(self) -> Optional[timedelta]: return self.value_timedelta\n",
    "    def __str__(self) -> str: return self.value_iso8601 or \"Invalid/Missing Duration\"\n",
    "\n",
    "class DefinedTerm(BaseModel):\n",
    "    \\\"\\\"\\\"Represents schema.org/DefinedTerm.\\\"\"\"\\\"\\\"\n",
    "    termCode: Optional[str] = Field(None, description=\"A code for this DefinedTerm within a DefinedTermSet.\")\n",
    "    name: Optional[str] = Field(None, description=\"The name of the item.\")\n",
    "    description: Optional[str] = Field(None, description=\"A description of the item.\")\n",
    "    inDefinedTermSet: Optional[AnyUrl] = Field(None, description=\"The DefinedTermSet that contains this term.\")\n",
    "    model_config = {'extra': 'allow'}\n",
    "\n",
    "class Money(BaseModel):\n",
    "     \\\"\\\"\\\"Represents an amount of money with a currency.\\\"\"\"\\\"\\\"\n",
    "     amount: Optional[decimal.Decimal] = Field(None, description=\"The amount of money.\")\n",
    "     currency: Optional[Annotated[str, constr(pattern=r'^[A-Z]{3}$')]] = Field(None, description=\"ISO 4217 Currency Code\")\n",
    "\n",
    "     @field_validator('amount', mode='before')\n",
    "     @classmethod\n",
    "     def clean_amount(cls, v: Any) -> Optional[decimal.Decimal]:\n",
    "         if isinstance(v, (int, float)):\n",
    "             try:\n",
    "                 return decimal.Decimal(v)\n",
    "             except Exception as e: \n",
    "                 logger.error(f\"Error converting {v} to Decimal: {e}\")\n",
    "                 raise ValueError(f\"Cannot convert {v} to Decimal\")\n",
    "         if isinstance(v, str):\n",
    "             try: \n",
    "                 return decimal.Decimal(v.strip())\n",
    "             except decimal.InvalidOperation:\n",
    "                 raise ValueError(f\"Invalid decimal format for amount: {v}\")\n",
    "         if isinstance(v, decimal.Decimal) or v is None: \n",
    "            return v   \n",
    "         raise ValueError(f\"Unexpected type for amount: {type(v)}\")\n",
    "\n",
    "     model_config = {'extra': 'forbid'}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "e13040b4-c465-468a-afe7-34de3cc945be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pydantic_model_code(\n",
    "    class_info: ClassInfo,\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo],\n",
    "    all_class_names: Set[str],\n",
    "    all_class_uris: Set[rdflib.URIRef],\n",
    "    schema_graph: rdflib.Graph, # Added argument\n",
    "    base_type_names: Set[str]  # Added argument\n",
    ") -> str:\n",
    "    \"\"\"Generates the Python code string for a single Pydantic model, including robust imports and correct base classes.\"\"\"\n",
    "\n",
    "    class_name = map_uri_to_classname(class_info.uri)\n",
    "\n",
    "    # --- Use the Corrected get_base_classes ---\n",
    "    valid_base_names_list = get_base_classes(class_info, all_class_names, all_class_uris, schema_graph)\n",
    "\n",
    "    # --- Import Handling Setup ---\n",
    "    typing_imports_needed = set()\n",
    "    pydantic_imports_needed = set()\n",
    "    datetime_imports_needed = set()\n",
    "    other_stdlib_imports = set()\n",
    "    cross_module_imports = set() # For forward refs in hints (populated in field loop)\n",
    "    runtime_base_class_imports = set() # For class def line imports\n",
    "    rich_type_imports = set()\n",
    "    needs_base_types_import = False\n",
    "\n",
    "    # Determine base class string and populate runtime imports\n",
    "    if not valid_base_names_list or valid_base_names_list == [\"BaseModel\"]:\n",
    "        base_class_str = \"BaseModel\"\n",
    "        pydantic_imports_needed.add(\"BaseModel\")\n",
    "    else:\n",
    "        base_class_str = \", \".join(valid_base_names_list)\n",
    "        for base_name in valid_base_names_list:\n",
    "             if base_name != \"BaseModel\":\n",
    "                  if base_name != class_name:\n",
    "                       # Use helper to get module path, ensure it's relative '.'\n",
    "                       module_path = get_module_path_for_class(base_name)\n",
    "                       runtime_base_class_imports.add(f\"from {module_path} import {base_name}\")\n",
    "                  else: logger.warning(f\"Class {class_name} listed itself as base?\")\n",
    "\n",
    "    # --- Analyze Fields ---\n",
    "    field_definitions = []\n",
    "    field_added = False\n",
    "    for prop_uri in sorted(list(class_info.properties)):\n",
    "        if prop_uri not in properties_info: continue\n",
    "        prop_info = properties_info[prop_uri]\n",
    "        field_name = map_uri_to_fieldname(prop_info.uri)\n",
    "        \n",
    "        # **** DETECT Need for Rich Types based on RANGES ****\n",
    "        prop_needs_rich_type = False\n",
    "        for r_uri in prop_info.ranges:\n",
    "            if r_uri in base_type_uris: # Check against the set of URIs defined in base_types\n",
    "                 class_name_for_import = map_uri_to_classname(r_uri)\n",
    "                 rich_type_imports.add(class_name_for_import)\n",
    "                 prop_needs_rich_type = True\n",
    "\n",
    "        # Clean prefixes based on imports that *will* be generated\n",
    "        type_hint_str = map_range_to_typehint(prop_info.ranges, all_class_names, base_type_names)\n",
    "        final_type_hint = type_hint_str\n",
    "        if \"Optional\" in final_type_hint or \"List\" in final_type_hint or \\\n",
    "           \"Union\" in final_type_hint or \"Any\" in final_type_hint:\n",
    "            final_type_hint = final_type_hint.replace(\"typing.\", \"\")\n",
    "            typing_imports_needed.update(re.findall(r'\\b(Optional|List|Union|Any)\\b', final_type_hint))\n",
    "        if \"date\" in final_type_hint or \"datetime\" in final_type_hint or \\\n",
    "           \"time\" in final_type_hint or \"timedelta\" in final_type_hint:\n",
    "            final_type_hint = final_type_hint.replace(\"datetime.\", \"\")\n",
    "            datetime_imports_needed.update(re.findall(r'\\b(date|datetime|time|timedelta)\\b', final_type_hint))\n",
    "        if \"AnyUrl\" in final_type_hint or \"EmailStr\" in final_type_hint:\n",
    "             final_type_hint = final_type_hint.replace(\"pydantic.\", \"\")\n",
    "             pydantic_imports_needed.update(re.findall(r'\\b(AnyUrl|EmailStr)\\b', final_type_hint))\n",
    "        # Rich types require prefix currently from map_range_to_typehint\n",
    "        # if \"base_types.Quantity\" in final_type_hint: rich_type_imports.add(\"Quantity\")\n",
    "        # if \"base_types.Distance\" in final_type_hint: rich_type_imports.add(\"Distance\")\n",
    "        # if \"base_types.Duration\" in final_type_hint: rich_type_imports.add(\"Duration\")\n",
    "        # if \"base_types.DefinedTerm\" in final_type_hint: rich_type_imports.add(\"DefinedTerm\")\n",
    "        # if \"base_types.Money\" in final_type_hint: rich_type_imports.add(\"Money\")\n",
    "        # Remove base_types prefix for the final hint string\n",
    "        # final_type_hint = final_type_hint.replace(\"base_types.\", \"\")\n",
    "\n",
    "        if \"Decimal\" in final_type_hint: other_stdlib_imports.add(\"import decimal\")\n",
    "        # if \"base_types.\" in final_type_hint:\n",
    "        #     needs_base_types_import = True\n",
    "\n",
    "\n",
    "        # Track cross-module imports needed only for hints\n",
    "        potential_classes_in_hint = set(re.findall(r\"'(\\w+)'\", final_type_hint))\n",
    "        for potential_class in potential_classes_in_hint:\n",
    "             if potential_class in all_class_names and \\\n",
    "                potential_class != class_name and \\\n",
    "                potential_class not in valid_base_names_list:\n",
    "                 cross_module_imports.add(potential_class)\n",
    "\n",
    "        field_args_dict = get_field_metadata(prop_info)\n",
    "        field_args_parts = [repr(field_args_dict.pop('default', None))]\n",
    "        field_args_parts.extend(f\"{k}={repr(v)}\" for k, v in field_args_dict.items())\n",
    "        field_call = f\"Field({', '.join(field_args_parts)})\"\n",
    "\n",
    "        field_definitions.append(f\"    {field_name}: {final_type_hint} = {field_call}\")\n",
    "        field_added = True\n",
    "\n",
    "    if field_added: pydantic_imports_needed.add(\"Field\")\n",
    "\n",
    "    # --- Assemble Code ---\n",
    "    code_parts = [\"from __future__ import annotations\"]\n",
    "\n",
    "    if datetime_imports_needed: code_parts.append(f\"from datetime import {', '.join(sorted(list(datetime_imports_needed)))}\")\n",
    "    code_parts.extend(sorted(list(other_stdlib_imports)))\n",
    "\n",
    "    needs_type_checking_block = bool(cross_module_imports)\n",
    "    if needs_type_checking_block: typing_imports_needed.add(\"TYPE_CHECKING\")\n",
    "    if typing_imports_needed: code_parts.append(f\"from typing import {', '.join(sorted(list(typing_imports_needed)))}\")\n",
    "\n",
    "    # Add BaseModel if needed, ensure it's not duplicated if already added\n",
    "    if \"BaseModel\" not in pydantic_imports_needed and base_class_str == \"BaseModel\":\n",
    "         pydantic_imports_needed.add(\"BaseModel\")\n",
    "    if pydantic_imports_needed: code_parts.append(f\"from pydantic import {', '.join(sorted(list(pydantic_imports_needed)))}\")\n",
    "    if needs_base_types_import:\n",
    "        code_parts.append(f\"from . import base_types\")\n",
    "    # if rich_type_imports: \n",
    "    code_parts.append(f\"from . import base_types\")\n",
    "        # code_parts.append(f\"from .base_types import {', '.join(sorted(list(rich_type_imports)))}\")\n",
    "\n",
    "    # Runtime base class imports MUST come before the TYPE_CHECKING block if they are needed by it\n",
    "    code_parts.extend(sorted(list(runtime_base_class_imports)))\n",
    "\n",
    "    if needs_type_checking_block:\n",
    "        code_parts.append(\"\\nif TYPE_CHECKING:\")\n",
    "        for class_to_import in sorted(list(cross_module_imports)):\n",
    "            module_path = get_module_path_for_class(class_to_import)\n",
    "            if class_to_import != class_name: code_parts.append(f\"    from {module_path} import {class_to_import}\")\n",
    "\n",
    "    code_parts.append(\"\\n\")\n",
    "    class_docstring = f'\"\"\"\\n    {class_name}: {textwrap.shorten(class_info.comment or \"No description provided.\", width=70)}\\n\\n    Generated from: {class_info.uri}\\n    \"\"\"'\n",
    "    code_parts.append(f\"class {class_name}({base_class_str}):\")\n",
    "    code_parts.append(f\"    {class_docstring}\")\n",
    "    if not field_definitions: code_parts.append(\"    pass\")\n",
    "    else: code_parts.extend(field_definitions)\n",
    "    code_parts.append(\"\\n    model_config = {'extra': 'forbid'}\")\n",
    "\n",
    "    return \"\\n\".join(code_parts) + \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "be2a3ead-ebb7-46bf-864d-c9be0e78c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Corrected Ontology Models Generator Function ---\n",
    "def generate_ontology_models(\n",
    "     analyzed_schema: Dict[str, Dict],\n",
    "     output_base_dir: str,\n",
    "     \n",
    "     # Added arguments needed by generate_pydantic_model_code\n",
    "     schema_graph: rdflib.Graph,\n",
    "     properties_info: Dict[rdflib.URIRef, PropertyInfo],\n",
    "     models_subdir: str = \"models\",\n",
    "     ) -> None:\n",
    "    \"\"\"\n",
    "    Generates Pydantic model files from analyzed schema info, including base_types.py\n",
    "    and a correctly structured __init__.py.\n",
    "    \"\"\"\n",
    "    classes_info: Dict[rdflib.URIRef, ClassInfo] = analyzed_schema.get(\"classes\", {})\n",
    "    if not classes_info:\n",
    "        logging.error(\"No class information found.\")\n",
    "        return\n",
    "\n",
    "    output_path = pathlib.Path(output_base_dir) / models_subdir\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"__init__.py\").touch()\n",
    "\n",
    "    base_type_names = {\"Quantity\", \"Distance\", \"Duration\", \"DefinedTerm\", \"Money\"}\n",
    "    base_type_uris = { SCHEMA.Quantity, SCHEMA.Distance, SCHEMA.Duration, SCHEMA.DefinedTerm, SCHEMA.Money }\n",
    "    # base_type_names = {map_uri_to_classname(uri) for uri in base_type_uris if uri in classes_info}\n",
    "    print(f\"base_type_uris: {base_type_uris}\")\n",
    "    print(f\"base_type_names: {base_type_names}\")\n",
    "\n",
    "    base_types_path = output_path / BASE_TYPES_FILENAME\n",
    "    try:\n",
    "        # Ensure BASE_TYPES_CODE is accessible\n",
    "        if 'BASE_TYPES_CODE' not in globals(): raise NameError(\"BASE_TYPES_CODE string not found.\")\n",
    "        with open(base_types_path, \"w\", encoding=\"utf-8\") as f: f.write(BASE_TYPES_CODE)\n",
    "        logging.info(f\"Generated base types file: {base_types_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write {BASE_TYPES_FILENAME}: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    all_class_names = {map_uri_to_classname(uri) for uri in classes_info.keys()}\n",
    "    all_class_names.update(base_type_names)\n",
    "    all_class_uris = set(classes_info.keys()) # Needed for get_base_classes\n",
    "\n",
    "    generation_order_uris = sorted(list(classes_info.keys()))\n",
    "    logging.info(f\"Attempting generation for {len(generation_order_uris)} classes...\")\n",
    "\n",
    "    generated_files = 0\n",
    "    module_to_class_map: Dict[str, str] = {}\n",
    "\n",
    "    for class_uri in generation_order_uris:\n",
    "        if class_uri not in classes_info: continue\n",
    "        if class_uri in base_type_uris: continue # Skip base types\n",
    "\n",
    "        class_info = classes_info[class_uri]\n",
    "        class_name = map_uri_to_classname(class_uri)\n",
    "        module_name_part = map_uri_to_fieldname(class_uri)\n",
    "\n",
    "        try:\n",
    "            model_code = generate_pydantic_model_code(\n",
    "                class_info=class_info,\n",
    "                properties_info=properties_info,\n",
    "                all_class_names=all_class_names,\n",
    "                all_class_uris=all_class_uris,\n",
    "                schema_graph=schema_graph,\n",
    "                base_type_names=base_type_names\n",
    "            )\n",
    "            file_path = output_path / f\"{module_name_part}.py\"\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f: f.write(model_code)\n",
    "            module_to_class_map[module_name_part] = class_name\n",
    "            generated_files += 1\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate code for class {class_name} ({class_uri}): {e}\", exc_info=True)\n",
    "\n",
    "    logging.info(f\"Finished generating {generated_files} specific Pydantic model files in {output_path}\")\n",
    "\n",
    "    # --- Corrected __init__.py Generation ---\n",
    "    init_py_path = output_path / \"__init__.py\"\n",
    "    try:\n",
    "        with open(init_py_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# flake8: noqa\\n\")\n",
    "            f.write(\"# Auto-generated __init__.py\\n\\n\")\n",
    "            f.write(\"import logging\\nimport importlib\\nimport pkgutil\\n\")\n",
    "            f.write(\"from typing import TYPE_CHECKING\\nfrom pydantic import BaseModel\\n\\n\")\n",
    "            f.write(\"logger: logging.Logger = logging.getLogger(__name__)\\n\\n\")\n",
    "\n",
    "            if base_type_names:\n",
    "                f.write(\"# --- Import Base Types ---\\n\")\n",
    "                f.write(\"try:\\n\")\n",
    "                f.write(f\"    from .{BASE_TYPES_FILENAME[:-3]} import ({', '.join(sorted(list(base_type_names)))})\\n\")\n",
    "                f.write(\"except ImportError as e_base:\\n\") # Use specific exception variable\n",
    "                f.write(\"    logger.warning(f'Could not import base_types: {e_base}')\\n\\n\")\n",
    "\n",
    "            if module_to_class_map: f.write(\"# --- Import Generated Models ---\\n\")\n",
    "            generated_class_names = set()\n",
    "            for module_name_part, class_name in sorted(module_to_class_map.items()):\n",
    "                 if class_name not in base_type_names:\n",
    "                     f.write(f\"try:\\n\")\n",
    "                     f.write(f\"    from .{module_name_part} import {class_name}\\n\")\n",
    "                     generated_class_names.add(class_name)\n",
    "                     f.write(f\"except ImportError as e_mod:\\n\") # Use specific exception variable\n",
    "                     f.write(f\"    logger.warning(f'Could not import {class_name} from .{module_name_part}: {{e_mod}}')\\n\")\n",
    "\n",
    "            all_names = sorted(list(base_type_names | generated_class_names))\n",
    "            f.write(\"\\n__all__ = [\\n\")\n",
    "            for name in all_names: f.write(f'    \"{name}\",\\n')\n",
    "            f.write(\"]\\n\\n\")\n",
    "\n",
    "            f.write(\"# --- Rebuild models to resolve forward references ---\\n\")\n",
    "            f.write(\"def rebuild_all() -> None:\\n\")\n",
    "            f.write(\"    package_name = __name__\\n\")\n",
    "            f.write(\"    package = importlib.import_module(package_name)\\n\")\n",
    "            f.write(\"    rebuilt_models = set()\\n\")\n",
    "            f.write(\"    if not hasattr(package, '__path__'): return\\n\") # Ensure it's a package\n",
    "            f.write('    logger.debug(f\"Attempting model rebuild in {package_name}\")\\n\\n')\n",
    "            f.write(\"    for loader, module_name, is_pkg in pkgutil.iter_modules(package.__path__, package_name + '.') :\\n\") # Use correct iter_modules signature\n",
    "            f.write(\"        if is_pkg: continue # Don't try to rebuild packages like 'tests'\\n\")\n",
    "            f.write(\"        # Skip __init__ itself and base_types\\n\")\n",
    "            f.write(\"        if module_name.endswith('.__init__') or module_name.endswith('.base_types'):\\n\")\n",
    "            f.write(\"            continue\\n\")\n",
    "            f.write(\"        try:\\n\")\n",
    "            f.write(\"            module = importlib.import_module(module_name)\\n\")\n",
    "            f.write(\"            for attribute_name in dir(module):\\n\")\n",
    "            f.write(\"                try: # Add inner try/except for attribute access/check\\n\")\n",
    "            f.write(\"                    attribute = getattr(module, attribute_name)\\n\")\n",
    "            f.write(\"                    if (isinstance(attribute, type) and\\n\")\n",
    "            f.write(\"                            issubclass(attribute, BaseModel) and\\n\")\n",
    "            f.write(\"                            attribute is not BaseModel and\\n\")\n",
    "            f.write(\"                            hasattr(attribute, 'model_rebuild') and\\n\")\n",
    "            f.write(\"                            attribute not in rebuilt_models):\\n\")\n",
    "            f.write(\"                        try:\\n\")\n",
    "            f.write('                            logger.debug(f\"Rebuilding: {attribute.__name__} in {module_name}\")\\n')\n",
    "            f.write(\"                            attribute.model_rebuild(force=True)\\n\")\n",
    "            f.write(\"                            rebuilt_models.add(attribute)\\n\")\n",
    "            f.write(\"                        except Exception as e_rebuild:\\n\")\n",
    "            f.write(\"                            logger.error(f'Error rebuilding model {attribute.__name__} in {module_name}: {e_rebuild}', exc_info=False)\\n\")\n",
    "            f.write(\"                except Exception as e_getattr: # Catch errors during getattr/issubclass\\n\")\n",
    "            f.write(\"                     # logger.debug(f'Skipping attribute {attribute_name} in {module_name}: {e_getattr}')\\n\")\n",
    "            f.write(\"                     pass # Ignore attributes that cause errors during introspection\\n\")\n",
    "            f.write(\"        except ModuleNotFoundError:\\n\")\n",
    "            f.write(\"            logger.warning(f\\\"Module not found during rebuild: {module_name}\\\")\\n\")\n",
    "            f.write(\"        except Exception as e_import:\\n\")\n",
    "            f.write(\"             logger.error(f'Error processing module {module_name} during rebuild: {e_import}', exc_info=False)\\n\\n\")\n",
    "\n",
    "            f.write(\"# Run rebuild automatically on import\\n\")\n",
    "            f.write(\"try:\\n\")\n",
    "            f.write(\"    rebuild_all()\\n\")\n",
    "            f.write(\"    logger.info(f'Pydantic models in {__name__} package rebuilt.')\\n\")\n",
    "            f.write(\"except Exception as e_global:\\n\")\n",
    "            f.write(\"    logger.error(f'Global error during model rebuild: {e_global}', exc_info=True)\\n\")\n",
    "\n",
    "        logging.info(f\"Successfully generated __init__.py at {init_py_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write __init__.py: {e}\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b7cc2017-6b3f-4c26-a914-e0e8a77a64e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting Schema.org to Pydantic Conversion Process...\n",
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Found 628 potential schema.org classes.\n",
      "INFO:root:Found 921 potential schema.org properties.\n",
      "INFO:root:Analyzed 628 classes and 921 properties.\n",
      "INFO:root:Generated base types file: output_ontology/models/base_types.py\n",
      "INFO:root:Attempting generation for 628 classes...\n",
      "INFO:root:Finished generating 625 specific Pydantic model files in output_ontology/models\n",
      "INFO:root:Successfully generated __init__.py at output_ontology/models/__init__.py\n",
      "INFO:__main__:Ontology generation process finished.\n",
      "INFO:__main__:Generated models located in: output_ontology/models\n",
      "INFO:__main__:Proceed with Step 5: Post-Processing & Verification.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_type_uris: {rdflib.term.URIRef('https://schema.org/Distance'), rdflib.term.URIRef('https://schema.org/Duration'), rdflib.term.URIRef('https://schema.org/DefinedTerm'), rdflib.term.URIRef('https://schema.org/Money'), rdflib.term.URIRef('https://schema.org/Quantity')}\n",
      "base_type_names: {'Duration', 'Money', 'Quantity', 'Distance', 'DefinedTerm'}\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'LocationFeatureSpecification'\n",
      "type_hint_core: Union['BedDetails', 'BedType', str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[bool, str]\n",
      "type_hint_core: 'HowTo'\n",
      "type_hint_core: 'ActionStatusType'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Place', 'PostalAddress', str]\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union['EntryPoint', pydantic.AnyUrl]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime, datetime.time]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime, datetime.time]\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: 'Offer'\n",
      "type_hint_core: Union['MediaSubscription', bool]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['Demand', 'Offer']\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: int\n",
      "type_hint_core: int\n",
      "type_hint_core: 'BoardingPolicyType'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union['Comment', 'CreativeWork']\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['SpeakableSpecification', pydantic.AnyUrl]\n",
      "type_hint_core: int\n",
      "type_hint_core: 'Question'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'AdministrativeArea'\n",
      "type_hint_core: Union['MediaObject', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: float\n",
      "type_hint_core: Union['BedType', str]\n",
      "type_hint_core: 'BlogPosting'\n",
      "type_hint_core: 'BlogPosting'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'BookFormatType'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: str\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: Union['ImageObject', pydantic.AnyUrl]\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['BroadcastFrequencySpecification', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'CableOrSatelliteService'\n",
      "type_hint_core: 'BroadcastService'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: bool\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['BroadcastFrequencySpecification', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'BroadcastChannel'\n",
      "type_hint_core: 'BroadcastService'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['BusStation', 'BusStop']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['BusStation', 'BusStop']\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'WarrantyPromise'\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'CreativeWorkSeries'\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['Comment', 'CreativeWork']\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: int\n",
      "type_hint_core: 'Comment'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: 'Language'\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: 'UnitPriceSpecification'\n",
      "type_hint_core: 'ActionAccessSpecification'\n",
      "type_hint_core: 'Offer'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place', str]\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: 'ContactPointOption'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'OpeningHoursSpecification'\n",
      "type_hint_core: Union['Product', str]\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place']\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['FoodEstablishment', 'Place']\n",
      "type_hint_core: 'FoodEvent'\n",
      "type_hint_core: 'Recipe'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['AlignmentObject', 'Course', str]\n",
      "type_hint_core: 'CourseInstance'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'ItemList'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'MediaObject'\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: Union['AudioObject', 'Clip']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['CreativeWork', str]\n",
      "type_hint_core: 'Comment'\n",
      "type_hint_core: int\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['Rating', str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: float\n",
      "type_hint_core: 'Country'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'AlignmentObject'\n",
      "type_hint_core: Union[base_types.DefinedTerm, str]\n",
      "type_hint_core: 'MediaObject'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'MediaObject'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: 'InteractionCounter'\n",
      "type_hint_core: str\n",
      "type_hint_core: bool\n",
      "type_hint_core: Union['CreativeWork', 'Product', pydantic.AnyUrl]\n",
      "type_hint_core: Union['CreativeWork', 'Product', pydantic.AnyUrl]\n",
      "type_hint_core: bool\n",
      "type_hint_core: Union['CreativeWork', pydantic.AnyUrl]\n",
      "type_hint_core: Union[base_types.DefinedTerm, pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[base_types.DefinedTerm, str]\n",
      "type_hint_core: Union['CreativeWork', pydantic.AnyUrl]\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Product', pydantic.AnyUrl, str]\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Demand', 'Offer']\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'PublicationEvent'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['CreativeWork', pydantic.AnyUrl]\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'PublicationEvent'\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[datetime.datetime, str]\n",
      "type_hint_core: Union[datetime.datetime, pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'ImageObject'\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: Union['Clip', 'VideoObject']\n",
      "type_hint_core: int\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: int\n",
      "type_hint_core: 'CreativeWorkSeries'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'Dataset'\n",
      "type_hint_core: Union['DataFeedItem', 'Thing', str]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'DataCatalog'\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: 'DataDownload'\n",
      "type_hint_core: 'DataCatalog'\n",
      "type_hint_core: 'DataCatalog'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['MonetaryAmount', float]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union['Country', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'PostalCodeRangeSpecification'\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place', str]\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: 'ShippingDeliveryTime'\n",
      "type_hint_core: bool\n",
      "type_hint_core: 'DefinedRegion'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['LoanOrCredit', 'PaymentMethod', str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place', str]\n",
      "type_hint_core: 'ItemAvailability'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime, datetime.time]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime, datetime.time]\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: 'BusinessFunction'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'BusinessEntityType'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: 'PriceSpecification'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'TypeAndQuantityNode'\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'OfferItemCondition'\n",
      "type_hint_core: Union['AggregateOffer', 'CreativeWork', 'Event', 'MenuItem', 'Product', 'Service', 'Trip']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'PriceSpecification'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'WarrantyPromise'\n",
      "type_hint_core: 'DigitalDocumentPermission'\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: 'DigitalDocumentPermissionType'\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'PriceSpecification'\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['MonetaryAmount', 'PriceSpecification', float]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['QualitativeValue', pydantic.AnyUrl, str]\n",
      "type_hint_core: 'SoftwareApplication'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'SoftwareApplication'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'CreativeWorkSeries'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'EventStatusType'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: bool\n",
      "type_hint_core: Union[base_types.DefinedTerm, pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['Place', 'PostalAddress', str]\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['Demand', 'Offer']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: int\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: base_types.Distance\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'SportsActivityLocation'\n",
      "type_hint_core: 'SportsEvent'\n",
      "type_hint_core: 'SportsTeam'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['Vehicle', str]\n",
      "type_hint_core: 'Airport'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'BoardingPolicyType'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'Airport'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[base_types.Duration, str]\n",
      "type_hint_core: Union[base_types.Distance, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['QualitativeValue', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[bool, pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['Menu', pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['Menu', pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Rating'\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union['QuantitativeValue', int]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Place', 'PostalAddress', pydantic.AnyUrl]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'VideoGame'\n",
      "type_hint_core: int\n",
      "type_hint_core: 'GameServerStatus'\n",
      "type_hint_core: 'GeoCoordinates'\n",
      "type_hint_core: Union[base_types.Distance, float, str]\n",
      "type_hint_core: Union['PostalAddress', str]\n",
      "type_hint_core: Union['Country', str]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['PostalAddress', str]\n",
      "type_hint_core: Union['Country', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['BedDetails', 'BedType', str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union['MonetaryAmount', str]\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union['CreativeWork', 'HowToSection', 'HowToStep', str]\n",
      "type_hint_core: Union['CreativeWork', 'ItemList', str]\n",
      "type_hint_core: Union['HowToSupply', str]\n",
      "type_hint_core: Union['HowToTool', str]\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union['QuantitativeValue', str]\n",
      "type_hint_core: Union['MediaObject', pydantic.AnyUrl]\n",
      "type_hint_core: Union['MediaObject', pydantic.AnyUrl]\n",
      "type_hint_core: Union['MediaObject', pydantic.AnyUrl]\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union['HowToSupply', str]\n",
      "type_hint_core: Union['HowToTool', str]\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union['QuantitativeValue', float, str]\n",
      "type_hint_core: Union['CreativeWork', 'ItemList', str]\n",
      "type_hint_core: Union['MonetaryAmount', str]\n",
      "type_hint_core: Union['MediaObject', str]\n",
      "type_hint_core: Union['PropertyValue', str]\n",
      "type_hint_core: bool\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union['SoftwareApplication', 'WebSite']\n",
      "type_hint_core: 'Action'\n",
      "type_hint_core: Union['Place', 'PostalAddress', str]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['MonetaryAmount', float]\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: str\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['MonetaryAmount', 'PriceSpecification']\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union['PaymentMethod', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['PaymentStatusType', str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Order'\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: Union['MonetaryAmount', 'PriceSpecification']\n",
      "type_hint_core: Union['ListItem', 'Thing', str]\n",
      "type_hint_core: Union['ItemListOrderType', str]\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['MonetaryAmount', 'PriceSpecification', float]\n",
      "type_hint_core: str\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['MonetaryAmount', 'MonetaryAmountDistribution', float]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[base_types.DefinedTerm, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Occupation'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[base_types.DefinedTerm, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'ListItem'\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: 'ListItem'\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: 'BlogPosting'\n",
      "type_hint_core: Union['MonetaryAmount', float]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'OpeningHoursSpecification'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'LocationFeatureSpecification'\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union[bool, str]\n",
      "type_hint_core: 'Rating'\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['QualitativeValue', str]\n",
      "type_hint_core: Union['QuantitativeValue', int]\n",
      "type_hint_core: Union['QuantitativeValue', int]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'MapCategoryType'\n",
      "type_hint_core: 'NewsArticle'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['MediaSubscription', bool]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'Offer'\n",
      "type_hint_core: 'MenuItem'\n",
      "type_hint_core: 'MenuSection'\n",
      "type_hint_core: Union['MenuItem', 'MenuSection']\n",
      "type_hint_core: 'NutritionInformation'\n",
      "type_hint_core: Union['Demand', 'Offer']\n",
      "type_hint_core: 'RestrictedDiet'\n",
      "type_hint_core: 'MenuItem'\n",
      "type_hint_core: 'MenuSection'\n",
      "type_hint_core: Union['ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: Union['ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: Union['Audience', 'Organization', 'Person']\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union['StructuredValue', bool, float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Country'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: float\n",
      "type_hint_core: 'MusicAlbumProductionType'\n",
      "type_hint_core: 'MusicRelease'\n",
      "type_hint_core: 'MusicAlbumReleaseType'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'MusicComposition'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: 'MusicComposition'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'MusicRecording'\n",
      "type_hint_core: 'MusicAlbum'\n",
      "type_hint_core: 'MusicAlbum'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['ItemList', 'MusicRecording']\n",
      "type_hint_core: 'MusicRecording'\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['ItemList', 'MusicRecording']\n",
      "type_hint_core: 'MusicRecording'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: 'MusicAlbum'\n",
      "type_hint_core: 'MusicPlaylist'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'MusicComposition'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: 'MusicReleaseFormatType'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'MusicAlbum'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: str\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: base_types.Quantity\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['MonetaryAmount', 'MonetaryAmountDistribution', float]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'AdministrativeArea'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[base_types.DefinedTerm, str]\n",
      "type_hint_core: Union['LoanOrCredit', 'PaymentMethod', str]\n",
      "type_hint_core: 'Offer'\n",
      "type_hint_core: 'PropertyValue'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place', str]\n",
      "type_hint_core: 'ItemAvailability'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime, datetime.time]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime, datetime.time]\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: 'BusinessFunction'\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'BusinessEntityType'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: 'PriceSpecification'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'TypeAndQuantityNode'\n",
      "type_hint_core: Union['GeoShape', 'Place', str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: bool\n",
      "type_hint_core: 'OfferItemCondition'\n",
      "type_hint_core: Union['AggregateOffer', 'CreativeWork', 'Event', 'MenuItem', 'Product', 'Service', 'Trip']\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'PriceSpecification'\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'OfferShippingDetails'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'WarrantyPromise'\n",
      "type_hint_core: 'ShippingDeliveryTime'\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: bool\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: 'DefinedRegion'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'DefinedRegion'\n",
      "type_hint_core: 'MonetaryAmount'\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: str\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: datetime.time\n",
      "type_hint_core: 'DayOfWeek'\n",
      "type_hint_core: datetime.time\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'Offer'\n",
      "type_hint_core: 'PostalAddress'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: bool\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'ParcelDelivery'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'OrderStatus'\n",
      "type_hint_core: Union['OrderItem', 'Product', 'Service']\n",
      "type_hint_core: 'Invoice'\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union['PaymentMethod', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: 'ParcelDelivery'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'OrderStatus'\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union['OrderItem', 'Product', 'Service']\n",
      "type_hint_core: Union['LoanOrCredit', 'PaymentMethod', str]\n",
      "type_hint_core: Union['PostalAddress', str]\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Brand', 'Organization']\n",
      "type_hint_core: 'ContactPoint'\n",
      "type_hint_core: 'ContactPoint'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'OfferCatalog'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[base_types.DefinedTerm, pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Place', 'PostalAddress', str]\n",
      "type_hint_core: Union['ImageObject', pydantic.AnyUrl]\n",
      "type_hint_core: 'Offer'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'ProgramMembership']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['OwnershipInfo', 'Product']\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['CreativeWork', pydantic.AnyUrl]\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: 'Demand'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place']\n",
      "type_hint_core: Union[base_types.DefinedTerm, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: float\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: Union['Product', 'Service']\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'PostalAddress'\n",
      "type_hint_core: 'DeliveryEvent'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: 'Product'\n",
      "type_hint_core: 'PostalAddress'\n",
      "type_hint_core: 'Order'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: 'PaymentMethod'\n",
      "type_hint_core: str\n",
      "type_hint_core: int\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['GenderType', str]\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: 'EntertainmentBusiness'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'Service'\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'AdministrativeArea'\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['PostalAddress', str]\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['EducationalOrganization', 'Organization']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['Brand', 'Organization']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['Person', pydantic.AnyUrl]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'ContactPoint'\n",
      "type_hint_core: 'ContactPoint'\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['GenderType', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Occupation'\n",
      "type_hint_core: 'OfferCatalog'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: Union['ContactPoint', 'Place']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Offer'\n",
      "type_hint_core: Union['Organization', 'ProgramMembership']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Country'\n",
      "type_hint_core: Union['MonetaryAmount', 'PriceSpecification']\n",
      "type_hint_core: Union['OwnershipInfo', 'Product']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: Union['CreativeWork', pydantic.AnyUrl]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Demand'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union[base_types.DefinedTerm, str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['ContactPoint', 'Place']\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'PropertyValue'\n",
      "type_hint_core: Union['PostalAddress', str]\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: 'LocationFeatureSpecification'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['GeoCoordinates', 'GeoShape']\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Map', pydantic.AnyUrl]\n",
      "type_hint_core: bool\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[base_types.DefinedTerm, pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: Union['ImageObject', pydantic.AnyUrl]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: int\n",
      "type_hint_core: 'OpeningHoursSpecification'\n",
      "type_hint_core: Union['ImageObject', 'Photograph']\n",
      "type_hint_core: Union['ImageObject', 'Photograph']\n",
      "type_hint_core: bool\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: str\n",
      "type_hint_core: bool\n",
      "type_hint_core: 'OpeningHoursSpecification'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: 'Event'\n",
      "type_hint_core: Union['Country', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'PriceSpecification'\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: bool\n",
      "type_hint_core: 'PropertyValue'\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Brand', 'Organization']\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Country'\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: 'Product'\n",
      "type_hint_core: 'Product'\n",
      "type_hint_core: bool\n",
      "type_hint_core: Union['Product', 'Service']\n",
      "type_hint_core: Union['Product', 'Service']\n",
      "type_hint_core: 'OfferItemCondition'\n",
      "type_hint_core: Union[base_types.DefinedTerm, pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['ImageObject', pydantic.AnyUrl]\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['Product', pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['ProductModel', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Demand', 'Offer']\n",
      "type_hint_core: str\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: 'ProductModel'\n",
      "type_hint_core: 'ProductModel'\n",
      "type_hint_core: 'ProductModel'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['StructuredValue', bool, float, str]\n",
      "type_hint_core: Union['Enumeration', 'PropertyValue', 'QualitativeValue', 'QuantitativeValue', 'StructuredValue']\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: bool\n",
      "type_hint_core: bool\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: bool\n",
      "type_hint_core: bool\n",
      "type_hint_core: 'BroadcastService'\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: 'PropertyValue'\n",
      "type_hint_core: 'QualitativeValue'\n",
      "type_hint_core: 'QualitativeValue'\n",
      "type_hint_core: 'QualitativeValue'\n",
      "type_hint_core: 'QualitativeValue'\n",
      "type_hint_core: 'QualitativeValue'\n",
      "type_hint_core: 'QualitativeValue'\n",
      "type_hint_core: Union['Enumeration', 'PropertyValue', 'QualitativeValue', 'QuantitativeValue', 'StructuredValue']\n",
      "type_hint_core: 'PropertyValue'\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['StructuredValue', bool, float, str]\n",
      "type_hint_core: Union['Enumeration', 'PropertyValue', 'QualitativeValue', 'QuantitativeValue', 'StructuredValue']\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: float\n",
      "type_hint_core: Union['Answer', 'ItemList']\n",
      "type_hint_core: int\n",
      "type_hint_core: Union['Comment', 'CreativeWork']\n",
      "type_hint_core: Union['Answer', 'ItemList']\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: int\n",
      "type_hint_core: int\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['CreativeWorkSeason', pydantic.AnyUrl]\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: Union['Audience', 'Organization', 'Person']\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'NutritionInformation'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['CreativeWork', 'ItemList', str]\n",
      "type_hint_core: Union['QuantitativeValue', str]\n",
      "type_hint_core: 'RestrictedDiet'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'RealEstateAgent'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'Comment'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: str\n",
      "type_hint_core: 'ProgramMembership'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'ReservationStatusType'\n",
      "type_hint_core: 'Ticket'\n",
      "type_hint_core: Union['PriceSpecification', float, str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Reservation'\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Rating'\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: float\n",
      "type_hint_core: 'Comment'\n",
      "type_hint_core: 'RsvpResponseType'\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Movie'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['QualitativeValue', str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'WarrantyPromise'\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: 'AggregateRating'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place', str]\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: 'ServiceChannel'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Brand', 'Organization']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Thing', str]\n",
      "type_hint_core: 'OfferCatalog'\n",
      "type_hint_core: 'OpeningHoursSpecification'\n",
      "type_hint_core: Union['Product', 'Service']\n",
      "type_hint_core: Union['Product', 'Service']\n",
      "type_hint_core: Union['ImageObject', pydantic.AnyUrl]\n",
      "type_hint_core: Union['Demand', 'Offer']\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Review'\n",
      "type_hint_core: Union['AdministrativeArea', 'GeoShape', 'Place']\n",
      "type_hint_core: 'Audience'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: base_types.Duration\n",
      "type_hint_core: 'Service'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'ContactPoint'\n",
      "type_hint_core: 'PostalAddress'\n",
      "type_hint_core: 'ContactPoint'\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: 'OpeningHoursSpecification'\n",
      "type_hint_core: datetime.time\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: bool\n",
      "type_hint_core: Union['DeliveryChargeSpecification', 'MonetaryAmount']\n",
      "type_hint_core: bool\n",
      "type_hint_core: 'DefinedRegion'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'MonetaryAmount'\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['ImageObject', pydantic.AnyUrl]\n",
      "type_hint_core: 'SoftwareApplication'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'DataFeed'\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['ComputerLanguage', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'SoftwareApplication'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Person', 'SportsTeam']\n",
      "type_hint_core: Union['Person', 'SportsTeam']\n",
      "type_hint_core: Union['Person', 'SportsTeam']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['BedDetails', 'BedType', str]\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'TVSeries'\n",
      "type_hint_core: 'Country'\n",
      "type_hint_core: 'TVSeries'\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: 'Country'\n",
      "type_hint_core: 'TVSeries'\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'Country'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: int\n",
      "type_hint_core: int\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: Union['CreativeWorkSeason', pydantic.AnyUrl]\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: Union['QuantitativeValue', int]\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: datetime.datetime\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['TextObject', str]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['PropertyValue', pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['ImageObject', pydantic.AnyUrl]\n",
      "type_hint_core: Union['CreativeWork', pydantic.AnyUrl]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Action'\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union['CreativeWork', 'Event']\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'Seat'\n",
      "type_hint_core: Union['PriceSpecification', float, str]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: Union['Audience', 'ContactPoint', 'Organization', 'Person']\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: Union['Audience', str]\n",
      "type_hint_core: 'DeliveryMethod'\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'PriceSpecification'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'TrainStation'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'TrainStation'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: base_types.Distance\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union[datetime.datetime, datetime.time]\n",
      "type_hint_core: Union['Demand', 'Offer']\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'Place'\n",
      "type_hint_core: float\n",
      "type_hint_core: 'BusinessFunction'\n",
      "type_hint_core: Union['Product', 'Service']\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: float\n",
      "type_hint_core: str\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[datetime.date, datetime.datetime]\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: Union['DriveWheelConfigurationValue', str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['QualitativeValue', pydantic.AnyUrl, str]\n",
      "type_hint_core: str\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union[float, str]\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'SteeringPositionValue'\n",
      "type_hint_core: str\n",
      "type_hint_core: 'EngineSpecification'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: Union['QuantitativeValue', float]\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['QualitativeValue', pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['Thing', pydantic.AnyUrl, str]\n",
      "type_hint_core: 'GameServer'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: 'GamePlayMode'\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: 'CreativeWork'\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: 'Episode'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['Place', 'PostalAddress', pydantic.AnyUrl]\n",
      "type_hint_core: Union['Thing', pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: int\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: int\n",
      "type_hint_core: 'GamePlayMode'\n",
      "type_hint_core: 'Organization'\n",
      "type_hint_core: 'Thing'\n",
      "type_hint_core: Union['CreativeWorkSeason', pydantic.AnyUrl]\n",
      "type_hint_core: 'CreativeWorkSeason'\n",
      "type_hint_core: 'VideoObject'\n",
      "type_hint_core: Union['PerformingGroup', 'Person']\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['MediaObject', str]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['MusicGroup', 'Person']\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: Union[int, str]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: Union[pydantic.AnyUrl, str]\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: Union['QuantitativeValue', base_types.Distance]\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: 'QuantitativeValue'\n",
      "type_hint_core: 'WarrantyScope'\n",
      "type_hint_core: str\n",
      "type_hint_core: Union['BreadcrumbList', str]\n",
      "type_hint_core: datetime.date\n",
      "type_hint_core: 'WebPageElement'\n",
      "type_hint_core: 'ImageObject'\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union['Organization', 'Person']\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: pydantic.AnyUrl\n",
      "type_hint_core: Union['SpeakableSpecification', pydantic.AnyUrl]\n",
      "type_hint_core: 'Specialty'\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: str\n",
      "type_hint_core: 'Person'\n",
      "type_hint_core: Union['Language', str]\n",
      "type_hint_core: 'Language'\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume parse_schema_to_graph and analyze_schema_graph are defined correctly above\n",
    "    # Also assume mapping helpers are defined (map_uri_to_classname etc.)\n",
    "\n",
    "    logger.info(\"Starting Schema.org to Pydantic Conversion Process...\")\n",
    "\n",
    "    schema_graph_main = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "    if not schema_graph_main:\n",
    "        logger.critical(\"Failed to parse schema graph. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    analyzed_schema_main = analyze_schema_graph(schema_graph_main)\n",
    "    if not analyzed_schema_main or not analyzed_schema_main.get(\"classes\"):\n",
    "        logger.critical(\"Schema analysis failed or yielded no classes. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    properties_info_main: Dict[rdflib.URIRef, PropertyInfo] = analyzed_schema_main.get(\"properties\", {})\n",
    "    if not properties_info_main:\n",
    "        logger.warning(\"No property information found during analysis.\")\n",
    "        # Continue, models will just lack fields\n",
    "    # print(f\"Analyzed Schema Main: {analyzed_schema_main}\")\n",
    "\n",
    "    try:\n",
    "         generate_ontology_models(\n",
    "             analyzed_schema=analyzed_schema_main,\n",
    "             output_base_dir=OUTPUT_DIR,\n",
    "             models_subdir=MODELS_SUBDIR,\n",
    "             # Pass the required arguments\n",
    "             schema_graph=schema_graph_main,\n",
    "             properties_info=properties_info_main\n",
    "         )\n",
    "         logger.info(\"Ontology generation process finished.\")\n",
    "         logger.info(f\"Generated models located in: {pathlib.Path(OUTPUT_DIR) / MODELS_SUBDIR}\")\n",
    "         logger.info(\"Proceed with Step 5: Post-Processing & Verification.\")\n",
    "    except Exception as e:\n",
    "         logger.critical(f\"Ontology generation failed: {e}\", exc_info=True)\n",
    "         exit(1)\n",
    "\n",
    "    # Step 5 (Post-Processing) would typically be run after this script finishes\n",
    "    # e.g., by calling run_post_processing_pipeline() if it's defined here,\n",
    "    # or by running the shell script / commands separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de440239-5e8a-405e-b7ef-ea4da815e51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac1563-8ef9-4bfd-b00c-36c490508d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98744b-3744-4bab-8bdc-86dafe6515d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7d5db-ac6d-46ac-b691-573e208ba64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffd56c-c9b2-4ccb-a4d2-5594c2efb3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba6aba-a924-4df4-ac5b-f19ffe43d512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e00a6-3af2-4653-9939-55b1a67a51d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137bc21-52de-4cca-8ea5-bd47db49210f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ae5b1-f9ad-415d-95f7-79c71b57f5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e31de78f-65a1-41bf-bf73-80f064269169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, ForwardRef, Any, cast, TYPE_CHECKING # Added TYPE_CHECKING\n",
    "from pydantic import BaseModel, Field, EmailStr, AnyUrl\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import isodate\n",
    "import decimal\n",
    "import keyword\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "\n",
    "# --- Assume functions and NamedTuples from Chunks 1, 2, 3 are available ---\n",
    "# parse_schema_to_graph, analyze_schema_graph\n",
    "# PropertyInfo, ClassInfo\n",
    "# TYPE_MAP, PROPERTY_ALIAS_MAP\n",
    "# safe_python_identifier, map_uri_to_classname, map_uri_to_fieldname,\n",
    "# map_range_to_typehint, get_field_metadata, get_base_classes\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"output_ontology\"\n",
    "MODELS_SUBDIR = \"models\"\n",
    "BASE_ONTOLOGY_MODULE = \"core_ontology_v0_1\" # Used for potential future imports if split\n",
    "\n",
    "# --- Helper for Code Generation ---\n",
    "\n",
    "# --- Helper to get import paths (adjust module structure if needed) ---\n",
    "def get_module_path_for_class(class_name: str) -> str:\n",
    "    \"\"\"Determines the expected module name for a given class name.\"\"\"\n",
    "    # Assumes snake_case filename based on class name\n",
    "    # This might need adjustment if filename generation logic changes\n",
    "    potential_field_name = map_uri_to_fieldname(SCHEMA[class_name]) # Hacky way to get snake_case\n",
    "    return f\".{safe_python_identifier(potential_field_name)}\" # Relative import\n",
    "\n",
    "# --- Revised Code Generation Function ---\n",
    "\n",
    "# def generate_pydantic_model_code(\n",
    "#     class_info: ClassInfo,\n",
    "#     properties_info: Dict[rdflib.URIRef, PropertyInfo],\n",
    "#     all_class_names: Set[str] # All class names being generated\n",
    "# ) -> str:\n",
    "#     \"\"\"Generates the Python code string for a single Pydantic model, including imports.\"\"\"\n",
    "\n",
    "#     class_name = map_uri_to_classname(class_info.uri)\n",
    "#     base_uris = class_info.superclasses\n",
    "#     # Map base URIs to class names, filtering for known/generated classes\n",
    "#     potential_base_names = {map_uri_to_classname(uri) for uri in base_uris}\n",
    "#     valid_base_names = sorted([name for name in potential_base_names if name in all_class_names and name != 'Thing'])\n",
    "\n",
    "#     if not valid_base_names:\n",
    "#          base_class_str = \"BaseModel\" # Inherit directly from BaseModel\n",
    "#          imports = {\"from pydantic import BaseModel, Field\"}\n",
    "#     else:\n",
    "#          base_class_str = \", \".join(valid_base_names)\n",
    "#          imports = {\"from pydantic import BaseModel, Field\"}\n",
    "#          # Add imports for base classes (might need TYPE_CHECKING block)\n",
    "#          for base_name in valid_base_names:\n",
    "#              imports.add(f\"from {get_module_path_for_class(base_name)} import {base_name}\")\n",
    "\n",
    "\n",
    "#     # Standard library imports potentially needed by type hints\n",
    "#     std_imports = set()\n",
    "#     typing_imports = {\"Optional\", \"List\", \"Union\", \"Any\", \"TYPE_CHECKING\"} # Always import TYPE_CHECKING now\n",
    "\n",
    "#     field_definitions = []\n",
    "#     field_type_imports = set() # Track imports needed for field types\n",
    "\n",
    "#     sorted_property_uris = sorted(list(class_info.properties))\n",
    "\n",
    "#     for prop_uri in sorted_property_uris:\n",
    "#         # ... (rest of property analysis and field generation logic is largely the same as before) ...\n",
    "#         if prop_uri not in properties_info:\n",
    "#             logging.warning(f\"Property {prop_uri} used by {class_name} not found. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         prop_info = properties_info[prop_uri]\n",
    "#         field_name = map_uri_to_fieldname(prop_info.uri)\n",
    "#         type_hint_str = map_range_to_typehint(prop_info.ranges, all_class_names)\n",
    "\n",
    "#         # Track imports needed based on type hint\n",
    "#         if \"datetime.\" in type_hint_str: std_imports.add(\"import datetime\")\n",
    "#         if \"decimal.\" in type_hint_str: std_imports.add(\"import decimal\")\n",
    "#         if \"timedelta\" in type_hint_str: std_imports.add(\"from datetime import timedelta\") # Specifically timedelta\n",
    "#         if \"pydantic.\" in type_hint_str:\n",
    "#             if \"AnyUrl\" in type_hint_str: imports.add(\"from pydantic import AnyUrl\")\n",
    "#             if \"EmailStr\" in type_hint_str: imports.add(\"from pydantic import EmailStr\")\n",
    "#             # Potentially add others like ConstrainedStr if defined via pydantic\n",
    "#         for t in typing_imports:\n",
    "#              # Check if typing constructs are actually used in the final hint string\n",
    "#              # Basic check, might need refinement\n",
    "#              if t in type_hint_str and t != 'TYPE_CHECKING': # Don't add TYPE_CHECKING to main import list\n",
    "#                  std_imports.add(f\"from typing import {t}\")\n",
    "\n",
    "#         # Track potential forward references / cross-module imports needed for type hints\n",
    "#         # Extract potential class names from the type hint string (simple regex approach)\n",
    "#         # A more robust way involves analyzing the mapped types *before* creating the string\n",
    "#         potential_classes_in_hint = set(re.findall(r\"'(\\w+)'\", type_hint_str))\n",
    "#         for potential_class in potential_classes_in_hint:\n",
    "#              if potential_class in all_class_names and potential_class != class_name:\n",
    "#                  field_type_imports.add(f\"from {get_module_path_for_class(potential_class)} import {potential_class}\")\n",
    "\n",
    "\n",
    "#         field_args_dict = get_field_metadata(prop_info)\n",
    "#         field_args_parts = []\n",
    "#         default_val_repr = repr(field_args_dict.pop('default', None))\n",
    "#         field_args_parts.append(default_val_repr)\n",
    "#         field_args_parts.extend(f\"{k}={repr(v)}\" for k, v in field_args_dict.items())\n",
    "#         field_call = f\"Field({', '.join(field_args_parts)})\"\n",
    "\n",
    "#         field_definitions.append(f\"    {field_name}: {type_hint_str} = {field_call}\")\n",
    "\n",
    "#     # --- Assemble the full class code ---\n",
    "#     code_parts = []\n",
    "\n",
    "#     # Add __future__ import first\n",
    "#     code_parts.append(\"from __future__ import annotations\")\n",
    "\n",
    "#     # Add standard library imports\n",
    "#     code_parts.extend(sorted(list(std_imports)))\n",
    "\n",
    "#     # Add base pydantic and potentially other direct imports\n",
    "#     code_parts.extend(sorted([imp for imp in imports if not imp.startswith(\"from .\")]))\n",
    "\n",
    "#     # Add forward reference imports within TYPE_CHECKING block\n",
    "#     if field_type_imports or any(imp.startswith(\"from .\") for imp in imports):\n",
    "#         code_parts.append(\"\\nif TYPE_CHECKING:\")\n",
    "#         # Add base class imports if they are from other modules\n",
    "#         for imp in sorted(list(imports)):\n",
    "#              if imp.startswith(\"from .\"):\n",
    "#                  code_parts.append(f\"    {imp}\")\n",
    "#         # Add field type imports\n",
    "#         for imp in sorted(list(field_type_imports)):\n",
    "#              code_parts.append(f\"    {imp}\")\n",
    "#         code_parts.append(\"\\n\")\n",
    "\n",
    "\n",
    "#     # Add class definition\n",
    "#     class_docstring = f'\"\"\"\\n    {class_name}: {textwrap.shorten(class_info.comment or \"No description provided.\", width=70)}\\n\\n    Generated from: {class_info.uri}\\n    \"\"\"'\n",
    "#     # Ensure base classes needed at runtime are imported directly if possible\n",
    "#     # This logic might need refinement - base classes might need direct import\n",
    "#     # outside TYPE_CHECKING if Python's MRO needs them immediately.\n",
    "#     # For now, assume TYPE_CHECKING handles most cases for hints + model_rebuild.\n",
    "#     runtime_base_imports = []\n",
    "#     actual_base_classes_str = base_class_str\n",
    "#     # A simple attempt: if inheriting directly from BaseModel no runtime import needed here\n",
    "#     # If inheriting from others, assume they are imported via __init__.py / TYPE_CHECKING + model_rebuild\n",
    "#     # This part is tricky without full MRO analysis during generation.\n",
    "\n",
    "\n",
    "#     code_parts.append(f\"class {class_name}({actual_base_classes_str}):\")\n",
    "#     code_parts.append(f\"    {class_docstring}\")\n",
    "\n",
    "#     if not field_definitions:\n",
    "#         code_parts.append(\"    pass\")\n",
    "#     else:\n",
    "#         code_parts.extend(field_definitions)\n",
    "\n",
    "#     code_parts.append(\"\\n    model_config = {'extra': 'forbid'}\")\n",
    "\n",
    "#     return \"\\n\".join(code_parts) + \"\\n\"\n",
    "\n",
    "\n",
    "# def generate_ontology_models(\n",
    "#      analyzed_schema: Dict[str, Dict],\n",
    "#      output_base_dir: str,\n",
    "#      models_subdir: str = \"models\"\n",
    "#      ):\n",
    "#     \"\"\"Generates Pydantic model files from analyzed schema info.\"\"\"\n",
    "\n",
    "#     classes_info: Dict[rdflib.URIRef, ClassInfo] = analyzed_schema.get(\"classes\", {})\n",
    "#     properties_info: Dict[rdflib.URIRef, PropertyInfo] = analyzed_schema.get(\"properties\", {})\n",
    "\n",
    "#     if not classes_info or not properties_info:\n",
    "#         logging.error(\"No class or property information found in analyzed schema.\")\n",
    "#         return\n",
    "\n",
    "#     # Create output directory\n",
    "#     output_path = pathlib.Path(output_base_dir) / models_subdir\n",
    "#     output_path.mkdir(parents=True, exist_ok=True)\n",
    "#     # Create __init__.py file\n",
    "#     (output_path / \"__init__.py\").touch()\n",
    "\n",
    "#     # Get all class names we intend to generate for forward ref handling\n",
    "#     all_class_names = {map_uri_to_classname(uri) for uri in classes_info.keys()}\n",
    "\n",
    "#     # Simple approach to handle inheritance order: generate multiple times or sort topologically.\n",
    "#     # For simplicity here, we'll just iterate. Multiple passes might be needed in a robust tool\n",
    "#     # or a proper topological sort based on the 'superclasses' links.\n",
    "#     generated_files = 0\n",
    "#     for class_uri, class_info in classes_info.items():\n",
    "#         class_name = map_uri_to_classname(class_uri)\n",
    "#         try:\n",
    "#             model_code = generate_pydantic_model_code(class_info, properties_info, all_class_names)\n",
    "\n",
    "#             # Write to file\n",
    "#             file_path = output_path / f\"{map_uri_to_fieldname(class_uri)}.py\" # Use snake_case for filenames\n",
    "#             with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(model_code)\n",
    "#             generated_files += 1\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Failed to generate code for class {class_name} ({class_uri}): {e}\")\n",
    "\n",
    "#     logging.info(f\"Generated {generated_files} Pydantic model files in {output_path}\")\n",
    "\n",
    "#     # Add model_rebuild calls to __init__.py for resolving ForwardRefs\n",
    "#     # This is crucial if models have circular dependencies\n",
    "#     init_py_path = output_path / \"__init__.py\"\n",
    "#     with open(init_py_path, \"a\", encoding=\"utf-8\") as f:\n",
    "#         f.write(\"\\n# --- Rebuild models to resolve forward references ---\\n\")\n",
    "#         f.write(\"import importlib\\n\")\n",
    "#         f.write(\"import pkgutil\\n\")\n",
    "#         f.write(\"import logging\\n\\n\")\n",
    "#         f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "#         f.write(\"def rebuild_all():\\n\")\n",
    "#         f.write(\"    package_name = __name__\\n\")\n",
    "#         f.write(\"    package = importlib.import_module(package_name)\\n\")\n",
    "#         f.write(\"    for _, module_name, _ in pkgutil.iter_modules(package.__path__, package_name + '.') :\\n\")\n",
    "#         f.write(\"        try:\\n\")\n",
    "#         f.write(\"            module = importlib.import_module(module_name)\\n\")\n",
    "#         f.write(\"            for attribute_name in dir(module):\\n\")\n",
    "#         f.write(\"                attribute = getattr(module, attribute_name)\\n\")\n",
    "#         f.write(\"                if isinstance(attribute, type) and issubclass(attribute, BaseModel) and attribute is not BaseModel:\\n\")\n",
    "#         f.write(\"                    try:\\n\")\n",
    "#         f.write(\"                        attribute.model_rebuild(force=True)\\n\")\n",
    "#         f.write(\"                        # logger.debug(f'Rebuilt model {attribute.__name__}')\\n\")\n",
    "#         f.write(\"                    except Exception as e_rebuild:\\n\")\n",
    "#         f.write(\"                        logger.error(f'Error rebuilding model {attribute.__name__} in {module_name}: {e_rebuild}', exc_info=True)\\n\")\n",
    "#         f.write(\"        except Exception as e_import:\\n\")\n",
    "#         f.write(\"             logger.error(f'Error importing module {module_name}: {e_import}', exc_info=True)\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "#         f.write(\"try:\\n\")\n",
    "#         f.write(\"    from pydantic import BaseModel\\n\")\n",
    "#         f.write(\"    rebuild_all()\\n\")\n",
    "#         f.write(\"    logger.info(f'Pydantic models in {__name__} rebuilt successfully.')\\n\")\n",
    "#         f.write(\"except ImportError:\\n\")\n",
    "#         f.write(\"    logger.warning('Pydantic not installed, skipping model rebuild.')\\n\")\n",
    "#         f.write(\"except Exception as e_global:\\n\")\n",
    "#         f.write(\"    logger.error(f'Global error during model rebuild: {e_global}', exc_info=True)\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50067b-9575-4ab9-8c52-378d9eff6809",
   "metadata": {},
   "source": [
    "# Attempt V3: Not Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d6e7bd86-9c40-4e0b-aefb-01b35ff1083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure these imports are present at the top of your script\n",
    "import rdflib\n",
    "from rdflib.namespace import RDFS, OWL\n",
    "from typing import List, Set\n",
    "import logging\n",
    "\n",
    "# Assume SCHEMA namespace and map_uri_to_classname helper are defined\n",
    "# Assume ClassInfo NamedTuple is defined\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_all_ancestors(graph: rdflib.Graph, class_uri: rdflib.URIRef, known_classes_uris: Set[rdflib.URIRef]) -> Set[rdflib.URIRef]:\n",
    "    \"\"\"Recursively find all superclass URIs for a given class URI within our known set.\"\"\"\n",
    "    ancestors = set()\n",
    "    parents = set(graph.objects(subject=class_uri, predicate=RDFS.subClassOf))\n",
    "    for parent_uri in parents:\n",
    "        if parent_uri in known_classes_uris and parent_uri not in [RDFS.Resource, OWL.Thing]:\n",
    "            if parent_uri not in ancestors: # Avoid infinite loops\n",
    "                ancestors.add(parent_uri)\n",
    "                ancestors.update(get_all_ancestors(graph, parent_uri, known_classes_uris))\n",
    "    return ancestors\n",
    "\n",
    "def get_base_classes(\n",
    "     class_info: ClassInfo,\n",
    "     all_class_names: Set[str], # Names are used for filtering potentially? Keep for now.\n",
    "     all_class_uris: Set[rdflib.URIRef], # URIs of all generated classes\n",
    "     graph: rdflib.Graph\n",
    "     ) -> List[str]:\n",
    "    \"\"\"Determines the most specific base classes for a Pydantic model, pruning redundant ancestors.\"\"\"\n",
    "    # Find direct superclass URIs that are part of the set we are generating\n",
    "    direct_superclass_uris = {sup for sup in class_info.superclasses if sup in all_class_uris and sup not in [RDFS.Resource, OWL.Thing]}\n",
    "\n",
    "    if not direct_superclass_uris:\n",
    "        return [\"BaseModel\"]\n",
    "\n",
    "    # Find all known ancestors for each direct superclass\n",
    "    ancestor_map = {sup: get_all_ancestors(graph, sup, all_class_uris) for sup in direct_superclass_uris}\n",
    "\n",
    "    # Pruning logic: Keep base 'B' if it is not an ancestor of any *other* direct base 'A'.\n",
    "    minimal_bases_uris = set()\n",
    "    for potential_base in direct_superclass_uris:\n",
    "        is_ancestor_of_another = False\n",
    "        for other_base in direct_superclass_uris:\n",
    "            if potential_base != other_base and potential_base in ancestor_map.get(other_base, set()):\n",
    "                is_ancestor_of_another = True\n",
    "                break\n",
    "        if not is_ancestor_of_another:\n",
    "            minimal_bases_uris.add(potential_base)\n",
    "\n",
    "    if not minimal_bases_uris:\n",
    "         logger.warning(f\"Could not determine minimal bases for {class_info.uri} from {direct_superclass_uris}, defaulting to BaseModel.\")\n",
    "         return [\"BaseModel\"]\n",
    "    else:\n",
    "         # Map the minimal URIs to class names\n",
    "         base_class_names = sorted([map_uri_to_classname(uri) for uri in minimal_bases_uris])\n",
    "         # Ensure BaseModel is implicitly handled by Pydantic's MRO via the listed bases\n",
    "         return base_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "571bebd1-86da-4a71-a983-b2da7d62e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working: Assumes necessary imports (typing, pydantic, datetime, etc.) are handled correctly\n",
    "# Assumes helper functions map_*, get_metadata, get_module_path are defined\n",
    "\n",
    "def generate_pydantic_model_code(\n",
    "    class_info: ClassInfo,\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo],\n",
    "    all_class_names: Set[str], # All generated class names (including base types)\n",
    "    # *** ADDED ARGUMENTS ***\n",
    "    all_class_uris: Set[rdflib.URIRef], # All generated class URIs\n",
    "    schema_graph: rdflib.Graph,      # The parsed RDF graph\n",
    "    base_type_names: Set[str]       # Names defined in base_types.py\n",
    ") -> str:\n",
    "    \"\"\"Generates the Python code string for a single Pydantic model, including robust imports and correct base classes.\"\"\"\n",
    "\n",
    "    class_name = map_uri_to_classname(class_info.uri)\n",
    "\n",
    "    # --- *** MODIFICATION: Call get_base_classes *** ---\n",
    "    valid_base_names_list = get_base_classes(class_info, all_class_names, all_class_uris, schema_graph)\n",
    "    # --- *** END MODIFICATION *** ---\n",
    "\n",
    "    # --- Import Handling Setup ---\n",
    "    typing_imports_needed = set()\n",
    "    pydantic_imports_needed = set()\n",
    "    datetime_imports_needed = set()\n",
    "    other_stdlib_imports = set()\n",
    "    cross_module_imports = set() # For TYPE_CHECKING block (forward refs in hints)\n",
    "    runtime_base_class_imports = set() # For class definition line imports\n",
    "    rich_type_imports = set() # For base_types like Quantity\n",
    "\n",
    "    # Determine base class string and imports needed for bases\n",
    "    if not valid_base_names_list or valid_base_names_list == [\"BaseModel\"]:\n",
    "        base_class_str = \"BaseModel\"\n",
    "        pydantic_imports_needed.add(\"BaseModel\")\n",
    "    else:\n",
    "        base_class_str = \", \".join(valid_base_names_list)\n",
    "        # Add imports needed for the base classes themselves at runtime\n",
    "        for base_name in valid_base_names_list:\n",
    "             if base_name != \"BaseModel\": # Prevent importing BaseModel relatively\n",
    "                  if base_name != class_name:\n",
    "                       runtime_base_class_imports.add(f\"from .{get_module_path_for_class(base_name)} import {base_name}\")\n",
    "                  else:\n",
    "                       logger.warning(f\"Class {class_name} listed itself as a base? Skipping runtime import.\")\n",
    "\n",
    "    # --- Analyze Fields (Populate field_definitions and track imports) ---\n",
    "    # (This part remains the same as the previous working version that fixed F401 errors)\n",
    "    field_definitions = []\n",
    "    field_added = False\n",
    "\n",
    "    sorted_property_uris = sorted(list(class_info.properties))\n",
    "\n",
    "    for prop_uri in sorted_property_uris:\n",
    "        if prop_uri not in properties_info: continue\n",
    "\n",
    "        prop_info = properties_info[prop_uri]\n",
    "        field_name = map_uri_to_fieldname(prop_info.uri)\n",
    "        # Pass base_type_names to the mapping function now\n",
    "        type_hint_str = map_range_to_typehint(prop_info.ranges, all_class_names, base_type_names)\n",
    "\n",
    "        # Clean prefixes and track actual usage for imports\n",
    "        final_type_hint = type_hint_str # Start with original\n",
    "        # Basic prefix removal - assumes map_range_to_typehint handles base_types prefix correctly\n",
    "        final_type_hint = final_type_hint.replace(\"typing.\", \"\")\n",
    "        final_type_hint = final_type_hint.replace(\"datetime.\", \"\")\n",
    "        final_type_hint = final_type_hint.replace(\"pydantic.\", \"\")\n",
    "\n",
    "        # Track imports based on final usage\n",
    "        if \"Optional\" in final_type_hint: typing_imports_needed.add(\"Optional\")\n",
    "        if \"List\" in final_type_hint: typing_imports_needed.add(\"List\")\n",
    "        if \"Union\" in final_type_hint: typing_imports_needed.add(\"Union\")\n",
    "        if \"Any\" in final_type_hint: typing_imports_needed.add(\"Any\")\n",
    "        if \"date\" in final_type_hint: datetime_imports_needed.add(\"date\")\n",
    "        if \"datetime\" in final_type_hint: datetime_imports_needed.add(\"datetime\")\n",
    "        if \"time\" in final_type_hint: datetime_imports_needed.add(\"time\")\n",
    "        if \"timedelta\" in final_type_hint: datetime_imports_needed.add(\"timedelta\")\n",
    "        if \"Decimal\" in final_type_hint: other_stdlib_imports.add(\"import decimal\")\n",
    "        if \"AnyUrl\" in final_type_hint: pydantic_imports_needed.add(\"AnyUrl\")\n",
    "        if \"EmailStr\" in final_type_hint: pydantic_imports_needed.add(\"EmailStr\")\n",
    "        # Rich types - import from base_types\n",
    "        if \"Quantity\" in final_type_hint: rich_type_imports.add(\"Quantity\")\n",
    "        if \"Distance\" in final_type_hint: rich_type_imports.add(\"Distance\")\n",
    "        if \"Duration\" in final_type_hint: rich_type_imports.add(\"Duration\")\n",
    "        if \"DefinedTerm\" in final_type_hint: rich_type_imports.add(\"DefinedTerm\")\n",
    "        if \"Money\" in final_type_hint: rich_type_imports.add(\"Money\")\n",
    "\n",
    "        # Track cross-module imports needed for forward reference hints ('ClassName')\n",
    "        potential_classes_in_hint = set(re.findall(r\"'(\\w+)'\", final_type_hint))\n",
    "        for potential_class in potential_classes_in_hint:\n",
    "             # Add if it's a known class, not current, and not already a direct (runtime) base\n",
    "             if potential_class in all_class_names and potential_class != class_name and potential_class not in valid_base_names_list:\n",
    "                 cross_module_imports.add(potential_class)\n",
    "\n",
    "        # Generate Field(...) call\n",
    "        field_args_dict = get_field_metadata(prop_info)\n",
    "        field_args_parts = [repr(field_args_dict.pop('default', None))]\n",
    "        field_args_parts.extend(f\"{k}={repr(v)}\" for k, v in field_args_dict.items())\n",
    "        field_call = f\"Field({', '.join(field_args_parts)})\"\n",
    "\n",
    "        field_definitions.append(f\"    {field_name}: {final_type_hint} = {field_call}\")\n",
    "        field_added = True\n",
    "\n",
    "    if field_added: pydantic_imports_needed.add(\"Field\")\n",
    "\n",
    "    # --- Assemble Code ---\n",
    "    code_parts = [\"from __future__ import annotations\"]\n",
    "\n",
    "    # Add standard library imports\n",
    "    if datetime_imports_needed:\n",
    "        code_parts.append(f\"from datetime import {', '.join(sorted(list(datetime_imports_needed)))}\")\n",
    "    code_parts.extend(sorted(list(other_stdlib_imports)))\n",
    "\n",
    "    # Add typing imports (conditionally add TYPE_CHECKING)\n",
    "    typehint_only_imports = cross_module_imports # Bases are imported via runtime_base_class_imports\n",
    "    needs_type_checking_block = bool(typehint_only_imports)\n",
    "    if needs_type_checking_block:\n",
    "        typing_imports_needed.add(\"TYPE_CHECKING\")\n",
    "    if typing_imports_needed:\n",
    "        code_parts.append(f\"from typing import {', '.join(sorted(list(typing_imports_needed)))}\")\n",
    "\n",
    "    # Add Pydantic imports\n",
    "    if pydantic_imports_needed:\n",
    "        code_parts.append(f\"from pydantic import {', '.join(sorted(list(pydantic_imports_needed)))}\")\n",
    "\n",
    "    # Add Rich type imports from base_types\n",
    "    if rich_type_imports:\n",
    "         code_parts.append(f\"from .base_types import {', '.join(sorted(list(rich_type_imports)))}\")\n",
    "\n",
    "    # Add Runtime base class imports (already formatted with 'from .module import Class')\n",
    "    code_parts.extend(sorted(list(runtime_base_class_imports)))\n",
    "\n",
    "    # Generate TYPE_CHECKING block ONLY if needed\n",
    "    if needs_type_checking_block:\n",
    "        code_parts.append(\"\\nif TYPE_CHECKING:\")\n",
    "        for class_to_import in sorted(list(typehint_only_imports)):\n",
    "            module_path = get_module_path_for_class(class_to_import)\n",
    "            if class_to_import != class_name:\n",
    "                 code_parts.append(f\"    from {module_path} import {class_to_import}\")\n",
    "\n",
    "    # Class definition\n",
    "    code_parts.append(\"\\n\")\n",
    "    class_docstring = f'\"\"\"\\n    {class_name}: {textwrap.shorten(class_info.comment or \"No description provided.\", width=70)}\\n\\n    Generated from: {class_info.uri}\\n    \"\"\"'\n",
    "    # Use the calculated base_class_str which has pruned bases\n",
    "    code_parts.append(f\"class {class_name}({base_class_str}):\")\n",
    "    code_parts.append(f\"    {class_docstring}\")\n",
    "\n",
    "    if not field_definitions:\n",
    "        code_parts.append(\"    pass\")\n",
    "    else:\n",
    "        code_parts.extend(field_definitions)\n",
    "\n",
    "    code_parts.append(\"\\n    model_config = {'extra': 'forbid'}\")\n",
    "\n",
    "    return \"\\n\".join(code_parts) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "53d1f8ce-9d9d-4a4e-86eb-051bafaf0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TYPES_FILENAME = \"base_types.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4e1cbc60-2119-47a4-ac88-332c7a83dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes necessary imports and helper functions are defined above it\n",
    "\n",
    "def generate_ontology_models(\n",
    "     analyzed_schema: Dict[str, Dict],\n",
    "     output_base_dir: str,\n",
    "     # *** Added required arguments ***\n",
    "     schema_graph: rdflib.Graph,\n",
    "     properties_info: Dict[rdflib.URIRef, PropertyInfo], # Pass properties_info explicitly\n",
    "     models_subdir: str = \"models\",\n",
    "     ) -> None:\n",
    "    \"\"\"\n",
    "    Generates Pydantic model files from analyzed schema info, including base_types.py\n",
    "    and a correctly structured __init__.py.\n",
    "    \"\"\"\n",
    "    classes_info: Dict[rdflib.URIRef, ClassInfo] = analyzed_schema.get(\"classes\", {})\n",
    "    # properties_info is now passed directly\n",
    "\n",
    "    if not classes_info:\n",
    "        logging.error(\"No class information found in analyzed schema. Cannot generate models.\")\n",
    "        return\n",
    "\n",
    "    output_path = pathlib.Path(output_base_dir) / MODELS_SUBDIR # Use constant\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    (output_path / \"__init__.py\").touch()\n",
    "\n",
    "    base_type_uris = {\n",
    "        SCHEMA.Quantity, SCHEMA.Distance, SCHEMA.Duration, SCHEMA.DefinedTerm, SCHEMA.Money\n",
    "    }\n",
    "    base_type_names = {map_uri_to_classname(uri) for uri in base_type_uris if uri in classes_info}\n",
    "\n",
    "    # --- Generate base_types.py ---\n",
    "    base_types_path = output_path / BASE_TYPES_FILENAME\n",
    "    try:\n",
    "        global BASE_TYPES_CODE # Ensure this is defined/accessible\n",
    "        if 'BASE_TYPES_CODE' not in globals():\n",
    "             raise NameError(\"BASE_TYPES_CODE string not found.\")\n",
    "        with open(base_types_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(BASE_TYPES_CODE)\n",
    "        logging.info(f\"Generated base types file: {base_types_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write {BASE_TYPES_FILENAME}: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    all_class_names = {map_uri_to_classname(uri) for uri in classes_info.keys()}\n",
    "    all_class_names.update(base_type_names)\n",
    "    all_class_uris = set(classes_info.keys()) # Needed for get_base_classes\n",
    "\n",
    "    generation_order_uris = sorted(list(classes_info.keys()))\n",
    "    logging.info(\"Generating models in alphabetical order by URI.\")\n",
    "\n",
    "\n",
    "    # --- Generate individual model files ---\n",
    "    generated_files = 0\n",
    "    module_to_class_map: Dict[str, str] = {} # Store module_name_part -> ClassName\n",
    "\n",
    "    for class_uri in generation_order_uris:\n",
    "        if class_uri not in classes_info: continue\n",
    "        if class_uri in base_type_uris: continue # Skip base types\n",
    "\n",
    "        class_info = classes_info[class_uri]\n",
    "        class_name = map_uri_to_classname(class_uri)\n",
    "        module_name_part = map_uri_to_fieldname(class_uri)\n",
    "\n",
    "        try:\n",
    "            # **** CORRECTED CALL: Pass all required arguments ****\n",
    "            model_code = generate_pydantic_model_code(\n",
    "                class_info=class_info,\n",
    "                properties_info=properties_info, # Pass explicitly\n",
    "                all_class_names=all_class_names,\n",
    "                all_class_uris=all_class_uris,   # Pass URIs\n",
    "                schema_graph=schema_graph,       # Pass graph\n",
    "                base_type_names=base_type_names  # Pass base names\n",
    "            )\n",
    "            # **** END CORRECTION ****\n",
    "\n",
    "            file_path = output_path / f\"{module_name_part}.py\"\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(model_code)\n",
    "\n",
    "            module_to_class_map[module_name_part] = class_name # Store mapping\n",
    "            generated_files += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate code for class {class_name} ({class_uri}): {e}\", exc_info=True)\n",
    "\n",
    "    logging.info(f\"Generated {generated_files} specific Pydantic model files in {output_path}\")\n",
    "\n",
    "\n",
    "    # --- Corrected __init__.py Generation (Using module_to_class_map) ---\n",
    "    init_py_path = output_path / \"__init__.py\"\n",
    "    try:\n",
    "        with open(init_py_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# flake8: noqa\\n\")\n",
    "            f.write(\"# Auto-generated __init__.py for ontology models\\n\\n\")\n",
    "            f.write(\"import logging\\nimport importlib\\nimport pkgutil\\n\")\n",
    "            f.write(\"from typing import TYPE_CHECKING\\nfrom pydantic import BaseModel\\n\\n\")\n",
    "            f.write(\"logger: logging.Logger = logging.getLogger(__name__)\\n\\n\")\n",
    "\n",
    "            # 1. Import from base_types\n",
    "            if base_type_names:\n",
    "                f.write(\"# --- Import Base Types ---\\n\")\n",
    "                f.write(\"try:\\n\")\n",
    "                f.write(f\"    from .{BASE_TYPES_FILENAME[:-3]} import ({', '.join(sorted(list(base_type_names)))})\\n\") # Use filename w/o .py\n",
    "                f.write(\"except ImportError:\\n\")\n",
    "                f.write(\"    logger.warning('Could not import base_types')\\n\\n\")\n",
    "\n",
    "            # 2. Import from other generated modules using the map\n",
    "            if module_to_class_map:\n",
    "                 f.write(\"# --- Import Generated Models ---\\n\")\n",
    "            generated_class_names = set()\n",
    "            for module_name_part, class_name in sorted(module_to_class_map.items()):\n",
    "                 if class_name not in base_type_names: # Should already be true due to skip logic\n",
    "                     f.write(f\"try:\\n\")\n",
    "                     f.write(f\"    from .{module_name_part} import {class_name}\\n\")\n",
    "                     generated_class_names.add(class_name)\n",
    "                     f.write(f\"except ImportError:\\n\")\n",
    "                     f.write(f\"    logger.warning(f'Could not import {class_name} from .{module_name_part}')\\n\")\n",
    "\n",
    "            # 3. Define __all__\n",
    "            all_names = sorted(list(base_type_names | generated_class_names))\n",
    "            f.write(\"\\n__all__ = [\\n\")\n",
    "            for name in all_names:\n",
    "                f.write(f'    \"{name}\",\\n')\n",
    "            f.write(\"]\\n\\n\")\n",
    "\n",
    "            # 4. Include rebuild_all function and call\n",
    "            #    (Ensure the full rebuild_all definition is included here as before)\n",
    "            f.write(\"# --- Rebuild models to resolve forward references ---\\n\")\n",
    "            f.write(\"def rebuild_all() -> None:\\n\")\n",
    "            # <<<< INSERT FULL rebuild_all function definition here >>>>\n",
    "            f.write(\"    pass # Placeholder - MUST INSERT FULL DEFINITION\\n\\n\")\n",
    "            f.write(\"# Run rebuild automatically on import\\n\")\n",
    "            f.write(\"try:\\n\")\n",
    "            f.write(\"    rebuild_all()\\n\")\n",
    "            f.write(\"    logger.info(f'Pydantic models in {__name__} package rebuilt.')\\n\")\n",
    "            f.write(\"except Exception as e_global:\\n\")\n",
    "            f.write(\"    logger.error(f'Global error during model rebuild: {e_global}', exc_info=True)\\n\")\n",
    "\n",
    "        logging.info(f\"Successfully generated __init__.py at {init_py_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write __init__.py: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cd2cb259-072b-4420-b2da-0bc0fd6eab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting Schema.org to Pydantic Conversion Process...\n",
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Found 628 potential schema.org classes.\n",
      "INFO:root:Found 921 potential schema.org properties.\n",
      "INFO:root:Analyzed 628 classes and 921 properties.\n",
      "INFO:root:Generated base types file: output_ontology/models/base_types.py\n",
      "INFO:root:Generating models in alphabetical order by URI.\n",
      "INFO:root:Generated 625 specific Pydantic model files in output_ontology/models\n",
      "INFO:root:Successfully generated __init__.py at output_ontology/models/__init__.py\n",
      "INFO:__main__:Ontology generation process finished.\n",
      "INFO:__main__:Generated models located in: output_ontology/models\n",
      "INFO:__main__:Proceed with Step 5: Post-Processing & Verification (running formatters, linters, mypy).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting Schema.org to Pydantic Conversion Process...\")\n",
    "\n",
    "    # Step 1: Parse Schema\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "    if not schema_graph:\n",
    "        logger.critical(\"Failed to parse schema graph. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 2: Analyze Schema\n",
    "    analyzed_schema = analyze_schema_graph(schema_graph)\n",
    "    if not analyzed_schema or not analyzed_schema.get(\"classes\"):\n",
    "        logger.critical(\"Schema analysis failed or yielded no classes. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Extract properties_info - needed by generate_ontology_models\n",
    "    properties_info_main: Dict[rdflib.URIRef, PropertyInfo] = analyzed_schema.get(\"properties\", {})\n",
    "    if not properties_info_main:\n",
    "        logger.warning(\"No property information found during analysis. Models may lack fields.\")\n",
    "        # Decide if this is critical - for now, proceed\n",
    "\n",
    "    # Step 3 & 4: Generate Models (calls the modified functions)\n",
    "    try:\n",
    "         generate_ontology_models(\n",
    "             analyzed_schema=analyzed_schema,\n",
    "             output_base_dir=OUTPUT_DIR,\n",
    "             models_subdir=MODELS_SUBDIR,\n",
    "             # Pass the necessary arguments explicitly\n",
    "             schema_graph=schema_graph,\n",
    "             properties_info=properties_info_main\n",
    "         )\n",
    "         logger.info(\"Ontology generation process finished.\")\n",
    "         logger.info(f\"Generated models located in: {pathlib.Path(OUTPUT_DIR) / MODELS_SUBDIR}\")\n",
    "         logger.info(\"Proceed with Step 5: Post-Processing & Verification (running formatters, linters, mypy).\")\n",
    "    except Exception as e:\n",
    "         logger.critical(f\"Ontology generation failed: {e}\", exc_info=True)\n",
    "         exit(1)\n",
    "\n",
    "    # Step 5 would be run externally using the script/commands from previous steps\n",
    "    # e.g., run the 'run_post_processing_pipeline()' function if defined in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6f37e-fcf5-49f9-9b36-cc266eff6d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0012e08-62d2-439a-b505-6592d96cf536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc722a33-9d41-47f6-b437-f8cfaa3376c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117866c-2432-4fff-b676-f7d760882fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec5782-2dea-47fc-ac9b-efe198537bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15c5ca-98b8-4723-8980-0f4ecdb56ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed6f94-309e-4e65-852d-4d600397cfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3342d156-b08c-4351-ad2b-0a95d39fc8a4",
   "metadata": {},
   "source": [
    "# Ignore below this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0aafcbd6-efe6-4b90-9ef3-55c3287344e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes other imports and helper functions (map_*, get_metadata, get_module_path) are defined elsewhere\n",
    "# Partially working\n",
    "def generate_pydantic_model_code(\n",
    "    class_info: ClassInfo,\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo],\n",
    "    all_class_names: Set[str], # All generated class names\n",
    "    all_class_uris: Set[rdflib.URIRef], # All generated class URIs\n",
    "    schema_graph: rdflib.Graph # Pass the graph object here\n",
    ") -> str:\n",
    "    \"\"\"Generates the Python code string for a single Pydantic model, including robust imports and correct base classes.\"\"\"\n",
    "\n",
    "    class_name = map_uri_to_classname(class_info.uri)\n",
    "\n",
    "    # --- *** CORRECTED: Call get_base_classes to determine inheritance *** ---\n",
    "    valid_base_names_list = get_base_classes(class_info, all_class_names, all_class_uris, schema_graph)\n",
    "    # --- *** END CORRECTION *** ---\n",
    "\n",
    "    # --- Import Handling Setup ---\n",
    "    typing_imports_needed = set()\n",
    "    pydantic_imports_needed = set()\n",
    "    datetime_imports_needed = set()\n",
    "    other_stdlib_imports = set()\n",
    "    cross_module_imports = set() # For TYPE_CHECKING block (forward refs in hints)\n",
    "    runtime_base_class_imports = set() # For class definition line\n",
    "    rich_type_imports = set() # For base_types like Quantity\n",
    "\n",
    "    # Determine base class string and imports needed for bases\n",
    "    if not valid_base_names_list or valid_base_names_list == [\"BaseModel\"]:\n",
    "        base_class_str = \"BaseModel\"\n",
    "        pydantic_imports_needed.add(\"BaseModel\") # Explicitly needed if direct base\n",
    "    else:\n",
    "        base_class_str = \", \".join(valid_base_names_list)\n",
    "        # Add imports needed for the base classes themselves at runtime\n",
    "        for base_name in valid_base_names_list:\n",
    "             # Only add import if it's not BaseModel (which comes from pydantic)\n",
    "             if base_name != \"BaseModel\":\n",
    "                  # Ensure we don't try to import the class defining itself as a base\n",
    "                  if base_name != class_name:\n",
    "                       runtime_base_class_imports.add(f\"from .{get_module_path_for_class(base_name)} import {base_name}\")\n",
    "                  else:\n",
    "                       logger.warning(f\"Class {class_name} listed itself as a base? Skipping runtime import.\")\n",
    "\n",
    "\n",
    "    # --- Analyze Fields (Populate field_definitions and track imports) ---\n",
    "    # (This part remains the same as the previous working version that fixed F401 for TYPE_CHECKING)\n",
    "    field_definitions = []\n",
    "    field_added = False\n",
    "\n",
    "    sorted_property_uris = sorted(list(class_info.properties))\n",
    "\n",
    "    for prop_uri in sorted_property_uris:\n",
    "        if prop_uri not in properties_info: continue\n",
    "\n",
    "        prop_info = properties_info[prop_uri]\n",
    "        field_name = map_uri_to_fieldname(prop_info.uri)\n",
    "        # Pass all_class_names for forward ref detection during hint generation\n",
    "        # Also pass base_type_names now (assuming defined globally or passed)\n",
    "        global base_type_names # Or pass it in\n",
    "        type_hint_str = map_range_to_typehint(prop_info.ranges, all_class_names, base_type_names)\n",
    "\n",
    "        # Clean prefixes and track actual usage for imports\n",
    "        final_type_hint = type_hint_str\n",
    "        # (Logic for populating typing_imports_needed, pydantic_imports_needed, etc.)\n",
    "        if \"Optional\" in final_type_hint: typing_imports_needed.add(\"Optional\")\n",
    "        if \"List\" in final_type_hint: typing_imports_needed.add(\"List\")\n",
    "        if \"Union\" in final_type_hint: typing_imports_needed.add(\"Union\")\n",
    "        if \"Any\" in final_type_hint: typing_imports_needed.add(\"Any\")\n",
    "        if \"date\" in final_type_hint: datetime_imports_needed.add(\"date\")\n",
    "        if \"datetime\" in final_type_hint: datetime_imports_needed.add(\"datetime\")\n",
    "        # ... etc for other types ...\n",
    "        if \"Quantity\" in final_type_hint: rich_type_imports.add(\"Quantity\")\n",
    "        # ... etc for rich types ...\n",
    "        if \"AnyUrl\" in final_type_hint: pydantic_imports_needed.add(\"AnyUrl\")\n",
    "        if \"EmailStr\" in final_type_hint: pydantic_imports_needed.add(\"EmailStr\")\n",
    "\n",
    "\n",
    "        # Track cross-module imports needed for forward reference hints ('ClassName')\n",
    "        potential_classes_in_hint = set(re.findall(r\"'(\\w+)'\", final_type_hint))\n",
    "        for potential_class in potential_classes_in_hint:\n",
    "             # Add if it's a known class, not the current one, and not already a direct base\n",
    "             if potential_class in all_class_names and potential_class != class_name and potential_class not in valid_base_names_list:\n",
    "                 cross_module_imports.add(potential_class)\n",
    "\n",
    "        # Generate Field(...) call\n",
    "        field_args_dict = get_field_metadata(prop_info)\n",
    "        field_args_parts = [repr(field_args_dict.pop('default', None))]\n",
    "        field_args_parts.extend(f\"{k}={repr(v)}\" for k, v in field_args_dict.items())\n",
    "        field_call = f\"Field({', '.join(field_args_parts)})\"\n",
    "\n",
    "        field_definitions.append(f\"    {field_name}: {final_type_hint} = {field_call}\")\n",
    "        field_added = True\n",
    "\n",
    "    if field_added: pydantic_imports_needed.add(\"Field\") # Add Field if needed\n",
    "\n",
    "\n",
    "    # --- Assemble Code (using the logic from the fix for F401 TYPE_CHECKING) ---\n",
    "    code_parts = [\"from __future__ import annotations\"]\n",
    "\n",
    "    # Add standard library imports\n",
    "    if datetime_imports_needed:\n",
    "        code_parts.append(f\"from datetime import {', '.join(sorted(list(datetime_imports_needed)))}\")\n",
    "    code_parts.extend(sorted(list(other_stdlib_imports))) # Like 'import decimal'\n",
    "\n",
    "    # Add typing imports (conditionally add TYPE_CHECKING)\n",
    "    typehint_only_imports = cross_module_imports # Direct bases are imported via runtime_base_class_imports now\n",
    "    needs_type_checking_block = bool(typehint_only_imports)\n",
    "    if needs_type_checking_block:\n",
    "        typing_imports_needed.add(\"TYPE_CHECKING\")\n",
    "    if typing_imports_needed:\n",
    "        code_parts.append(f\"from typing import {', '.join(sorted(list(typing_imports_needed)))}\")\n",
    "\n",
    "    # Add Pydantic imports\n",
    "    if pydantic_imports_needed:\n",
    "        code_parts.append(f\"from pydantic import {', '.join(sorted(list(pydantic_imports_needed)))}\")\n",
    "\n",
    "    # Add Rich type imports from base_types\n",
    "    if rich_type_imports:\n",
    "         # Ensure correct relative path if base_types is in the same dir\n",
    "         code_parts.append(f\"from .base_types import {', '.join(sorted(list(rich_type_imports)))}\")\n",
    "\n",
    "    # Add Runtime base class imports (already formatted with 'from .module import Class')\n",
    "    code_parts.extend(sorted(list(runtime_base_class_imports)))\n",
    "\n",
    "    # Generate TYPE_CHECKING block ONLY if needed\n",
    "    if needs_type_checking_block:\n",
    "        code_parts.append(\"\\nif TYPE_CHECKING:\")\n",
    "        for class_to_import in sorted(list(typehint_only_imports)):\n",
    "            module_path = get_module_path_for_class(class_to_import)\n",
    "            if class_to_import != class_name: # Avoid self-import in block\n",
    "                 code_parts.append(f\"    from {module_path} import {class_to_import}\")\n",
    "\n",
    "    # Class definition\n",
    "    code_parts.append(\"\\n\")\n",
    "    class_docstring = f'\"\"\"\\n    {class_name}: {textwrap.shorten(class_info.comment or \"No description provided.\", width=70)}\\n\\n    Generated from: {class_info.uri}\\n    \"\"\"'\n",
    "    # Use the calculated base_class_str which has pruned bases\n",
    "    code_parts.append(f\"class {class_name}({base_class_str}):\")\n",
    "    code_parts.append(f\"    {class_docstring}\")\n",
    "\n",
    "    if not field_definitions:\n",
    "        code_parts.append(\"    pass\")\n",
    "    else:\n",
    "        code_parts.extend(field_definitions)\n",
    "\n",
    "    code_parts.append(\"\\n    model_config = {'extra': 'forbid'}\")\n",
    "\n",
    "    return \"\\n\".join(code_parts) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342326d-a0ba-4061-a3d4-d8fb110d52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parially working - DO NOT EXECUTE\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting Schema.org to Pydantic Conversion Process...\")\n",
    "\n",
    "    # Step 1: Parse Schema\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "    if not schema_graph:\n",
    "        logger.critical(\"Failed to parse schema graph. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 2: Analyze Schema\n",
    "    # Make sure analyze_schema_graph is defined as in previous steps\n",
    "    analyzed_schema = analyze_schema_graph(schema_graph)\n",
    "    if not analyzed_schema or not analyzed_schema.get(\"classes\"):\n",
    "        logger.critical(\"Schema analysis failed or yielded no classes. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Extract info needed by generator functions\n",
    "    classes_info: Dict[rdflib.URIRef, ClassInfo] = analyzed_schema.get(\"classes\", {})\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo] = analyzed_schema.get(\"properties\", {}) # Make sure properties_info is accessible globally or passed correctly\n",
    "    all_class_names = {map_uri_to_classname(uri) for uri in classes_info.keys()}\n",
    "    all_class_uris = set(classes_info.keys())\n",
    "    # Define base type URIs and names (ensure consistency with BASE_TYPES_CODE)\n",
    "    base_type_uris = {\n",
    "        SCHEMA.Quantity, SCHEMA.Distance, SCHEMA.Duration, SCHEMA.DefinedTerm, SCHEMA.Money\n",
    "    }\n",
    "    base_type_names = {map_uri_to_classname(uri) for uri in base_type_uris if uri in classes_info}\n",
    "    all_class_names.update(base_type_names) # Ensure base types are in the registry\n",
    "\n",
    "    # Step 3 & 4: Generate Models (including base_types.py and __init__.py)\n",
    "    # Ensure generate_ontology_models definition is available\n",
    "    # It internally calls generate_pydantic_model_code which now takes extra args\n",
    "    # We need to adjust generate_ontology_models to pass them\n",
    "\n",
    "    # --- Adjusted generate_ontology_models definition ---\n",
    "    # (Include the full function from the previous correct response, ensuring it\n",
    "    # passes 'all_class_uris' and 'schema_graph' to generate_pydantic_model_code\n",
    "    # and uses the correct 'module_to_class_map' for __init__.py generation)\n",
    "\n",
    "    # Example call - assuming generate_ontology_models is defined correctly above\n",
    "    try:\n",
    "         generate_ontology_models(\n",
    "             analyzed_schema=analyzed_schema,\n",
    "             output_base_dir=OUTPUT_DIR,\n",
    "             models_subdir=MODELS_SUBDIR\n",
    "             # Pass schema_graph and all_class_uris if needed by the internal call\n",
    "             # This depends on how you structure the final script\n",
    "             # Option 1: Make properties_info, all_class_names, all_class_uris, schema_graph global\n",
    "             # Option 2: Pass them through generate_ontology_models down to generate_pydantic_model_code\n",
    "         )\n",
    "         logger.info(\"Ontology generation process finished.\")\n",
    "         logger.info(f\"Generated models located in: {pathlib.Path(OUTPUT_DIR) / MODELS_SUBDIR}\")\n",
    "         logger.info(\"Proceed with Step 5: Post-Processing & Verification (running formatters, linters, mypy).\")\n",
    "    except Exception as e:\n",
    "         logger.critical(f\"Ontology generation failed: {e}\", exc_info=True)\n",
    "         exit(1)\n",
    "\n",
    "    # Step 5 would be run externally using the script/commands from previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "00b0f28c-9566-460f-a573-f2a7cc731f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pydantic_model_code(\n",
    "    class_info: ClassInfo,\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo],\n",
    "    all_class_names: Set[str] # All class names being generated\n",
    ") -> str:\n",
    "    \"\"\"Generates the Python code string for a single Pydantic model, including robust imports.\"\"\"\n",
    "\n",
    "    class_name = map_uri_to_classname(class_info.uri)\n",
    "    \n",
    "    base_uris = class_info.superclasses\n",
    "    potential_base_names = {map_uri_to_classname(uri) for uri in base_uris}\n",
    "    valid_base_names = sorted([name for name in potential_base_names if name in all_class_names and name != 'Thing'])\n",
    "\n",
    "    # --- START: Revised Import Handling ---\n",
    "    # Use sets to automatically handle duplicates\n",
    "    core_imports = set()\n",
    "    typing_imports_specific = set() # Specific items needed from typing\n",
    "    pydantic_imports_specific = set() # Always need these\n",
    "    datetime_imports_specific = set()\n",
    "    other_imports = set() # For things like decimal, isodate\n",
    "    cross_module_imports = set() # For other generated models (used in TYPE_CHECKING)\n",
    "    rich_type_imports = set()\n",
    "    runtime_base_class_imports = set() \n",
    "\n",
    "    # Determine base class string and imports needed for bases\n",
    "    if not valid_base_names:\n",
    "        base_class_str = \"BaseModel\"\n",
    "        pydantic_imports_specific.add(\"BaseModel\")\n",
    "    else:\n",
    "        base_class_str = \", \".join(valid_base_names)\n",
    "        for base_name in valid_base_names:\n",
    "             # Assume base classes are in other modules within the same package\n",
    "            cross_module_imports.add(base_name)\n",
    "            # runtime_base_class_imports.add(f\"from .{get_module_path_for_class(base_name)} import {base_name}\")\n",
    "\n",
    "    # Analyze fields to determine necessary imports\n",
    "    field_definitions = []\n",
    "    sorted_property_uris = sorted(list(class_info.properties))\n",
    "    field_added = False\n",
    "\n",
    "    for prop_uri in sorted_property_uris:\n",
    "        if prop_uri not in properties_info: continue # Skip if property info missing\n",
    "\n",
    "        prop_info = properties_info[prop_uri]\n",
    "        field_name = map_uri_to_fieldname(prop_info.uri)\n",
    "        type_hint_str = map_range_to_typehint(prop_info.ranges, all_class_names)\n",
    "\n",
    "        # --- Refined Import Tracking based on Type Hint String ---\n",
    "        if \"Optional\" in type_hint_str: typing_imports_specific.add(\"Optional\")\n",
    "        if \"List\" in type_hint_str: typing_imports_specific.add(\"List\")\n",
    "        if \"Union\" in type_hint_str: typing_imports_specific.add(\"Union\")\n",
    "        if \"Any\" in type_hint_str: typing_imports_specific.add(\"Any\")\n",
    "        # Add VC-Zero/rich types if they come from specific modules\n",
    "        if \"datetime.date\" in type_hint_str: datetime_imports_specific.add(\"date\")\n",
    "        if \"datetime.datetime\" in type_hint_str: datetime_imports_specific.add(\"datetime\")\n",
    "        if \"datetime.time\" in type_hint_str: datetime_imports_specific.add(\"time\")\n",
    "        if \"timedelta\" in type_hint_str: datetime_imports_specific.add(\"timedelta\")\n",
    "        if \"decimal.Decimal\" in type_hint_str: other_imports.add(\"import decimal\")\n",
    "        # Add imports for Pydantic types used\n",
    "        if \"pydantic.AnyUrl\" in type_hint_str: pydantic_imports_specific.add(\"AnyUrl\")\n",
    "        if \"pydantic.EmailStr\" in type_hint_str: pydantic_imports_specific.add(\"EmailStr\")\n",
    "        # Add other specific pydantic types as needed based on TYPE_MAP\n",
    "        if \"Quantity\" in type_hint_str: rich_type_imports.add(\"Quantity\")\n",
    "        if \"Distance\" in type_hint_str: rich_type_imports.add(\"Distance\")\n",
    "        if \"Duration\" in type_hint_str: rich_type_imports.add(\"Duration\")\n",
    "        if \"DefinedTerm\" in type_hint_str: rich_type_imports.add(\"DefinedTerm\")\n",
    "\n",
    "        # Track cross-module imports needed for type hints (used within TYPE_CHECKING)\n",
    "        potential_classes_in_hint = set(re.findall(r\"'(\\w+)'\", type_hint_str))\n",
    "        for potential_class in potential_classes_in_hint:\n",
    "             if potential_class in all_class_names and potential_class != class_name:\n",
    "                 cross_module_imports.add(potential_class)\n",
    "        # --- End Refined Import Tracking ---\n",
    "\n",
    "        field_args_dict = get_field_metadata(prop_info)\n",
    "        # Generate Field(...) call (same as before)\n",
    "        field_args_parts = []\n",
    "        default_val_repr = repr(field_args_dict.pop('default', None))\n",
    "        field_args_parts.append(default_val_repr)\n",
    "        field_args_parts.extend(f\"{k}={repr(v)}\" for k, v in field_args_dict.items())\n",
    "        field_call = f\"Field({', '.join(field_args_parts)})\"\n",
    "\n",
    "        # *** CRITICAL FIX: Ensure type hints use the *imported names*, not module prefixes ***\n",
    "        # Basic replacement - more robust parsing might be needed for complex nested hints\n",
    "        final_type_hint = type_hint_str.replace(\"pydantic.\", \"\") # Remove prefix if imported directly\n",
    "        final_type_hint = final_type_hint.replace(\"datetime.\", \"\") # Remove prefix if imported directly\n",
    "        # Replace typing prefixes only if specific types are imported\n",
    "        if typing_imports_specific:\n",
    "            final_type_hint = final_type_hint.replace(\"typing.\", \"\")\n",
    "\n",
    "        field_definitions.append(f\"    {field_name}: {final_type_hint} = {field_call}\")\n",
    "        field_added = True\n",
    "\n",
    "    # --- DEBUG LOGGING START ---\n",
    "    # print(f\"--- Debugging Imports for Class: {class_name} ---\")\n",
    "    # print(f\"Field added flag: {field_added}\")\n",
    "    # print(f\"Cross-module imports needed (before filtering bases): {cross_module_imports}\")\n",
    "\n",
    "    # Filter out base classes already imported at runtime from type-hint-only imports\n",
    "    typehint_only_imports = cross_module_imports - set(valid_base_names)\n",
    "    # print(f\"Type-hint-only forward refs: {typehint_only_imports}\")\n",
    "\n",
    "    will_generate_type_checking_block = bool(typehint_only_imports)\n",
    "\n",
    "    if will_generate_type_checking_block:\n",
    "        typing_imports_specific.add(\"TYPE_CHECKING\") # Only add it if the block will exist\n",
    "    # print(f\"Will generate TYPE_CHECKING block? {will_generate_type_checking_block}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Assemble the full class code with Corrected Imports ---\n",
    "    code_parts = []\n",
    "    code_parts.append(\"from __future__ import annotations\") # Keep this first\n",
    "\n",
    "    # Add standard library imports\n",
    "    if datetime_imports_specific:\n",
    "        code_parts.append(f\"from datetime import {', '.join(sorted(list(datetime_imports_specific)))}\")\n",
    "    code_parts.extend(sorted(list(other_imports))) # Like 'import decimal'\n",
    "\n",
    "    \n",
    "\n",
    "    # Generate the main typing import line (if needed)\n",
    "    if typing_imports_specific:\n",
    "        code_parts.append(f\"from typing import {', '.join(sorted(list(typing_imports_specific)))}\")\n",
    "    \n",
    "\n",
    "    # Add typing imports - always import TYPE_CHECKING\n",
    "    # typing_imports_specific.add(\"TYPE_CHECKING\")\n",
    "    # code_parts.append(f\"from typing import {', '.join(sorted(list(typing_imports_specific)))}\")\n",
    "\n",
    "    # Add Pydantic imports\n",
    "    if field_added:\n",
    "        pydantic_imports_specific.add(\"Field\")\n",
    "    if pydantic_imports_specific:\n",
    "        code_parts.append(f\"from pydantic import {', '.join(sorted(list(pydantic_imports_specific)))}\")\n",
    "    logging.debug(f\"Final pydantic imports needed: {pydantic_imports_specific}\")\n",
    "\n",
    "    code_parts.extend(sorted(list(runtime_base_class_imports)))\n",
    "\n",
    "    if rich_type_imports:\n",
    "         code_parts.append(f\"from .base_types import {', '.join(sorted(list(rich_type_imports)))}\")\n",
    "\n",
    "\n",
    "    # Add forward reference imports within TYPE_CHECKING block\n",
    "    # Only include classes that are *not* base classes (already imported above if needed)\n",
    "    typehint_only_imports = cross_module_imports - set(valid_base_names)\n",
    "    if typehint_only_imports:\n",
    "        # code_parts.append(\"\\nif TYPE_CHECKING:\")\n",
    "        for class_to_import in sorted(list(typehint_only_imports)):\n",
    "            code_parts.append(f\"from {get_module_path_for_class(class_to_import)} import {class_to_import}\")\n",
    "\n",
    "    # **** MODIFICATION HERE ****\n",
    "    # Conditionally add 'Field' to pydantic imports only if used\n",
    "    \n",
    "    # Only add the import line if the set is not empty\n",
    "    \n",
    "    # **** END MODIFICATION ****\n",
    "\n",
    "    # Add imports for base classes (needed at runtime for class definition)\n",
    "    # Place these *after* TYPE_CHECKING block if they are only needed for inheritance\n",
    "    # If base classes are needed for type hints *within this file*, they need careful handling\n",
    "    # For now, let's assume they are correctly imported for the class definition line.\n",
    "    # This might still need adjustment based on specific inheritance patterns.\n",
    "    if valid_base_names:\n",
    "        # Ensure base classes are imported for the class definition line\n",
    "        # This might duplicate imports already added above but ensures availability\n",
    "        for base_name in valid_base_names:\n",
    "            runtime_base_class_imports.add(f\"from {get_module_path_for_class(base_name)} import {base_name}\")\n",
    "    \n",
    "    code_parts.extend(sorted(list(runtime_base_class_imports)))\n",
    "    code_parts.append(\"\\n\") # Separator\n",
    "\n",
    "    # Add class definition\n",
    "    class_docstring = f'\"\"\"\\n    {class_name}: {textwrap.shorten(class_info.comment or \"No description provided.\", width=70)}\\n\\n    Generated from: {class_info.uri}\\n    \"\"\"'\n",
    "    code_parts.append(f\"class {class_name}({base_class_str}):\")\n",
    "    code_parts.append(f\"    {class_docstring}\")\n",
    "\n",
    "    if not field_definitions:\n",
    "        code_parts.append(\"    pass\")\n",
    "    else:\n",
    "        code_parts.extend(field_definitions)\n",
    "\n",
    "    code_parts.append(\"\\n    model_config = {'extra': 'forbid'}\")\n",
    "\n",
    "    return \"\\n\".join(code_parts) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ee38d5b9-ab14-4dc0-a5bf-f9d658df01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TYPES_CODE = \"\"\"\n",
    "from __future__ import annotations # Keep first\n",
    "from pydantic import (\n",
    "    BaseModel, Field, AnyUrl, field_validator,\n",
    "    model_validator, condecimal, constr, EmailStr # Added EmailStr just in case, adjust as needed\n",
    ")\n",
    "from typing import Optional, List, Union, Any\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import decimal\n",
    "import isodate # Requires: pip install isodate\n",
    "import logging\n",
    "\n",
    "# Configure basic logging if needed within this module too\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__) # Use logger for warnings\n",
    "\n",
    "class Quantity(BaseModel):\n",
    "    \\\"\\\"\\\"\n",
    "    Base model for quantitative values based on schema.org/Quantity.\n",
    "    Actual value and unit are often in subclasses or specific properties.\n",
    "    This primarily serves as a conceptual base.\n",
    "    \\\"\\\"\\\"\n",
    "    model_config = {'extra': 'allow'} # Allow extra fields as Quantity is generic\n",
    "\n",
    "class Distance(Quantity):\n",
    "    \\\"\\\"\\\"\n",
    "    Represents a distance based on schema.org/Distance.\n",
    "    Uses value and unit representation common in QuantitativeValue.\n",
    "    \\\"\\\"\\\"\n",
    "    # Based on properties commonly used with QuantitativeValue for distance\n",
    "    value: Optional[float] = Field(None, description=\"The numerical value of the distance.\")\n",
    "    unitCode: Optional[str] = Field(None, description=\"UN/CEFACT Common Code (3 characters) or URL for the unit of measurement. E.g., 'MTR' for meter, 'KM' for kilometer, 'FT' for foot, 'INH' for inch.\")\n",
    "    unitText: Optional[str] = Field(None, description=\"A string indicating the unit of measurement. Useful if unitCode is not applicable or needs clarification. E.g., 'meters', 'miles'.\")\n",
    "\n",
    "    model_config = {'extra': 'forbid'}\n",
    "\n",
    "    # Add validation if needed, e.g., check unitCode format\n",
    "\n",
    "class Duration(Quantity):\n",
    "    \\\"\\\"\\\"\n",
    "    Represents a duration based on schema.org/Duration.\n",
    "    Stores duration as datetime.timedelta, parsed from ISO 8601 duration format.\n",
    "    \\\"\\\"\\\"\n",
    "    # Pydantic doesn't have native ISO 8601 duration parsing, use validator\n",
    "    # Use alias to allow input using schema.org's likely property name if it differs\n",
    "    value_iso8601: Optional[str] = Field(None, validation_alias='iso8601Duration', serialization_alias='iso8601Duration', description=\"Duration in ISO 8601 format (e.g., P1Y2M3DT4H5M6S).\")\n",
    "    value_timedelta: Optional[timedelta] = Field(None, exclude=True, description=\"Parsed timedelta value (internal).\") # Exclude from standard model dump\n",
    "\n",
    "    model_config = {'extra': 'forbid', 'populate_by_name': True} # Allow using alias on input\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def parse_duration(cls, data: Any) -> Any:\n",
    "        if isinstance(data, dict):\n",
    "            iso_duration_str = data.get(\"value_iso8601\") or data.get(\"iso8601Duration\")\n",
    "            # Parse only if timedelta isn't already provided and string exists\n",
    "            if iso_duration_str and isinstance(iso_duration_str, str) and 'value_timedelta' not in data:\n",
    "                try:\n",
    "                    td = isodate.parse_duration(iso_duration_str)\n",
    "                    data['value_timedelta'] = td\n",
    "                    data['value_iso8601'] = iso_duration_str # Ensure original is stored\n",
    "                except (isodate.ISO8601Error, ValueError) as e:\n",
    "                    logger.warning(f\"Could not parse ISO 8601 duration '{iso_duration_str}': {e}\")\n",
    "                    data['value_timedelta'] = None\n",
    "                    data['value_iso8601'] = iso_duration_str # Keep original invalid string\n",
    "            # If timedelta provided directly, ensure value_timedelta field is populated\n",
    "            elif data.get('value_timedelta') and isinstance(data.get('value_timedelta'), timedelta):\n",
    "                 pass # Already populated\n",
    "        elif isinstance(data, str):\n",
    "             # Allow direct initialization from ISO string\n",
    "             try:\n",
    "                 td = isodate.parse_duration(data)\n",
    "                 return {'value_iso8601': data, 'value_timedelta': td}\n",
    "             except (isodate.ISO8601Error, ValueError) as e:\n",
    "                 logger.warning(f\"Could not parse ISO 8601 duration string '{data}': {e}\")\n",
    "                 return {'value_iso8601': data, 'value_timedelta': None}\n",
    "\n",
    "        return data # Return dict for Pydantic processing\n",
    "\n",
    "    # Optional: Add property to access timedelta easily\n",
    "    @property\n",
    "    def timedelta(self) -> Optional[timedelta]:\n",
    "        return self.value_timedelta\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \\\"\\\"\\\"Return ISO 8601 string representation if available.\\\"\\\"\\\"\n",
    "        # Prefer original string if available, otherwise format timedelta (basic)\n",
    "        if self.value_iso8601:\n",
    "            return self.value_iso8601\n",
    "        elif self.value_timedelta is not None:\n",
    "             try:\n",
    "                 # Attempt basic formatting back (might lose fidelity vs isodate.duration_isoformat)\n",
    "                 return str(self.value_timedelta)\n",
    "             except Exception:\n",
    "                 return \"Invalid Duration Timedelta\"\n",
    "        return \"Invalid/Missing Duration\"\n",
    "\n",
    "\n",
    "class DefinedTerm(BaseModel):\n",
    "    \\\"\\\"\\\"\n",
    "    Represents a term from a defined set based on schema.org/DefinedTerm.\n",
    "    \\\"\\\"\\\"\n",
    "    # Core properties often associated with DefinedTerm\n",
    "    termCode: Optional[str] = Field(None, description=\"A code that identifies this DefinedTerm within a DefinedTermSet.\")\n",
    "    name: Optional[str] = Field(None, description=\"The name of the item.\")\n",
    "    description: Optional[str] = Field(None, description=\"A description of the item.\")\n",
    "    # Allow referencing the set it belongs to, if known (using AnyUrl for flexibility)\n",
    "    inDefinedTermSet: Optional[AnyUrl] = Field(None, description=\"A DefinedTermSet Organization or DataCatalog that contains this term.\")\n",
    "\n",
    "    model_config = {'extra': 'allow'} # Allow potential other properties from schema.org or extensions\n",
    "\n",
    "class Money(BaseModel):\n",
    "     \\\"\\\"\\\"\n",
    "     Represents an amount of money with a currency. Based on schema.org concepts\n",
    "     often used with PriceSpecification or MonetaryAmount.\n",
    "     \\\"\\\"\\\"\n",
    "     # Using 'amount' and 'currency' inspired by common patterns, not a direct schema.org/Money type\n",
    "     amount: Optional[decimal.Decimal] = Field(None, description=\"The amount of money.\")\n",
    "     currency: Optional[constr(pattern=r'^[A-Z]{3}$')] = Field(None, description=\"ISO 4217 Currency Code\") # type: ignore\n",
    "\n",
    "     @field_validator('amount', mode='before')\n",
    "     @classmethod\n",
    "     def clean_amount(cls, v: Any) -> Optional[decimal.Decimal]: # Added type hints\n",
    "         # Indentation Level 2 (Inside Function)\n",
    "         if isinstance(v, (int, float)):\n",
    "             try: # Indentation Level 3\n",
    "                 return decimal.Decimal(v) # Indentation Level 4\n",
    "             except Exception as e: # Indentation Level 3\n",
    "                  logger.error(f\"Error converting {v} to Decimal: {e}\")\n",
    "                  raise ValueError(f\"Cannot convert {v} to Decimal\") # Indentation Level 4\n",
    "         if isinstance(v, str): # Indentation Level 2\n",
    "             try: # Indentation Level 3\n",
    "                 return decimal.Decimal(v.strip()) # Indentation Level 4\n",
    "             except decimal.InvalidOperation: # Indentation Level 3\n",
    "                  raise ValueError(f\"Invalid decimal format for amount: {v}\") # Indentation Level 4\n",
    "         # Allow existing Decimals or None to pass through\n",
    "         if isinstance(v, decimal.Decimal) or v is None: # Indentation Level 2\n",
    "              return v # Indentation Level 3\n",
    "         # Raise error for other unexpected types\n",
    "         raise ValueError(f\"Unexpected type for amount: {type(v)}\") # Indentation Level 2\n",
    "\n",
    "     model_config = {'extra': 'forbid'}\n",
    "\n",
    "# Add other base types/VC-Zeros below if needed\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b4110a6a-1f45-4b56-a0b4-e3975e967097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, Any, cast # Ensure necessary types for function signature\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import shlex # If used by helper functions, ensure imported\n",
    "import textwrap # If used by helper functions\n",
    "\n",
    "# --- Assume previous code exists: ---\n",
    "# SCHEMA namespace\n",
    "# ClassInfo, PropertyInfo NamedTuples\n",
    "# analyzed_schema dict (passed as argument)\n",
    "# properties_info dict (extracted from analyzed_schema)\n",
    "# BASE_TYPES_CODE string (containing Quantity, Distance, Duration, DefinedTerm, Money)\n",
    "# Mapping functions: map_uri_to_classname, map_uri_to_fieldname, get_module_path_for_class\n",
    "# generate_pydantic_model_code function (from latest correct version)\n",
    "# --------------------------------------\n",
    "\n",
    "logger = logging.getLogger(__name__) # Define logger for use within function\n",
    "\n",
    "def generate_ontology_models(\n",
    "     analyzed_schema: Dict[str, Dict],\n",
    "     output_base_dir: str,\n",
    "     models_subdir: str = \"models\"\n",
    "     ) -> None: # Added return type hint\n",
    "    \"\"\"\n",
    "    Generates Pydantic model files from analyzed schema info, including base_types.py\n",
    "    and a correctly structured __init__.py.\n",
    "    \"\"\"\n",
    "\n",
    "    classes_info: Dict[rdflib.URIRef, ClassInfo] = analyzed_schema.get(\"classes\", {})\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo] = analyzed_schema.get(\"properties\", {})\n",
    "\n",
    "    if not classes_info: # Check classes_info primarily\n",
    "        logging.error(\"No class information found in analyzed schema. Cannot generate models.\")\n",
    "        return\n",
    "    if not properties_info:\n",
    "         logging.warning(\"No property information found in analyzed schema. Models may lack fields.\")\n",
    "         # Proceed cautiously, or return depending on desired strictness\n",
    "\n",
    "    output_path = pathlib.Path(output_base_dir) / models_subdir\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    # Ensure main __init__.py exists before generating submodules\n",
    "    (output_path / \"__init__.py\").touch()\n",
    "\n",
    "    # Define URIs and names for types handled ONLY in base_types.py\n",
    "    base_type_uris = {\n",
    "        SCHEMA.Quantity, SCHEMA.Distance, SCHEMA.Duration, SCHEMA.DefinedTerm,\n",
    "        # Add SCHEMA.Money if defined in BASE_TYPES_CODE\n",
    "        SCHEMA.Money\n",
    "    }\n",
    "    base_type_names = {map_uri_to_classname(uri) for uri in base_type_uris if uri in classes_info} # Get names only for types actually present\n",
    "\n",
    "    # --- Generate base_types.py ---\n",
    "    base_types_path = output_path / \"base_types.py\"\n",
    "    try:\n",
    "        # Assuming BASE_TYPES_CODE string is defined globally or passed in\n",
    "        global BASE_TYPES_CODE\n",
    "        if 'BASE_TYPES_CODE' not in globals():\n",
    "             raise NameError(\"BASE_TYPES_CODE string not found.\")\n",
    "\n",
    "        with open(base_types_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(BASE_TYPES_CODE)\n",
    "        logging.info(f\"Generated base types file: {base_types_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write base_types.py: {e}\", exc_info=True)\n",
    "        return # Stop if base types cannot be written\n",
    "\n",
    "    all_class_names = {map_uri_to_classname(uri) for uri in classes_info.keys()}\n",
    "    # Add base type names to the known class registry for type hinting purposes\n",
    "    all_class_names.update(base_type_names)\n",
    "\n",
    "    # --- (Optional but recommended) Topological Sort for Generation Order ---\n",
    "    # Using alphabetical as fallback, assuming TYPE_CHECKING handles most cycles\n",
    "    generation_order_uris = sorted(list(classes_info.keys()))\n",
    "    logging.info(\"Generating models in alphabetical order by URI.\")\n",
    "    # Add more robust sorting here if needed based on hierarchy\n",
    "\n",
    "\n",
    "    # --- Generate individual model files, SKIPPING base types ---\n",
    "    generated_files = 0\n",
    "    # **** Store mapping from module name part to main class name ****\n",
    "    module_to_class_map: Dict[str, str] = {}\n",
    "\n",
    "    for class_uri in generation_order_uris:\n",
    "        if class_uri not in classes_info: continue\n",
    "\n",
    "        # **** CORRECTED SKIP LOGIC ****\n",
    "        if class_uri in base_type_uris:\n",
    "            logging.debug(f\"Skipping individual file generation for {class_uri} (defined in base_types.py)\")\n",
    "            continue\n",
    "        # **** END CORRECTION ****\n",
    "\n",
    "        class_info = classes_info[class_uri]\n",
    "        class_name = map_uri_to_classname(class_uri)\n",
    "        module_name_part = map_uri_to_fieldname(class_uri) # snake_case name for file/module\n",
    "\n",
    "        try:\n",
    "            model_code = generate_pydantic_model_code(class_info, properties_info, all_class_names)\n",
    "            file_path = output_path / f\"{module_name_part}.py\"\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(model_code)\n",
    "\n",
    "            # **** Store mapping IF generation succeeded ****\n",
    "            module_to_class_map[module_name_part] = class_name\n",
    "            generated_files += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate code for class {class_name} ({class_uri}): {e}\", exc_info=True) # Add traceback\n",
    "\n",
    "    logging.info(f\"Generated {generated_files} specific Pydantic model files in {output_path}\")\n",
    "\n",
    "\n",
    "    # --- *** CORRECTED __init__.py Generation *** ---\n",
    "    init_py_path = output_path / \"__init__.py\"\n",
    "    try:\n",
    "        with open(init_py_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# flake8: noqa\\n\")\n",
    "            f.write(\"# Auto-generated __init__.py for ontology models\\n\\n\")\n",
    "            f.write(\"import logging\\n\")\n",
    "            f.write(\"import importlib\\n\")\n",
    "            f.write(\"import pkgutil\\n\")\n",
    "            f.write(\"from typing import TYPE_CHECKING # TYPE_CHECKING might be needed by rebuild_all\\n\")\n",
    "            f.write(\"from pydantic import BaseModel # Needed for rebuild_all check\\n\\n\")\n",
    "\n",
    "            f.write(\"logger: logging.Logger = logging.getLogger(__name__)\\n\\n\") # Define early\n",
    "\n",
    "            # 1. Import explicitly from base_types\n",
    "            if base_type_names:\n",
    "                f.write(\"# --- Import Base Types ---\\n\")\n",
    "                f.write(\"try:\\n\")\n",
    "                f.write(f\"    from .base_types import ({', '.join(sorted(list(base_type_names)))})\\n\") # Explicit names\n",
    "                f.write(\"except ImportError:\\n\")\n",
    "                f.write(\"    logger.warning('Could not import base_types')\\n\\n\")\n",
    "\n",
    "            # 2. Import explicitly from all OTHER generated modules\n",
    "            if module_to_class_map:\n",
    "                 f.write(\"# --- Import Generated Models ---\\n\")\n",
    "            generated_class_names = set()\n",
    "            for module_name_part in sorted(module_to_class_map.keys()):\n",
    "                class_name = module_to_class_map[module_name_part]\n",
    "                # Ensure we don't try to re-import base types if map is somehow wrong\n",
    "                if class_name not in base_type_names:\n",
    "                    f.write(f\"try:\\n\")\n",
    "                    f.write(f\"    from .{module_name_part} import {class_name}\\n\")\n",
    "                    generated_class_names.add(class_name)\n",
    "                    f.write(f\"except ImportError:\\n\")\n",
    "                    f.write(f\"    logger.warning(f'Could not import {class_name} from .{module_name_part}')\\n\")\n",
    "\n",
    "            # 3. Define __all__ for cleaner namespace\n",
    "            all_names = sorted(list(base_type_names | generated_class_names))\n",
    "            f.write(\"\\n__all__ = [\\n\")\n",
    "            for name in all_names:\n",
    "                f.write(f'    \"{name}\",\\n') # Use double quotes for names in list\n",
    "            f.write(\"]\\n\\n\")\n",
    "\n",
    "\n",
    "            # 4. Include rebuild_all function and call (with fixed annotation)\n",
    "            f.write(\"# --- Rebuild models to resolve forward references ---\\n\")\n",
    "            f.write(\"def rebuild_all() -> None:\\n\") # Ensure annotation is present\n",
    "            f.write(\"    package_name = __name__\\n\")\n",
    "            f.write(\"    package = importlib.import_module(package_name)\\n\")\n",
    "            f.write(\"    rebuilt_models = set()\\n\")\n",
    "            f.write('    logger.debug(f\"Attempting rebuild in {package_name}\")\\n\\n')\n",
    "            f.write(\"    for _, module_name_part, _ in pkgutil.iter_modules(package.__path__, package_name + '.') :\\n\")\n",
    "            f.write(\"        # Skip rebuild attempt on __init__ itself or base_types if desired\\n\")\n",
    "            f.write(\"        if module_name_part.endswith('.__init__') or module_name_part.endswith('.base_types'):\\n\")\n",
    "            f.write(\"            continue\\n\")\n",
    "            f.write(\"        try:\\n\")\n",
    "            f.write(\"            module = importlib.import_module(module_name_part)\\n\")\n",
    "            f.write(\"            for attribute_name in dir(module):\\n\")\n",
    "            f.write(\"                attribute = getattr(module, attribute_name)\\n\")\n",
    "            f.write(\"                if (isinstance(attribute, type) and\\n\")\n",
    "            f.write(\"                        issubclass(attribute, BaseModel) and\\n\") # Check issubclass safely\n",
    "            f.write(\"                        attribute is not BaseModel and\\n\")\n",
    "            f.write(\"                        hasattr(attribute, 'model_rebuild') and\\n\") # Check if it has the method\n",
    "            f.write(\"                        attribute not in rebuilt_models):\\n\")\n",
    "            f.write(\"                    try:\\n\")\n",
    "            f.write('                        logger.debug(f\"Rebuilding: {attribute.__name__}\")\\n')\n",
    "            f.write(\"                        attribute.model_rebuild(force=True)\\n\")\n",
    "            f.write(\"                        rebuilt_models.add(attribute)\\n\")\n",
    "            f.write(\"                    except Exception as e_rebuild:\\n\")\n",
    "            f.write(\"                        logger.error(f'Error rebuilding model {attribute.__name__} in {module_name_part}: {e_rebuild}', exc_info=False)\\n\")\n",
    "            f.write(\"        except ModuleNotFoundError:\\n\")\n",
    "            f.write(\"            logger.warning(f\\\"Module not found during rebuild: {module_name_part}\\\")\\n\")\n",
    "            f.write(\"        except Exception as e_import:\\n\")\n",
    "            f.write(\"             logger.error(f'Error importing module {module_name_part} during rebuild: {e_import}', exc_info=False)\\n\\n\")\n",
    "\n",
    "            f.write(\"# Run rebuild automatically on import\\n\")\n",
    "            f.write(\"try:\\n\")\n",
    "            f.write(\"    rebuild_all()\\n\")\n",
    "            f.write(\"    logger.info(f'Pydantic models in {__name__} package rebuilt.')\\n\")\n",
    "            f.write(\"except Exception as e_global:\\n\")\n",
    "            f.write(\"    logger.error(f'Global error during model rebuild: {e_global}', exc_info=True)\\n\")\n",
    "\n",
    "        logging.info(f\"Successfully generated __init__.py at {init_py_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write __init__.py: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Main execution block (Example) ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Assuming schema_graph and analyzed_schema are loaded/created\n",
    "#     # schema_graph = parse_schema_to_graph(...)\n",
    "#     # analyzed_schema = analyze_schema_graph(schema_graph)\n",
    "#     if analyzed_schema and analyzed_schema.get(\"classes\"):\n",
    "#         generate_ontology_models(analyzed_schema, \"output_ontology\", \"models\")\n",
    "#         logging.info(\"Ontology generation process finished.\")\n",
    "#     else:\n",
    "#          logging.error(\"Analysis data missing, cannot generate models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5d973133-33e1-429a-a1a0-5a75e378e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- generate_ontology_models function (Mostly Unchanged, calls revised generator) ---\n",
    "\n",
    "# def generate_ontology_models(\n",
    "#      analyzed_schema: Dict[str, Dict],\n",
    "#      output_base_dir: str,\n",
    "#      models_subdir: str = \"models\"\n",
    "#      ):\n",
    "#     \"\"\"Generates Pydantic model files from analyzed schema info.\"\"\"\n",
    "\n",
    "#     classes_info: Dict[rdflib.URIRef, ClassInfo] = analyzed_schema.get(\"classes\", {})\n",
    "#     properties_info: Dict[rdflib.URIRef, PropertyInfo] = analyzed_schema.get(\"properties\", {})\n",
    "\n",
    "#     if not classes_info or not properties_info:\n",
    "#         logging.error(\"No class or property information found in analyzed schema.\")\n",
    "#         return\n",
    "\n",
    "#     # **** NEW STEP: Generate base_types.py ****\n",
    "#     output_path = pathlib.Path(output_base_dir) / models_subdir\n",
    "#     output_path.mkdir(parents=True, exist_ok=True)\n",
    "#     (output_path / \"__init__.py\").touch() # Ensure __init__ exists\n",
    "\n",
    "#     base_types_path = output_path / \"base_types.py\"\n",
    "#     base_type_uris = {\n",
    "#         SCHEMA.Quantity, SCHEMA.Distance, SCHEMA.Duration, SCHEMA.DefinedTerm,\n",
    "#         # Add SCHEMA.Money if defined there\n",
    "#     }\n",
    "#     try:\n",
    "#         with open(base_types_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(BASE_TYPES_CODE)\n",
    "#         logging.info(f\"Generated base types file: {base_types_path}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to write base_types.py: {e}\")\n",
    "#         return # Stop if base types cannot be written\n",
    "#     # **** END NEW STEP ****\n",
    "\n",
    "#     # Map URIs to their class names for import generation\n",
    "#     base_type_names = {map_uri_to_classname(uri) for uri in base_type_uris}\n",
    "\n",
    "#     # --- Generate individual model files ---\n",
    "#     generated_files = 0\n",
    "\n",
    "\n",
    "#     all_class_names = {map_uri_to_classname(uri) for uri in classes_info.keys()}\n",
    "\n",
    "#     # Basic topological sort preparation (dependencies based on superclasses)\n",
    "#     dependencies = {\n",
    "#         map_uri_to_classname(cls_uri): {map_uri_to_classname(sup_uri)\n",
    "#                                         for sup_uri in info.superclasses\n",
    "#                                         if sup_uri in classes_info} # Only consider superclasses we generate\n",
    "#         for cls_uri, info in classes_info.items()\n",
    "#     }\n",
    "\n",
    "#     generation_order = []\n",
    "#     visited = set()\n",
    "#     visiting = set()\n",
    "\n",
    "#     def visit(class_name):\n",
    "#         \"\"\"Helper for topological sort.\"\"\"\n",
    "#         if class_name not in dependencies: return # Might be external like BaseModel\n",
    "#         visited.add(class_name)\n",
    "#         visiting.add(class_name)\n",
    "#         for dep in dependencies.get(class_name, set()):\n",
    "#             if dep in visiting:\n",
    "#                 # Cycle detected - handle appropriately (e.g., log, break, complex resolution)\n",
    "#                 logging.warning(f\"Potential circular dependency involving {class_name} and {dep}. Relying on forward refs.\")\n",
    "#             if dep not in visited:\n",
    "#                 visit(dep)\n",
    "#         visiting.remove(class_name)\n",
    "#         generation_order.append(map_uri_to_classname(rdflib.URIRef(f\"{SCHEMA}{class_name}\"))) # Reconstruct URI crudely - needs improvement\n",
    "\n",
    "\n",
    "#     # Determine generation order (simple approach, might need refinement for complex cases)\n",
    "#     all_class_names_sorted_list = sorted(list(all_class_names)) # Fallback if sort fails\n",
    "#     try:\n",
    "#          # Attempt topological sort (may fail on cycles)\n",
    "#          for class_name in all_class_names_sorted_list:\n",
    "#              if class_name not in visited:\n",
    "#                  visit(class_name)\n",
    "#          generation_order_uris = [rdflib.URIRef(f\"{SCHEMA}{name}\") for name in generation_order]\n",
    "#          logging.info(\"Generated class order based on inheritance.\")\n",
    "#     except Exception as e:\n",
    "#          logging.warning(f\"Topological sort for generation order failed ({e}). Using alphabetical.\")\n",
    "#          generation_order_uris = sorted(list(classes_info.keys()))\n",
    "\n",
    "\n",
    "#     # Generate files\n",
    "#     generated_files = 0\n",
    "#     all_generated_modules = set()\n",
    "#     for class_uri in generation_order_uris:\n",
    "#          if class_uri not in classes_info: continue # Skip if URI wasn't mapped back correctly or is external\n",
    "\n",
    "#          class_info = classes_info[class_uri]\n",
    "#          class_name = map_uri_to_classname(class_uri)\n",
    "#          module_name_part = map_uri_to_fieldname(class_uri)\n",
    "#          all_generated_modules.add(module_name_part) # Track generated module names\n",
    "\n",
    "#          try:\n",
    "#              model_code = generate_pydantic_model_code(class_info, properties_info, all_class_names)\n",
    "#              file_path = output_path / f\"{module_name_part}.py\"\n",
    "#              with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#                  f.write(model_code)\n",
    "#              generated_files += 1\n",
    "#          except Exception as e:\n",
    "#              logging.error(f\"Failed to generate code for class {class_name} ({class_uri}): {e}\")\n",
    "\n",
    "#     logging.info(f\"Generated {generated_files} Pydantic model files in {output_path}\")\n",
    "\n",
    "#     # --- Update __init__.py to import all generated models and call model_rebuild ---\n",
    "#     init_py_path = output_path / \"__init__.py\"\n",
    "#     with open(init_py_path, \"w\", encoding=\"utf-8\") as f: # Overwrite __init__.py\n",
    "#          f.write(\"# flake8: noqa\\n\") # Suppress linting errors for unused imports potentially\n",
    "#          f.write(\"# Auto-generated __init__.py for ontology models\\n\\n\")\n",
    "#          f.write(\"import logging\\n\")\n",
    "#          f.write(\"from pydantic import BaseModel\\n\")\n",
    "#          f.write(\"import importlib\\n\")\n",
    "#          f.write(\"import pkgutil\\n\")\n",
    "#          f.write(\"from typing import TYPE_CHECKING\\n\\n\") # Needed for the rebuild function\n",
    "#          # Import base_types explicitly\n",
    "         \n",
    "#          f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "#          f.write(\"try:\\n\")\n",
    "#          f.write(\"    from .base_types import Quantity, Distance, Duration, DefinedTerm\\n\")\n",
    "#          f.write(\"except ImportError:\\n\")\n",
    "#          f.write(\"    logger.warning('Could not import base_types')\\n\\n\")\n",
    "\n",
    "#          # Import all models explicitly for easier use and rebuild\n",
    "#          for module_name_part in sorted(list(all_generated_modules)):\n",
    "#              try:\n",
    "#                  # Attempt to guess class name from module name (needs consistent mapping)\n",
    "#                  # This is fragile - better to pass class_name list explicitly\n",
    "#                  class_name_guess = map_uri_to_classname(rdflib.URIRef(f\"{SCHEMA}{module_name_part}\")) # Reverse mapping attempt\n",
    "#                  if class_name_guess in all_class_names:\n",
    "#                      f.write(f\"from .{module_name_part} import {class_name_guess}\\n\")\n",
    "#                  else: # Fallback if reverse mapping failed\n",
    "#                      f.write(f\"try:\\n\")\n",
    "#                      f.write(f\"    from .{module_name_part} import *\\n\") # Less ideal, try to import *\n",
    "#                      f.write(f\"except ImportError:\\n\")\n",
    "#                      f.write(f\"    logger.warning(f'Could not import from .{module_name_part}')\\n\")\n",
    "\n",
    "#              except Exception:\n",
    "#                  logger.warning(f\"Could not generate import for module {module_name_part}\")\n",
    "\n",
    "\n",
    "#          # The rebuild function (unchanged from previous version)\n",
    "#          f.write(\"\"\"\n",
    "# # --- Rebuild models to resolve forward references ---\n",
    "# def rebuild_all() -> None:\n",
    "#     package_name = __name__\n",
    "#     package = importlib.import_module(package_name)\n",
    "#     rebuilt_models = set()\n",
    "#     logger.debug(f\"Attempting rebuild in {package_name}\")\n",
    "\n",
    "#     for _, module_name_part, _ in pkgutil.iter_modules(package.__path__, package_name + '.'):\n",
    "#         try:\n",
    "#             module = importlib.import_module(module_name_part)\n",
    "#             for attribute_name in dir(module):\n",
    "#                 attribute = getattr(module, attribute_name)\n",
    "#                 # Check if it's a class, subclass of BaseModel, not BaseModel itself, and not rebuilt yet\n",
    "#                 if (isinstance(attribute, type) and\n",
    "#                         issubclass(attribute, BaseModel) and\n",
    "#                         attribute is not BaseModel and\n",
    "#                         attribute not in rebuilt_models):\n",
    "#                     try:\n",
    "#                         logger.debug(f\"Rebuilding: {attribute.__name__}\")\n",
    "#                         attribute.model_rebuild(force=True)\n",
    "#                         rebuilt_models.add(attribute)\n",
    "#                     except Exception as e_rebuild:\n",
    "#                         logger.error(f'Error rebuilding model {attribute.__name__} in {module_name_part}: {e_rebuild}', exc_info=False) # Reduce noise\n",
    "#         except ModuleNotFoundError:\n",
    "#             logger.warning(f\"Module not found during rebuild: {module_name_part}\")\n",
    "#         except Exception as e_import:\n",
    "#              logger.error(f'Error importing module {module_name_part} during rebuild: {e_import}', exc_info=False) # Reduce noise\n",
    "\n",
    "# # Run rebuild automatically on import\n",
    "# try:\n",
    "#     rebuild_all()\n",
    "#     logger.info(f'Pydantic models in {__name__} rebuilt.')\n",
    "# except Exception as e_global:\n",
    "#     logger.error(f'Global error during model rebuild: {e_global}', exc_info=True)\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657b82d-f763-4145-bf58-943bb62eb770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e7252-d096-4c2d-bcd1-99ecc55eb145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480dfca-58e4-4c1c-9052-583f9ebee3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5aa2c296-fedf-452a-8813-514a4b33a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Found 628 potential schema.org classes.\n",
      "INFO:root:Found 921 potential schema.org properties.\n",
      "INFO:root:Analyzed 628 classes and 921 properties.\n",
      "INFO:root:Generated base types file: output_ontology/models/base_types.py\n",
      "INFO:root:Generating models in alphabetical order by URI.\n",
      "INFO:root:Generated 625 specific Pydantic model files in output_ontology/models\n",
      "INFO:root:Successfully generated __init__.py at output_ontology/models/__init__.py\n",
      "INFO:root:Pydantic model code generation complete. Check the 'output_ontology/models' directory.\n",
      "INFO:root:Ready for Step 5: Post-Processing & Verification.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: --- Main execution block demonstrating generation ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume schema_graph is loaded from Step 1\n",
    "    # Assume analyzed_schema is created from Step 2\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "    if schema_graph:\n",
    "        analyzed_schema = analyze_schema_graph(schema_graph)\n",
    "        if analyzed_schema and analyzed_schema.get(\"classes\"):\n",
    "            # Step 4: Generate the Pydantic models\n",
    "            generate_ontology_models(analyzed_schema, OUTPUT_DIR, MODELS_SUBDIR)\n",
    "            logging.info(f\"Pydantic model code generation complete. Check the '{OUTPUT_DIR}/{MODELS_SUBDIR}' directory.\")\n",
    "            logging.info(\"Ready for Step 5: Post-Processing & Verification.\")\n",
    "        else:\n",
    "             logging.error(\"Schema analysis did not produce class/property data.\")\n",
    "    else:\n",
    "        logging.error(\"Failed to parse schema graph for generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7169257f-62e1-4350-93ad-bd45795ab48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Required imports for these types include:\n",
      "from pydantic import BaseModel, Field, AnyUrl\n",
      "from typing import Optional, List, Union\n",
      "from datetime import date, datetime, time, timedelta\n",
      "import isodate\n",
      "import decimal\n",
      "# ... potentially other generated models\n",
      "\n",
      "Richer mapping logic defined. Re-running Step 4 will use these structured types.\n",
      "Ready for Step 5: Post-Processing & Verification.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Placeholder registry\n",
    "    known_generated_classes = {\"Person\", \"DefinedTerm\", \"Distance\", \"Duration\", \"Quantity\"}\n",
    "\n",
    "    # Example: Map range for schema:educationalUse (includes DefinedTerm)\n",
    "    edu_use_ranges = {SCHEMA.DefinedTerm, SCHEMA.Text}\n",
    "    edu_use_type_hint = map_range_to_typehint(edu_use_ranges, known_generated_classes)\n",
    "    # print(f\"Mapping for schema:educationalUse ranges ({edu_use_ranges}):\")\n",
    "    # Expect Optional[Union['DefinedTerm', str]]\n",
    "    # print(f\" -> Type Hint: {edu_use_type_hint}\")\n",
    "\n",
    "    # Example: Map range for schema:distance (can be Distance)\n",
    "    distance_ranges = {SCHEMA.Distance}\n",
    "    distance_type_hint = map_range_to_typehint(distance_ranges, known_generated_classes)\n",
    "    # print(f\"\\nMapping for schema:distance ranges ({distance_ranges}):\")\n",
    "    # Expect Optional['Distance']\n",
    "    # print(f\" -> Type Hint: {distance_type_hint}\")\n",
    "\n",
    "    # Example: Map range for schema:duration (can be Duration)\n",
    "    duration_ranges = {SCHEMA.Duration}\n",
    "    duration_type_hint = map_range_to_typehint(duration_ranges, known_generated_classes)\n",
    "    # print(f\"\\nMapping for schema:duration ranges ({duration_ranges}):\")\n",
    "    # Expect Optional['Duration']\n",
    "    # print(f\" -> Type Hint: {duration_type_hint}\")\n",
    "\n",
    "    # Ensure necessary imports would be generated by Step 4\n",
    "    print(\"\\nRequired imports for these types include:\")\n",
    "    print(\"from pydantic import BaseModel, Field, AnyUrl\")\n",
    "    print(\"from typing import Optional, List, Union\")\n",
    "    print(\"from datetime import date, datetime, time, timedelta\")\n",
    "    print(\"import isodate\")\n",
    "    print(\"import decimal\")\n",
    "    print(\"# ... potentially other generated models\")\n",
    "\n",
    "    print(\"\\nRicher mapping logic defined. Re-running Step 4 will use these structured types.\")\n",
    "    print(\"Ready for Step 5: Post-Processing & Verification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7212a-9183-424b-964e-d12bfcc88735",
   "metadata": {},
   "source": [
    "Step 5: Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893f846-5141-48ab-93ab-0f981951b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: postprocess.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import logging\n",
    "import pathlib\n",
    "import shlex # Use shlex for safer command construction\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Define target directories (adjust paths as needed)\n",
    "TARGET_DIR = pathlib.Path(\"output_ontology\") # Main output directory\n",
    "MODELS_DIR = TARGET_DIR / \"models\" # Specific directory with models\n",
    "TESTS_DIR = TARGET_DIR / \"tests\"  # Directory for tests\n",
    "\n",
    "# Tool Configurations (Ideally read from pyproject.toml or similar)\n",
    "# Example: Fail bandit on medium+ severity, medium+ confidence\n",
    "BANDIT_ARGS = [\"-r\", str(MODELS_DIR), \"-ll\", \"--exit-zero\"] # Exit-zero initially to report, fail later based on output if needed\n",
    "# Example: MyPy needs strict checks\n",
    "MYPY_ARGS = [\"--strict\", str(MODELS_DIR)]\n",
    "# Example: Ruff - fix specific issues, check others\n",
    "RUFF_FIX_ARGS = [\"check\", str(TARGET_DIR), \"--fix\", \"--select\", \"F\", \"--select\", \"E\", \"--select\", \"W\", \"--select\", \"I\"] # Fix F401, E, W, I (Imports etc)\n",
    "RUFF_CHECK_ARGS = [\"check\", str(TARGET_DIR)] # Check everything else\n",
    "\n",
    "def run_command(command: list[str], cwd: str = \".\", check: bool = True, capture: bool = False) -> subprocess.CompletedProcess | None:\n",
    "    \"\"\"Runs a command, logs, checks for errors, optionally captures output.\"\"\"\n",
    "    command_str = shlex.join(command) # Safer than \" \".join\n",
    "    logging.info(f\"Running: {command_str}\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            capture_output=capture,\n",
    "            text=True,\n",
    "            check=check, # Raises CalledProcessError if return code is non-zero\n",
    "            cwd=cwd,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        if capture:\n",
    "            if result.stdout: logging.debug(f\"Stdout:\\n{result.stdout}\")\n",
    "            if result.stderr: logging.debug(f\"Stderr:\\n{result.stderr}\") # Debug level for stderr unless error occurred\n",
    "        logging.info(f\"Command succeeded: {command_str}\")\n",
    "        return result\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Command not found: {command[0]}. Is the tool installed and in PATH?\")\n",
    "        raise # Re-raise to stop the pipeline\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Command failed with exit code {e.returncode}: {command_str}\")\n",
    "        if capture: # Log captured output on error\n",
    "             if e.stdout: logging.error(f\"Failed command stdout:\\n{e.stdout}\")\n",
    "             if e.stderr: logging.error(f\"Failed command stderr:\\n{e.stderr}\")\n",
    "        raise # Re-raise to stop the pipeline\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred running {command_str}: {e}\")\n",
    "        raise # Re-raise to stop the pipeline\n",
    "\n",
    "def run_post_processing_pipeline():\n",
    "    \"\"\"\n",
    "    Executes a robust post-processing and verification pipeline\n",
    "    for the generated ontology models.\n",
    "    Stops immediately if any critical step fails.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Robust Post-Processing & Verification Pipeline ---\")\n",
    "    try:\n",
    "        # Preparation: Ensure directories and necessary files exist\n",
    "        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        (MODELS_DIR / \"py.typed\").touch()\n",
    "        TESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        (TESTS_DIR / \"__init__.py\").touch()\n",
    "        logging.info(\"Directories and marker files ensured.\")\n",
    "\n",
    "        # Stage 1: Formatting (Fail on error)\n",
    "        # Using Ruff format as an example, Black is also fine\n",
    "        logging.info(\"Stage 1: Formatting...\")\n",
    "        run_command([\"ruff\", \"format\", str(TARGET_DIR)], check=True)\n",
    "\n",
    "        # Stage 2: Linting & Auto-Fixing (Fail on error)\n",
    "        # Run fix for specific safe-to-fix categories first\n",
    "        logging.info(\"Stage 2a: Linting & Auto-Fixing (Imports, Syntax, Style)...\")\n",
    "        run_command([\"ruff\", \"check\", str(TARGET_DIR), \"--fix\", \"--select\", \"F\", \"--select\", \"E\", \"--select\", \"W\", \"--select\", \"I\", \"--exit-zero-even-if-fixed\"], check=True)\n",
    "        # Run check for remaining issues (don't auto-fix things like complexity)\n",
    "        logging.info(\"Stage 2b: Linting (Remaining Checks)...\")\n",
    "        run_command([\"ruff\", \"check\", str(TARGET_DIR)], check=True) # Fail if non-fixable errors remain\n",
    "\n",
    "        # Stage 3: Static Type Checking (Fail on error)\n",
    "        logging.info(\"Stage 3: Type Checking...\")\n",
    "        run_command([\"mypy\"] + MYPY_ARGS, check=True)\n",
    "\n",
    "        # Stage 4: Security Scanning (Fail on error - check output if needed)\n",
    "        logging.info(\"Stage 4: Security Scanning...\")\n",
    "        # Run bandit, capture output, decide later if specific findings should fail the build\n",
    "        bandit_result = run_command([\"bandit\"] + BANDIT_ARGS, check=False, capture=True)\n",
    "        # Example: Fail build only if Bandit found issues and exited non-zero (if not using --exit-zero)\n",
    "        # Or parse bandit_result.stdout (JSON format possible) for high-severity issues\n",
    "        # For now, we just log it based on its exit code (using --exit-zero means it won't fail the script here)\n",
    "        if bandit_result and bandit_result.returncode != 0:\n",
    "             logging.warning(\"Bandit found issues, but configured not to fail the build (--exit-zero). Review output.\")\n",
    "             # If not using --exit-zero, the run_command would have raised an error if issues found.\n",
    "\n",
    "        # Stage 5: Testing (Fail on error)\n",
    "        logging.info(\"Stage 5: Testing...\")\n",
    "        logging.warning(\"Ensure tests are present in \" + str(TESTS_DIR))\n",
    "        run_command([\"pytest\", str(TESTS_DIR)], check=True)\n",
    "\n",
    "        logging.info(\"--- Pipeline Completed Successfully ---\")\n",
    "\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError, Exception) as e:\n",
    "        # Error already logged by run_command\n",
    "        logging.critical(\"--- Pipeline Failed ---\")\n",
    "        sys.exit(1) # Exit with non-zero code to signal failure\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_post_processing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d46ee-6ecf-4ddc-8005-04c37cce56d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97dd5c-be48-4ecf-bcd9-e811a8992e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-edtech)",
   "language": "python",
   "name": "edtech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
