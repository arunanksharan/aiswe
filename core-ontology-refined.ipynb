{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01535c65-9d35-4d11-ad8d-4616eabff018",
   "metadata": {},
   "source": [
    "# Step 1: Parsing from schema.org ttl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06701993-23e2-47ac-a730-5cf5d09ed408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the input schema file path (assuming schema.txt is in the same directory)\n",
    "# In a real application, this path would be configurable.\n",
    "SCHEMA_FILE = \"schema.txt\"\n",
    "SCHEMA_FORMAT = \"turtle\" # Explicitly state the format of schema.txt\n",
    "\n",
    "# Define output path for JSON-LD (optional, for inspection)\n",
    "JSON_LD_OUTPUT_FILE = \"schema_output.jsonld\"\n",
    "\n",
    "def parse_schema_to_graph(file_path: str, file_format: str) -> rdflib.Graph | None:\n",
    "    \"\"\"\n",
    "    Parses an RDF schema file into an rdflib Graph object.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the schema file.\n",
    "        file_format: The format of the RDF file (e.g., 'turtle', 'xml', 'json-ld').\n",
    "\n",
    "    Returns:\n",
    "        An rdflib.Graph object or None if parsing fails.\n",
    "    \"\"\"\n",
    "    g = rdflib.Graph()\n",
    "    try:\n",
    "        logging.info(f\"Attempting to parse schema file: {file_path} (format: {file_format})\")\n",
    "        g.parse(source=file_path, format=file_format)\n",
    "        logging.info(f\"Successfully parsed {len(g)} triples.\")\n",
    "        return g\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Schema file not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing schema file '{file_path}' with format '{file_format}': {e}\")\n",
    "        return None\n",
    "\n",
    "def serialize_graph_to_jsonld(graph: rdflib.Graph, output_file: str) -> bool:\n",
    "    \"\"\"\n",
    "    Serializes an rdflib Graph object to a JSON-LD file.\n",
    "\n",
    "    Args:\n",
    "        graph: The rdflib.Graph object.\n",
    "        output_file: Path to save the JSON-LD output file.\n",
    "\n",
    "    Returns:\n",
    "        True if serialization was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        logging.error(\"Cannot serialize: Graph object is None.\")\n",
    "        return False\n",
    "    try:\n",
    "        logging.info(f\"Attempting to serialize graph to JSON-LD: {output_file}\")\n",
    "        # Common context for schema.org can help make JSON-LD more readable\n",
    "        # Using a standard context URL\n",
    "        context = {\n",
    "            \"@vocab\": \"https://schema.org/\"\n",
    "            # Add other prefixes if needed, though @vocab covers schema.org terms\n",
    "        }\n",
    "        # Serialize to JSON-LD format\n",
    "        # Note: rdflib's json-ld serialization might produce a list of objects\n",
    "        json_ld_data = graph.serialize(format='json-ld', context=context, indent=2)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(json_ld_data)\n",
    "        logging.info(f\"Successfully serialized graph to {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error serializing graph to JSON-LD: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d0c67-f3e2-4e3c-9e22-d2241204d072",
   "metadata": {},
   "source": [
    "### Execute Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ee2bb6-c156-467e-a0bc-2e4ace5e09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Attempting to serialize graph to JSON-LD: schema_output.jsonld\n",
      "INFO:root:Successfully serialized graph to schema_output.jsonld\n",
      "INFO:root:Graph parsed. Ready for Step 2: Schema Analysis (in next code chunk).\n"
     ]
    }
   ],
   "source": [
    "# Execute Step 1:\n",
    "# --- Main execution block for this step ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Parse the schema\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "\n",
    "    if schema_graph:\n",
    "        # Step 2 (as requested): Serialize to JSON-LD for inspection\n",
    "        serialize_graph_to_jsonld(schema_graph, JSON_LD_OUTPUT_FILE)\n",
    "\n",
    "        # Next steps (to be implemented in subsequent chunks) would involve\n",
    "        # analyzing 'schema_graph' to extract class/property info\n",
    "        # and then generating Pydantic models.\n",
    "        logging.info(\"Graph parsed. Ready for Step 2: Schema Analysis (in next code chunk).\")\n",
    "    else:\n",
    "        logging.error(\"Failed to parse schema graph. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac758fa3-afe3-4020-9171-b147a4985424",
   "metadata": {},
   "source": [
    "# Step 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbc0ea9-cb25-4b8a-9d66-9da212adfa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD # Common RDF/RDFS/OWL namespaces\n",
    "from typing import List, Set, Dict, Optional, NamedTuple\n",
    "import logging\n",
    "\n",
    "# Assuming the 'parse_schema_to_graph' function from Chunk 1 exists\n",
    "# and 'schema_graph' is the rdflib.Graph object returned by it.\n",
    "\n",
    "# Define the schema.org namespace\n",
    "SCHEMA = rdflib.Namespace(\"https://schema.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ed3148-4b2c-4d19-bda1-07ba12e3cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple structures to hold extracted info\n",
    "class PropertyInfo(NamedTuple):\n",
    "    uri: rdflib.URIRef\n",
    "    label: Optional[str]\n",
    "    comment: Optional[str]\n",
    "    domains: Set[rdflib.URIRef] # Classes where this property applies\n",
    "    ranges: Set[rdflib.URIRef] # Expected types for this property's value\n",
    "\n",
    "class ClassInfo(NamedTuple):\n",
    "    uri: rdflib.URIRef\n",
    "    label: Optional[str]\n",
    "    comment: Optional[str]\n",
    "    superclasses: Set[rdflib.URIRef] # Direct parent classes\n",
    "    properties: Set[rdflib.URIRef] # Properties associated via domainIncludes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad6e760-f699-4156-8e62-b49123fe0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(graph: rdflib.Graph, subject: rdflib.URIRef) -> Optional[str]:\n",
    "    \"\"\"Gets the rdfs:label for a subject.\"\"\"\n",
    "    label = graph.value(subject=subject, predicate=RDFS.label)\n",
    "    return str(label) if label else None\n",
    "\n",
    "def get_comment(graph: rdflib.Graph, subject: rdflib.URIRef) -> Optional[str]:\n",
    "    \"\"\"Gets the rdfs:comment for a subject.\"\"\"\n",
    "    comment = graph.value(subject=subject, predicate=RDFS.comment)\n",
    "    return str(comment) if comment else None\n",
    "\n",
    "def find_schema_classes(graph: rdflib.Graph) -> Set[rdflib.URIRef]:\n",
    "    \"\"\"Finds all subjects defined as RDFS Classes within the schema.org namespace.\"\"\"\n",
    "    classes = set()\n",
    "    # Find things explicitly declared as rdfs:Class or owl:Class\n",
    "    for class_type in [RDFS.Class, OWL.Class]:\n",
    "        for subject in graph.subjects(predicate=RDF.type, object=class_type):\n",
    "            # Filter to include only those within the schema.org namespace\n",
    "            if str(subject).startswith(str(SCHEMA)):\n",
    "                 # Also check if it's a schema.org DataType, treat those differently later\n",
    "                 is_datatype = (subject, RDF.type, SCHEMA.DataType) in graph\n",
    "                 if not is_datatype:\n",
    "                    classes.add(subject)\n",
    "    # Schema.org also defines types like schema:Person without explicitly stating\n",
    "    # rdf:type rdfs:Class in all serializations, but implies they are classes\n",
    "    # by using them in domain/range or subClassOf. A more robust approach\n",
    "    # might involve looking for usage in rdfs:subClassOf, :domainIncludes, :rangeIncludes\n",
    "    # For now, primarily rely on explicit declaration if present.\n",
    "    logging.info(f\"Found {len(classes)} potential schema.org classes.\")\n",
    "    return classes\n",
    "\n",
    "def find_schema_properties(graph: rdflib.Graph) -> Set[rdflib.URIRef]:\n",
    "    \"\"\"Finds all subjects defined as RDF Properties within the schema.org namespace.\"\"\"\n",
    "    properties = set()\n",
    "    for subject in graph.subjects(predicate=RDF.type, object=RDF.Property):\n",
    "         if str(subject).startswith(str(SCHEMA)):\n",
    "             properties.add(subject)\n",
    "    logging.info(f\"Found {len(properties)} potential schema.org properties.\")\n",
    "    return properties\n",
    "\n",
    "def get_property_details(graph: rdflib.Graph, prop_uri: rdflib.URIRef) -> PropertyInfo:\n",
    "    \"\"\"Extracts details for a given property URI.\"\"\"\n",
    "    label = get_label(graph, prop_uri)\n",
    "    comment = get_comment(graph, prop_uri)\n",
    "    domains = set(graph.objects(subject=prop_uri, predicate=SCHEMA.domainIncludes))\n",
    "    ranges = set(graph.objects(subject=prop_uri, predicate=SCHEMA.rangeIncludes))\n",
    "    return PropertyInfo(uri=prop_uri, label=label, comment=comment, domains=domains, ranges=ranges)\n",
    "\n",
    "def get_class_details(graph: rdflib.Graph, class_uri: rdflib.URIRef, all_properties: Dict[rdflib.URIRef, PropertyInfo]) -> ClassInfo:\n",
    "    \"\"\"Extracts details for a given class URI.\"\"\"\n",
    "    label = get_label(graph, class_uri)\n",
    "    comment = get_comment(graph, class_uri)\n",
    "    superclasses = set(graph.objects(subject=class_uri, predicate=RDFS.subClassOf))\n",
    "    # Find properties where this class is listed in the domain\n",
    "    associated_properties = set()\n",
    "    for prop_uri, prop_info in all_properties.items():\n",
    "        if class_uri in prop_info.domains:\n",
    "            associated_properties.add(prop_uri)\n",
    "\n",
    "    # Also consider properties inherited from superclasses (requires recursive lookup - omitted for v0.1 simplicity)\n",
    "    # For v0.1, we primarily care about properties directly associated via domainIncludes\n",
    "\n",
    "    return ClassInfo(uri=class_uri, label=label, comment=comment, superclasses=superclasses, properties=associated_properties)\n",
    "\n",
    "def analyze_schema_graph(graph: rdflib.Graph) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Analyzes the RDF graph to extract structured info about classes and properties.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing 'classes' and 'properties' information.\n",
    "    \"\"\"\n",
    "    if not graph:\n",
    "        logging.error(\"Analysis failed: Graph is empty or None.\")\n",
    "        return {\"classes\": {}, \"properties\": {}}\n",
    "\n",
    "    schema_classes_uris = find_schema_classes(graph)\n",
    "    schema_property_uris = find_schema_properties(graph)\n",
    "\n",
    "    properties_info = {}\n",
    "    for prop_uri in schema_property_uris:\n",
    "        properties_info[prop_uri] = get_property_details(graph, prop_uri)\n",
    "\n",
    "    classes_info = {}\n",
    "    for class_uri in schema_classes_uris:\n",
    "        # Skip RDFS/OWL base classes if they somehow got included\n",
    "        if class_uri in [RDFS.Resource, OWL.Thing, RDFS.Class]:\n",
    "             continue\n",
    "        classes_info[class_uri] = get_class_details(graph, class_uri, properties_info)\n",
    "\n",
    "    logging.info(f\"Analyzed {len(classes_info)} classes and {len(properties_info)} properties.\")\n",
    "    return {\"classes\": classes_info, \"properties\": properties_info}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9402cadc-7dfd-4484-aec5-431528c87cc4",
   "metadata": {},
   "source": [
    "### Execute Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ccbd60-328a-491e-b79f-93d9a9bc63f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Found 628 potential schema.org classes.\n",
      "INFO:root:Found 921 potential schema.org properties.\n",
      "INFO:root:Analyzed 628 classes and 921 properties.\n",
      "INFO:root:Schema analysis complete. Ready for Step 3: Mapping Logic Definition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analysis for schema:Person ---\n",
      "ClassInfo(uri=rdflib.term.URIRef('https://schema.org/Person'), label='Person', comment='A person (alive, dead, undead, or fictional).', superclasses={rdflib.term.URIRef('https://schema.org/Thing')}, properties={rdflib.term.URIRef('https://schema.org/gender'), rdflib.term.URIRef('https://schema.org/email'), rdflib.term.URIRef('https://schema.org/brand'), rdflib.term.URIRef('https://schema.org/deathPlace'), rdflib.term.URIRef('https://schema.org/weight'), rdflib.term.URIRef('https://schema.org/workLocation'), rdflib.term.URIRef('https://schema.org/parent'), rdflib.term.URIRef('https://schema.org/makesOffer'), rdflib.term.URIRef('https://schema.org/hasOfferCatalog'), rdflib.term.URIRef('https://schema.org/knows'), rdflib.term.URIRef('https://schema.org/duns'), rdflib.term.URIRef('https://schema.org/birthPlace'), rdflib.term.URIRef('https://schema.org/birthDate'), rdflib.term.URIRef('https://schema.org/spouse'), rdflib.term.URIRef('https://schema.org/additionalName'), rdflib.term.URIRef('https://schema.org/alumniOf'), rdflib.term.URIRef('https://schema.org/memberOf'), rdflib.term.URIRef('https://schema.org/sibling'), rdflib.term.URIRef('https://schema.org/publishingPrinciples'), rdflib.term.URIRef('https://schema.org/follows'), rdflib.term.URIRef('https://schema.org/isicV4'), rdflib.term.URIRef('https://schema.org/honorificPrefix'), rdflib.term.URIRef('https://schema.org/awards'), rdflib.term.URIRef('https://schema.org/jobTitle'), rdflib.term.URIRef('https://schema.org/givenName'), rdflib.term.URIRef('https://schema.org/children'), rdflib.term.URIRef('https://schema.org/relatedTo'), rdflib.term.URIRef('https://schema.org/colleague'), rdflib.term.URIRef('https://schema.org/siblings'), rdflib.term.URIRef('https://schema.org/skills'), rdflib.term.URIRef('https://schema.org/funder'), rdflib.term.URIRef('https://schema.org/homeLocation'), rdflib.term.URIRef('https://schema.org/taxID'), rdflib.term.URIRef('https://schema.org/owns'), rdflib.term.URIRef('https://schema.org/honorificSuffix'), rdflib.term.URIRef('https://schema.org/vatID'), rdflib.term.URIRef('https://schema.org/hasPOS'), rdflib.term.URIRef('https://schema.org/sponsor'), rdflib.term.URIRef('https://schema.org/worksFor'), rdflib.term.URIRef('https://schema.org/nationality'), rdflib.term.URIRef('https://schema.org/affiliation'), rdflib.term.URIRef('https://schema.org/contactPoint'), rdflib.term.URIRef('https://schema.org/seeks'), rdflib.term.URIRef('https://schema.org/height'), rdflib.term.URIRef('https://schema.org/award'), rdflib.term.URIRef('https://schema.org/address'), rdflib.term.URIRef('https://schema.org/faxNumber'), rdflib.term.URIRef('https://schema.org/telephone'), rdflib.term.URIRef('https://schema.org/hasOccupation'), rdflib.term.URIRef('https://schema.org/performerIn'), rdflib.term.URIRef('https://schema.org/deathDate'), rdflib.term.URIRef('https://schema.org/naics'), rdflib.term.URIRef('https://schema.org/colleagues'), rdflib.term.URIRef('https://schema.org/netWorth'), rdflib.term.URIRef('https://schema.org/familyName'), rdflib.term.URIRef('https://schema.org/parents'), rdflib.term.URIRef('https://schema.org/contactPoints'), rdflib.term.URIRef('https://schema.org/globalLocationNumber')})\n",
      "------------------------------\n",
      "\n",
      "--- Analysis for schema:address ---\n",
      "PropertyInfo(uri=rdflib.term.URIRef('https://schema.org/address'), label='address', comment='Physical address of the item.', domains={rdflib.term.URIRef('https://schema.org/Person'), rdflib.term.URIRef('https://schema.org/Organization'), rdflib.term.URIRef('https://schema.org/Place'), rdflib.term.URIRef('https://schema.org/GeoCoordinates'), rdflib.term.URIRef('https://schema.org/GeoShape')}, ranges={rdflib.term.URIRef('https://schema.org/Text'), rdflib.term.URIRef('https://schema.org/PostalAddress')})\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution block demonstrating analysis ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume schema_graph is loaded from Step 1 (re-parse for standalone demo)\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "\n",
    "    if schema_graph:\n",
    "        # Step 2: Analyze the graph\n",
    "        analyzed_schema = analyze_schema_graph(schema_graph)\n",
    "\n",
    "        # Example: Print info for schema:Person and schema:address\n",
    "        person_uri = SCHEMA.Person\n",
    "        address_prop_uri = SCHEMA.address\n",
    "\n",
    "        if person_uri in analyzed_schema[\"classes\"]:\n",
    "            print(\"\\n--- Analysis for schema:Person ---\")\n",
    "            print(analyzed_schema[\"classes\"][person_uri])\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        if address_prop_uri in analyzed_schema[\"properties\"]:\n",
    "            print(\"\\n--- Analysis for schema:address ---\")\n",
    "            print(analyzed_schema[\"properties\"][address_prop_uri])\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        # Output confirms we have structured data ready for mapping rules (Step 3)\n",
    "        # and code generation (Step 4) in the next chunks.\n",
    "        logging.info(\"Schema analysis complete. Ready for Step 3: Mapping Logic Definition.\")\n",
    "    else:\n",
    "        logging.error(\"Failed to parse schema graph for analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b8fe1-7cd8-4148-a9f9-d99b83aaaf03",
   "metadata": {},
   "source": [
    "# Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18de136-98da-49b9-a6ef-a5f796da1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, ForwardRef, Any\n",
    "from pydantic import BaseModel, Field, EmailStr, AnyUrl\n",
    "from datetime import date, datetime, time\n",
    "import re\n",
    "import keyword\n",
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, ForwardRef, Any, cast\n",
    "# Added imports for richer types\n",
    "from pydantic import BaseModel, Field, EmailStr, AnyUrl, constr, conint, condecimal, field_validator, model_validator\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import isodate # Library to parse ISO 8601 durations\n",
    "import decimal\n",
    "import keyword\n",
    "import logging\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e3df28-4580-4e9a-a05a-97a508b87fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantity(BaseModel):\n",
    "    \"\"\"\n",
    "    Base model for quantitative values based on schema.org/Quantity.\n",
    "    Actual value and unit are often in subclasses or specific properties.\n",
    "    This primarily serves as a conceptual base.\n",
    "    \"\"\"\n",
    "    model_config = {'extra': 'allow'} # Allow extra fields as Quantity is generic\n",
    "\n",
    "class Distance(Quantity):\n",
    "    \"\"\"\n",
    "    Represents a distance based on schema.org/Distance.\n",
    "    Uses value and unit representation common in QuantitativeValue.\n",
    "    \"\"\"\n",
    "    # Based on properties commonly used with QuantitativeValue for distance\n",
    "    value: Optional[float] = Field(None, description=\"The numerical value of the distance.\")\n",
    "    unitCode: Optional[str] = Field(None, description=\"UN/CEFACT Common Code (3 characters) or URL for the unit of measurement. E.g., 'MTR' for meter, 'KM' for kilometer, 'FT' for foot, 'INH' for inch.\")\n",
    "    unitText: Optional[str] = Field(None, description=\"A string indicating the unit of measurement. Useful if unitCode is not applicable or needs clarification. E.g., 'meters', 'miles'.\")\n",
    "\n",
    "    model_config = {'extra': 'forbid'}\n",
    "\n",
    "    # Add validation if needed, e.g., check unitCode format\n",
    "\n",
    "class Duration(Quantity):\n",
    "    \"\"\"\n",
    "    Represents a duration based on schema.org/Duration.\n",
    "    Stores duration as datetime.timedelta, parsed from ISO 8601 duration format.\n",
    "    \"\"\"\n",
    "    # Pydantic doesn't have native ISO 8601 duration parsing, use validator\n",
    "    value_iso8601: Optional[str] = Field(None, alias=\"iso8601Duration\", description=\"Duration in ISO 8601 format (e.g., P1Y2M3DT4H5M6S).\")\n",
    "    value_timedelta: Optional[timedelta] = Field(None, exclude=True, description=\"Parsed timedelta value (internal).\") # Exclude from standard model dump\n",
    "\n",
    "    model_config = {'extra': 'forbid', 'populate_by_name': True} # Allow using alias on input\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def parse_duration(cls, data: Any) -> Any:\n",
    "        if isinstance(data, dict):\n",
    "            iso_duration_str = data.get(\"value_iso8601\") or data.get(\"iso8601Duration\")\n",
    "            if iso_duration_str and isinstance(iso_duration_str, str):\n",
    "                try:\n",
    "                    # Use isodate library to parse ISO 8601 duration\n",
    "                    td = isodate.parse_duration(iso_duration_str)\n",
    "                    data['value_timedelta'] = td\n",
    "                    # Keep original string too\n",
    "                    data['value_iso8601'] = iso_duration_str\n",
    "                except (isodate.ISO8601Error, ValueError) as e:\n",
    "                    # Or raise validation error depending on strictness needed\n",
    "                    logging.warning(f\"Could not parse ISO 8601 duration '{iso_duration_str}': {e}\")\n",
    "                    data['value_timedelta'] = None # Set internal value to None on error\n",
    "            # If timedelta is provided directly\n",
    "            elif data.get('value_timedelta') and isinstance(data.get('value_timedelta'), timedelta):\n",
    "                 # Optionally generate ISO string if needed, though complex\n",
    "                 pass\n",
    "        elif isinstance(data, str):\n",
    "             # Allow direct initialization from ISO string\n",
    "             try:\n",
    "                 td = isodate.parse_duration(data)\n",
    "                 return {'value_iso8601': data, 'value_timedelta': td}\n",
    "             except (isodate.ISO8601Error, ValueError) as e:\n",
    "                 logging.warning(f\"Could not parse ISO 8601 duration string '{data}': {e}\")\n",
    "                 return {'value_iso8601': data, 'value_timedelta': None} # Return original string, mark as failed parse\n",
    "\n",
    "        return data # Return dict for Pydantic processing\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return ISO 8601 string representation.\"\"\"\n",
    "        if self.value_timedelta is not None:\n",
    "             try:\n",
    "                 # isodate can also format, but requires careful handling of years/months\n",
    "                 # For simplicity, return original string if present, else standard timedelta str\n",
    "                 return self.value_iso8601 or str(self.value_timedelta)\n",
    "             except Exception:\n",
    "                 return str(self.value_timedelta) # Fallback\n",
    "        return self.value_iso8601 or \"Invalid Duration\"\n",
    "\n",
    "\n",
    "class DefinedTerm(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a term from a defined set based on schema.org/DefinedTerm.\n",
    "    \"\"\"\n",
    "    # Core properties often associated with DefinedTerm\n",
    "    termCode: Optional[str] = Field(None, description=\"A code that identifies this DefinedTerm within a DefinedTermSet.\")\n",
    "    name: Optional[str] = Field(None, description=\"The name of the item.\")\n",
    "    description: Optional[str] = Field(None, description=\"A description of the item.\")\n",
    "    # Allow referencing the set it belongs to, if known\n",
    "    inDefinedTermSet: Optional[AnyUrl] = Field(None, description=\"A DefinedTermSet Organization or DataCatalog that contains this term.\")\n",
    "\n",
    "    model_config = {'extra': 'allow'} # Allow potential other properties from schema.org or extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3110bfcf-6251-404d-b2b4-02d2177ab3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mapping Configuration ---\n",
    "# Map schema.org basic types to Python/Pydantic types\n",
    "# This needs careful expansion based on VC-Zero availability and desired types\n",
    "TYPE_MAP = {\n",
    "    # Schema.org Types\n",
    "    SCHEMA.Text: \"str\",\n",
    "    SCHEMA.URL: \"pydantic.AnyUrl\",\n",
    "    SCHEMA.Date: \"datetime.date\",\n",
    "    SCHEMA.DateTime: \"datetime.datetime\",\n",
    "    SCHEMA.Time: \"datetime.time\",\n",
    "    SCHEMA.Number: \"float\",\n",
    "    SCHEMA.Float: \"float\",\n",
    "    SCHEMA.Integer: \"int\",\n",
    "    SCHEMA.Boolean: \"bool\",\n",
    "    SCHEMA.Quantity: \"Quantity\", # Map base Quantity\n",
    "    SCHEMA.Distance: \"Distance\", # Map specific Quantity subclass\n",
    "    SCHEMA.Duration: \"Duration\", # Map specific Quantity subclass\n",
    "    SCHEMA.Mass: \"Quantity\",      # Map other Quantities to base for now\n",
    "    SCHEMA.Energy: \"Quantity\",     # Map other Quantities to base for now\n",
    "    SCHEMA.CssSelectorType: \"str\",\n",
    "    SCHEMA.XPathType: \"str\",\n",
    "    SCHEMA.DefinedTerm: \"DefinedTerm\", # RICH MAPPING\n",
    "    SCHEMA.DigitalPlatformEnumeration: \"str\", # Keep as string for v0.1 enum simplicity\n",
    "    # XSD Types (ensure decimal import)\n",
    "    XSD.string: \"str\",\n",
    "    XSD.date: \"datetime.date\",\n",
    "    XSD.dateTime: \"datetime.datetime\",\n",
    "    XSD.time: \"datetime.time\",\n",
    "    XSD.integer: \"int\",\n",
    "    XSD.decimal: \"decimal.Decimal\",\n",
    "    XSD.float: \"float\",\n",
    "    XSD.double: \"float\",\n",
    "    XSD.boolean: \"bool\",\n",
    "    XSD.anyURI: \"pydantic.AnyUrl\",\n",
    "    XSD.duration: \"Duration\", # Map XSD duration too\n",
    "}\n",
    "# Known common aliases for properties\n",
    "PROPERTY_ALIAS_MAP = {\n",
    "    SCHEMA.birthDate: {'validation_alias': 'dob'},\n",
    "    # Add other common aliases as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58c1aefa-364c-49de-9134-af26e4a38519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_python_identifier(name: str) -> str:\n",
    "    \"\"\"Converts a name to a valid Python identifier, handling keywords.\"\"\"\n",
    "    if keyword.iskeyword(name):\n",
    "        return name + \"_\"\n",
    "    # Basic check for valid identifier start/characters - can be improved\n",
    "    if not name or not (name[0].isalpha() or name[0] == '_'):\n",
    "        name = '_' + name # Ensure valid start if needed\n",
    "    # Replace invalid characters (simplistic)\n",
    "    name = re.sub(r'\\W|^(?=\\d)', '_', name)\n",
    "    return name\n",
    "\n",
    "def map_uri_to_classname(uri: rdflib.URIRef) -> str:\n",
    "    \"\"\"Converts a schema.org URI to a Python CamelCase class name.\"\"\"\n",
    "    if not str(uri).startswith(str(SCHEMA)):\n",
    "        # Handle non-schema.org URIs if necessary, maybe return original or raise error\n",
    "        return str(uri).split('/')[-1].split('#')[-1] # Best guess\n",
    "    local_name = uri.replace(SCHEMA, \"\")\n",
    "    # Basic check for upper camel case, assuming schema.org mostly uses this\n",
    "    if local_name and local_name[0].isupper():\n",
    "        return safe_python_identifier(local_name)\n",
    "    else:\n",
    "        # Attempt to convert potentially lowerCamelCase or other cases\n",
    "        # This is a simple heuristic, might need refinement\n",
    "        parts = re.split(r'[-_ ]', local_name)\n",
    "        return safe_python_identifier(\"\".join(part.capitalize() for part in parts))\n",
    "\n",
    "\n",
    "def map_uri_to_fieldname(uri: rdflib.URIRef) -> str:\n",
    "    \"\"\"Converts a schema.org property URI to a Python snake_case field name.\"\"\"\n",
    "    if not str(uri).startswith(str(SCHEMA)):\n",
    "         return safe_python_identifier(str(uri).split('/')[-1].split('#')[-1].lower()) # Best guess\n",
    "    local_name = uri.replace(SCHEMA, \"\")\n",
    "    # Convert camelCase or PascalCase to snake_case\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', local_name)\n",
    "    snake_case_name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "    return safe_python_identifier(snake_case_name)\n",
    "\n",
    "def map_range_to_typehint(\n",
    "    ranges: Set[rdflib.URIRef],\n",
    "    class_registry: Set[str], # Set of known generated class names\n",
    "    default_optional: bool = True,\n",
    "    use_list_for_multi: bool = True # Assume List for properties allowing multiple values in RDF? Risky default.\n",
    "                                     # Schema.org generally doesn't use OWL cardinality.\n",
    "                                     # A safer default might be Optional[Union[T1, T2, List[T1], List[T2]]]\n",
    "                                     # For v0.1, let's keep it simpler: use Optional[Union[...]]\n",
    ") -> str:\n",
    "    \"\"\"Maps a set of RDF range URIs to a Python type hint string.\"\"\"\n",
    "    if not ranges:\n",
    "        return \"typing.Any\" # No range specified\n",
    "\n",
    "    mapped_types = set()\n",
    "    for r_uri in ranges:\n",
    "        if r_uri in TYPE_MAP:\n",
    "            mapped_types.add(TYPE_MAP[r_uri])\n",
    "        else:\n",
    "            # Assume it's a class defined in our ontology\n",
    "            class_name = map_uri_to_classname(r_uri)\n",
    "            if class_name in class_registry:\n",
    "                 # Use string literal for forward reference\n",
    "                mapped_types.add(f\"'{class_name}'\")\n",
    "            elif str(r_uri) == str(SCHEMA.Thing) or str(r_uri) == str(RDFS.Resource):\n",
    "                 mapped_types.add(\"typing.Any\") # Map generic Thing/Resource\n",
    "            else:\n",
    "                 # Unknown range URI - treat as Any or potentially raise error/warning\n",
    "                 logging.warning(f\"Unknown range URI encountered: {r_uri}. Mapping to Any.\")\n",
    "                 mapped_types.add(\"typing.Any\")\n",
    "\n",
    "    # Remove duplicates and sort for consistent output\n",
    "    unique_types = sorted(list(mapped_types))\n",
    "\n",
    "    if not unique_types:\n",
    "         return \"typing.Any\" # Should not happen if ranges is not empty, but safety check\n",
    "\n",
    "    # Build the Union if multiple types\n",
    "    type_hint_core = \"\"\n",
    "    if len(unique_types) == 1:\n",
    "        type_hint_core = unique_types[0]\n",
    "    else:\n",
    "        type_hint_core = f\"typing.Union[{', '.join(unique_types)}]\"\n",
    "\n",
    "    # Handle Optionality (defaulting to Optional for v0.1 simplicity)\n",
    "    # A more advanced version would check OWL cardinality if present\n",
    "    if default_optional:\n",
    "        # Check if None is effectively already included via Optional[...] in the union parts\n",
    "        is_already_optional = any(t.startswith(\"typing.Optional[\") or t == 'None' for t in unique_types)\n",
    "        if not is_already_optional:\n",
    "             return f\"typing.Optional[{type_hint_core}]\"\n",
    "        else:\n",
    "             # If Optional is already part of a Union, just return the Union\n",
    "             # e.g., Union[Optional['Thing'], str] is valid\n",
    "             # This logic might need refinement based on desired strictness\n",
    "             return type_hint_core\n",
    "    else:\n",
    "        return type_hint_core\n",
    "\n",
    "def get_field_metadata(prop_info: PropertyInfo) -> Dict[str, Union[str, Dict]]:\n",
    "    \"\"\"Generates arguments for pydantic.Field based on PropertyInfo.\"\"\"\n",
    "    args = {}\n",
    "    if prop_info.comment:\n",
    "        # Basic cleaning of comment string\n",
    "        clean_comment = ' '.join(prop_info.comment.split())\n",
    "        args['description'] = clean_comment\n",
    "    # Add aliases for common variations\n",
    "    if prop_info.uri in PROPERTY_ALIAS_MAP:\n",
    "        # Pydantic v2 alias handling might differ slightly, adjust as needed\n",
    "        for alias_type, alias_value in PROPERTY_ALIAS_MAP[prop_info.uri].items():\n",
    "             args[alias_type] = alias_value # e.g., validation_alias='dob'\n",
    "\n",
    "    # Example: Add examples if available (assuming they could be parsed from RDF)\n",
    "    # if prop_info.examples: args['examples'] = prop_info.examples\n",
    "\n",
    "    # Default value is None for Optional fields, handled by type hint + Field(None)\n",
    "    # Required fields would have no default in Field()\n",
    "    # We default to Optional, so default is usually None\n",
    "    default_value = None\n",
    "\n",
    "    field_args_str = f\"default={default_value}\"\n",
    "    if args:\n",
    "        args_repr = ', '.join(f\"{k}={repr(v)}\" for k, v in args.items())\n",
    "        field_args_str += f\", {args_repr}\"\n",
    "\n",
    "    # Return structure suitable for formatting into Field(...) call\n",
    "    # Returning dict for easier manipulation before final string formatting\n",
    "    return {'default': default_value, **args}\n",
    "\n",
    "\n",
    "### Not in use\n",
    "def get_all_ancestors(graph: rdflib.Graph, class_uri: rdflib.URIRef, known_classes_uris: Set[rdflib.URIRef]) -> Set[rdflib.URIRef]:\n",
    "    \"\"\"Recursively find all superclass URIs for a given class URI within our known set.\"\"\"\n",
    "    ancestors = set()\n",
    "    # Use graph.transitive_objects for potentially more efficient RDFS reasoning if needed,\n",
    "    # but manual recursion works fine.\n",
    "    parents = set(graph.objects(subject=class_uri, predicate=RDFS.subClassOf))\n",
    "    for parent_uri in parents:\n",
    "        # Include direct parent only if it's one we are generating/tracking\n",
    "        # and avoid recursing into excessively generic external types like owl:Thing\n",
    "        if parent_uri in known_classes_uris and parent_uri not in [RDFS.Resource, OWL.Thing]:\n",
    "            if parent_uri not in ancestors: # Avoid infinite loops in case of cycles in data\n",
    "                ancestors.add(parent_uri)\n",
    "                ancestors.update(get_all_ancestors(graph, parent_uri, known_classes_uris)) # Recurse\n",
    "    return ancestors\n",
    "### Not in use\n",
    "\n",
    "\n",
    "def get_base_classes(class_info: ClassInfo, class_registry: Set[str]) -> List[str]:\n",
    "    \"\"\"Determines the base classes for a Pydantic model.\"\"\"\n",
    "    base_class_names = set()\n",
    "    if class_info.superclasses:\n",
    "        for super_uri in class_info.superclasses:\n",
    "             # Only inherit from known classes within our generated set or schema base types we handle\n",
    "             mapped_name = map_uri_to_classname(super_uri)\n",
    "             # Avoid inheriting from extremely generic types unless necessary\n",
    "             # and ensure it's a class we are actually generating\n",
    "             if mapped_name in class_registry and mapped_name not in ['Thing']:\n",
    "                 base_class_names.add(mapped_name)\n",
    "\n",
    "    if not base_class_names:\n",
    "        # All models ultimately inherit from pydantic.BaseModel\n",
    "        # If no *other* valid parent identified, use BaseModel directly\n",
    "        # This handles top-level classes like Thing (if we generated it) or classes\n",
    "        # whose parents aren't in our selected scope.\n",
    "         base_class_names.add(\"pydantic.BaseModel\")\n",
    "    else:\n",
    "        # If inheriting from other generated models, ensure BaseModel is implicitly included\n",
    "        # Pydantic handles this automatically if the parents inherit from BaseModel.\n",
    "        pass\n",
    "\n",
    "    # Sort for consistent order, although MRO is determined by Python\n",
    "    return sorted(list(base_class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18854179-bf02-47e0-a408-5048a3436357",
   "metadata": {},
   "source": [
    "### Execute Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a7bcf8-aa25-45b0-ba75-45a24df2b036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for schema:address ranges ({rdflib.term.URIRef('https://schema.org/Text'), rdflib.term.URIRef('https://schema.org/PostalAddress')}):\n",
      " -> Type Hint: typing.Optional[typing.Union['PostalAddress', str]]\n",
      "\n",
      "Mapping for schema:givenName ranges ({rdflib.term.URIRef('https://schema.org/Text')}):\n",
      " -> Type Hint: typing.Optional[str]\n",
      "\n",
      "Mapping for schema:Person superclasses ({rdflib.term.URIRef('https://schema.org/Thing')}):\n",
      " -> Base Classes: ['pydantic.BaseModel']\n",
      "\n",
      "Mapping logic defined. Ready for Step 4: Pydantic Class Code Generation.\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage (Conceptual - assumes 'analyzed_schema' from Step 2 exists) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # This block is illustrative; the actual generator script (Step 4)\n",
    "    # will use these functions extensively.\n",
    "\n",
    "    # Placeholder for the registry of class names we intend to generate\n",
    "    # In the real generator, this would be populated from analyzed_schema['classes']\n",
    "    # known_generated_classes = {map_uri_to_classname(uri) for uri in analyzed_schema[\"classes\"]}\n",
    "    known_generated_classes = {\"Person\", \"PostalAddress\", \"Thing\"} # Manual example\n",
    "\n",
    "    # Example: Map range for schema:address\n",
    "    address_prop_uri = SCHEMA.address\n",
    "    # Assume address_prop_info = analyzed_schema['properties'][address_prop_uri]\n",
    "    # Example ranges for address: {SCHEMA.PostalAddress, SCHEMA.Text}\n",
    "    address_ranges = {SCHEMA.PostalAddress, SCHEMA.Text}\n",
    "    address_type_hint = map_range_to_typehint(address_ranges, known_generated_classes)\n",
    "    print(f\"Mapping for schema:address ranges ({address_ranges}):\")\n",
    "    print(f\" -> Type Hint: {address_type_hint}\") # Expect Optional[Union['PostalAddress', str]]\n",
    "\n",
    "    # Example: Map range for schema:givenName\n",
    "    givenname_prop_uri = SCHEMA.givenName\n",
    "    # Assume givenname_prop_info = analyzed_schema['properties'][givenname_prop_uri]\n",
    "    # Example ranges for givenName: {SCHEMA.Text}\n",
    "    givenname_ranges = {SCHEMA.Text}\n",
    "    givenname_type_hint = map_range_to_typehint(givenname_ranges, known_generated_classes)\n",
    "    print(f\"\\nMapping for schema:givenName ranges ({givenname_ranges}):\")\n",
    "    print(f\" -> Type Hint: {givenname_type_hint}\") # Expect Optional[str]\n",
    "\n",
    "    # Example: Get base classes for schema:Person\n",
    "    person_uri = SCHEMA.Person\n",
    "    # Assume person_class_info = analyzed_schema['classes'][person_uri]\n",
    "    # Example superclasses for Person: {SCHEMA.Thing}\n",
    "    person_superclasses_uris = {SCHEMA.Thing}\n",
    "    # Simulate ClassInfo structure\n",
    "    class PersonInfoSim(NamedTuple): superclasses: Set; uri: Any=None; label: Any=None; comment: Any=None; properties: Any=None\n",
    "    person_class_info_sim = PersonInfoSim(superclasses=person_superclasses_uris)\n",
    "    person_bases = get_base_classes(person_class_info_sim, known_generated_classes)\n",
    "    print(f\"\\nMapping for schema:Person superclasses ({person_superclasses_uris}):\")\n",
    "    print(f\" -> Base Classes: {person_bases}\") # Expect ['BaseModel'] if Thing excluded, or ['Thing'] if Thing generated\n",
    "\n",
    "    print(\"\\nMapping logic defined. Ready for Step 4: Pydantic Class Code Generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec3816c-6ac9-4421-934d-7a215f199dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a22e07a-2a86-40ce-b9c5-9cddff94ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, ForwardRef, Any, cast, TYPE_CHECKING # Added TYPE_CHECKING\n",
    "from pydantic import BaseModel, Field, EmailStr, AnyUrl\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import isodate\n",
    "import decimal\n",
    "import keyword\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"output_ontology\"\n",
    "MODELS_SUBDIR = \"models\"\n",
    "BASE_ONTOLOGY_MODULE = \"core_ontology_v0_1\" # Used for potential future imports if split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df271c1a-1b59-46f4-81fa-43046589fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper to get import paths (adjust module structure if needed) ---\n",
    "def get_module_path_for_class(class_name: str) -> str:\n",
    "    \"\"\"Determines the expected module name for a given class name.\"\"\"\n",
    "    # Assumes snake_case filename based on class name\n",
    "    # This might need adjustment if filename generation logic changes\n",
    "    potential_field_name = map_uri_to_fieldname(SCHEMA[class_name]) # Hacky way to get snake_case\n",
    "    return f\".{safe_python_identifier(potential_field_name)}\" # Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d6f66b-e610-4cd8-b3aa-53d1ba2eefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pydantic_model_code(\n",
    "    class_info: ClassInfo,\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo],\n",
    "    all_class_names: Set[str] # All class names being generated\n",
    ") -> str:\n",
    "    \"\"\"Generates the Python code string for a single Pydantic model, including robust imports.\"\"\"\n",
    "\n",
    "    class_name = map_uri_to_classname(class_info.uri)\n",
    "    \n",
    "    base_uris = class_info.superclasses\n",
    "    potential_base_names = {map_uri_to_classname(uri) for uri in base_uris}\n",
    "    valid_base_names = sorted([name for name in potential_base_names if name in all_class_names and name != 'Thing'])\n",
    "\n",
    "    # --- START: Revised Import Handling ---\n",
    "    # Use sets to automatically handle duplicates\n",
    "    core_imports = set()\n",
    "    typing_imports_specific = set() # Specific items needed from typing\n",
    "    pydantic_imports_specific = set() # Always need these\n",
    "    datetime_imports_specific = set()\n",
    "    other_imports = set() # For things like decimal, isodate\n",
    "    cross_module_imports = set() # For other generated models (used in TYPE_CHECKING)\n",
    "    rich_type_imports = set()\n",
    "    runtime_base_class_imports = set() \n",
    "\n",
    "    # Determine base class string and imports needed for bases\n",
    "    if not valid_base_names:\n",
    "        base_class_str = \"BaseModel\"\n",
    "        pydantic_imports_specific.add(\"BaseModel\")\n",
    "    else:\n",
    "        base_class_str = \", \".join(valid_base_names)\n",
    "        for base_name in valid_base_names:\n",
    "             # Assume base classes are in other modules within the same package\n",
    "            cross_module_imports.add(base_name)\n",
    "            # runtime_base_class_imports.add(f\"from .{get_module_path_for_class(base_name)} import {base_name}\")\n",
    "\n",
    "    # Analyze fields to determine necessary imports\n",
    "    field_definitions = []\n",
    "    sorted_property_uris = sorted(list(class_info.properties))\n",
    "    field_added = False\n",
    "\n",
    "    for prop_uri in sorted_property_uris:\n",
    "        if prop_uri not in properties_info: continue # Skip if property info missing\n",
    "\n",
    "        prop_info = properties_info[prop_uri]\n",
    "        field_name = map_uri_to_fieldname(prop_info.uri)\n",
    "        type_hint_str = map_range_to_typehint(prop_info.ranges, all_class_names)\n",
    "\n",
    "        # --- Refined Import Tracking based on Type Hint String ---\n",
    "        if \"Optional\" in type_hint_str: typing_imports_specific.add(\"Optional\")\n",
    "        if \"List\" in type_hint_str: typing_imports_specific.add(\"List\")\n",
    "        if \"Union\" in type_hint_str: typing_imports_specific.add(\"Union\")\n",
    "        if \"Any\" in type_hint_str: typing_imports_specific.add(\"Any\")\n",
    "        # Add VC-Zero/rich types if they come from specific modules\n",
    "        if \"datetime.date\" in type_hint_str: datetime_imports_specific.add(\"date\")\n",
    "        if \"datetime.datetime\" in type_hint_str: datetime_imports_specific.add(\"datetime\")\n",
    "        if \"datetime.time\" in type_hint_str: datetime_imports_specific.add(\"time\")\n",
    "        if \"timedelta\" in type_hint_str: datetime_imports_specific.add(\"timedelta\")\n",
    "        if \"decimal.Decimal\" in type_hint_str: other_imports.add(\"import decimal\")\n",
    "        # Add imports for Pydantic types used\n",
    "        if \"pydantic.AnyUrl\" in type_hint_str: pydantic_imports_specific.add(\"AnyUrl\")\n",
    "        if \"pydantic.EmailStr\" in type_hint_str: pydantic_imports_specific.add(\"EmailStr\")\n",
    "        # Add other specific pydantic types as needed based on TYPE_MAP\n",
    "        if \"Quantity\" in type_hint_str: rich_type_imports.add(\"Quantity\")\n",
    "        if \"Distance\" in type_hint_str: rich_type_imports.add(\"Distance\")\n",
    "        if \"Duration\" in type_hint_str: rich_type_imports.add(\"Duration\")\n",
    "        if \"DefinedTerm\" in type_hint_str: rich_type_imports.add(\"DefinedTerm\")\n",
    "\n",
    "        # Track cross-module imports needed for type hints (used within TYPE_CHECKING)\n",
    "        potential_classes_in_hint = set(re.findall(r\"'(\\w+)'\", type_hint_str))\n",
    "        for potential_class in potential_classes_in_hint:\n",
    "             if potential_class in all_class_names and potential_class != class_name:\n",
    "                 cross_module_imports.add(potential_class)\n",
    "        # --- End Refined Import Tracking ---\n",
    "\n",
    "        field_args_dict = get_field_metadata(prop_info)\n",
    "        # Generate Field(...) call (same as before)\n",
    "        field_args_parts = []\n",
    "        default_val_repr = repr(field_args_dict.pop('default', None))\n",
    "        field_args_parts.append(default_val_repr)\n",
    "        field_args_parts.extend(f\"{k}={repr(v)}\" for k, v in field_args_dict.items())\n",
    "        field_call = f\"Field({', '.join(field_args_parts)})\"\n",
    "\n",
    "        # *** CRITICAL FIX: Ensure type hints use the *imported names*, not module prefixes ***\n",
    "        # Basic replacement - more robust parsing might be needed for complex nested hints\n",
    "        final_type_hint = type_hint_str.replace(\"pydantic.\", \"\") # Remove prefix if imported directly\n",
    "        final_type_hint = final_type_hint.replace(\"datetime.\", \"\") # Remove prefix if imported directly\n",
    "        # Replace typing prefixes only if specific types are imported\n",
    "        if typing_imports_specific:\n",
    "            final_type_hint = final_type_hint.replace(\"typing.\", \"\")\n",
    "\n",
    "        field_definitions.append(f\"    {field_name}: {final_type_hint} = {field_call}\")\n",
    "        field_added = True\n",
    "\n",
    "    # Filter out base classes already imported at runtime from type-hint-only imports\n",
    "    typehint_only_imports = cross_module_imports - set(valid_base_names)\n",
    "    # print(f\"Type-hint-only forward refs: {typehint_only_imports}\")\n",
    "\n",
    "    will_generate_type_checking_block = bool(typehint_only_imports)\n",
    "\n",
    "    if will_generate_type_checking_block:\n",
    "        typing_imports_specific.add(\"TYPE_CHECKING\") # Only add it if the block will exist\n",
    "    # print(f\"Will generate TYPE_CHECKING block? {will_generate_type_checking_block}\")\n",
    "\n",
    "    # --- Assemble the full class code with Corrected Imports ---\n",
    "    code_parts = []\n",
    "    code_parts.append(\"from __future__ import annotations\") # Keep this first\n",
    "\n",
    "    # Add standard library imports\n",
    "    if datetime_imports_specific:\n",
    "        code_parts.append(f\"from datetime import {', '.join(sorted(list(datetime_imports_specific)))}\")\n",
    "    code_parts.extend(sorted(list(other_imports))) # Like 'import decimal'\n",
    "\n",
    "    # Generate the main typing import line (if needed)\n",
    "    if typing_imports_specific:\n",
    "        code_parts.append(f\"from typing import {', '.join(sorted(list(typing_imports_specific)))}\")\n",
    "\n",
    "    # Add Pydantic imports\n",
    "    if field_added:\n",
    "        pydantic_imports_specific.add(\"Field\")\n",
    "    if pydantic_imports_specific:\n",
    "        code_parts.append(f\"from pydantic import {', '.join(sorted(list(pydantic_imports_specific)))}\")\n",
    "    logging.debug(f\"Final pydantic imports needed: {pydantic_imports_specific}\")\n",
    "\n",
    "    code_parts.extend(sorted(list(runtime_base_class_imports)))\n",
    "\n",
    "    if rich_type_imports:\n",
    "         code_parts.append(f\"from .base_types import {', '.join(sorted(list(rich_type_imports)))}\")\n",
    "\n",
    "\n",
    "    # Add forward reference imports within TYPE_CHECKING block\n",
    "    # Only include classes that are *not* base classes (already imported above if needed)\n",
    "    typehint_only_imports = cross_module_imports - set(valid_base_names)\n",
    "    if typehint_only_imports:\n",
    "        # code_parts.append(\"\\nif TYPE_CHECKING:\")\n",
    "        for class_to_import in sorted(list(typehint_only_imports)):\n",
    "            code_parts.append(f\"from {get_module_path_for_class(class_to_import)} import {class_to_import}\")\n",
    "\n",
    "    if valid_base_names:\n",
    "        # Ensure base classes are imported for the class definition line\n",
    "        # This might duplicate imports already added above but ensures availability\n",
    "        for base_name in valid_base_names:\n",
    "            runtime_base_class_imports.add(f\"from {get_module_path_for_class(base_name)} import {base_name}\")\n",
    "    \n",
    "    code_parts.extend(sorted(list(runtime_base_class_imports)))\n",
    "    code_parts.append(\"\\n\") # Separator\n",
    "\n",
    "    # Add class definition\n",
    "    class_docstring = f'\"\"\"\\n    {class_name}: {textwrap.shorten(class_info.comment or \"No description provided.\", width=70)}\\n\\n    Generated from: {class_info.uri}\\n    \"\"\"'\n",
    "    code_parts.append(f\"class {class_name}({base_class_str}):\")\n",
    "    code_parts.append(f\"    {class_docstring}\")\n",
    "\n",
    "    if not field_definitions:\n",
    "        code_parts.append(\"    pass\")\n",
    "    else:\n",
    "        code_parts.extend(field_definitions)\n",
    "\n",
    "    code_parts.append(\"\\n    model_config = {'extra': 'forbid'}\")\n",
    "\n",
    "    return \"\\n\".join(code_parts) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fba141a-1057-4ae7-9ee5-4cb467a6aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TYPES_CODE = \"\"\"\n",
    "from __future__ import annotations # Keep first\n",
    "from pydantic import (\n",
    "    BaseModel, Field, AnyUrl, field_validator,\n",
    "    model_validator, condecimal, constr, EmailStr # Added EmailStr just in case, adjust as needed\n",
    ")\n",
    "from typing import Optional, List, Union, Any\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import decimal\n",
    "import isodate # Requires: pip install isodate\n",
    "import logging\n",
    "\n",
    "# Configure basic logging if needed within this module too\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__) # Use logger for warnings\n",
    "\n",
    "class Quantity(BaseModel):\n",
    "    \\\"\\\"\\\"\n",
    "    Base model for quantitative values based on schema.org/Quantity.\n",
    "    Actual value and unit are often in subclasses or specific properties.\n",
    "    This primarily serves as a conceptual base.\n",
    "    \\\"\\\"\\\"\n",
    "    model_config = {'extra': 'allow'} # Allow extra fields as Quantity is generic\n",
    "\n",
    "class Distance(Quantity):\n",
    "    \\\"\\\"\\\"\n",
    "    Represents a distance based on schema.org/Distance.\n",
    "    Uses value and unit representation common in QuantitativeValue.\n",
    "    \\\"\\\"\\\"\n",
    "    # Based on properties commonly used with QuantitativeValue for distance\n",
    "    value: Optional[float] = Field(None, description=\"The numerical value of the distance.\")\n",
    "    unitCode: Optional[str] = Field(None, description=\"UN/CEFACT Common Code (3 characters) or URL for the unit of measurement. E.g., 'MTR' for meter, 'KM' for kilometer, 'FT' for foot, 'INH' for inch.\")\n",
    "    unitText: Optional[str] = Field(None, description=\"A string indicating the unit of measurement. Useful if unitCode is not applicable or needs clarification. E.g., 'meters', 'miles'.\")\n",
    "\n",
    "    model_config = {'extra': 'forbid'}\n",
    "\n",
    "    # Add validation if needed, e.g., check unitCode format\n",
    "\n",
    "class Duration(Quantity):\n",
    "    \\\"\\\"\\\"\n",
    "    Represents a duration based on schema.org/Duration.\n",
    "    Stores duration as datetime.timedelta, parsed from ISO 8601 duration format.\n",
    "    \\\"\\\"\\\"\n",
    "    # Pydantic doesn't have native ISO 8601 duration parsing, use validator\n",
    "    # Use alias to allow input using schema.org's likely property name if it differs\n",
    "    value_iso8601: Optional[str] = Field(None, validation_alias='iso8601Duration', serialization_alias='iso8601Duration', description=\"Duration in ISO 8601 format (e.g., P1Y2M3DT4H5M6S).\")\n",
    "    value_timedelta: Optional[timedelta] = Field(None, exclude=True, description=\"Parsed timedelta value (internal).\") # Exclude from standard model dump\n",
    "\n",
    "    model_config = {'extra': 'forbid', 'populate_by_name': True} # Allow using alias on input\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def parse_duration(cls, data: Any) -> Any:\n",
    "        if isinstance(data, dict):\n",
    "            iso_duration_str = data.get(\"value_iso8601\") or data.get(\"iso8601Duration\")\n",
    "            # Parse only if timedelta isn't already provided and string exists\n",
    "            if iso_duration_str and isinstance(iso_duration_str, str) and 'value_timedelta' not in data:\n",
    "                try:\n",
    "                    td = isodate.parse_duration(iso_duration_str)\n",
    "                    data['value_timedelta'] = td\n",
    "                    data['value_iso8601'] = iso_duration_str # Ensure original is stored\n",
    "                except (isodate.ISO8601Error, ValueError) as e:\n",
    "                    logger.warning(f\"Could not parse ISO 8601 duration '{iso_duration_str}': {e}\")\n",
    "                    data['value_timedelta'] = None\n",
    "                    data['value_iso8601'] = iso_duration_str # Keep original invalid string\n",
    "            # If timedelta provided directly, ensure value_timedelta field is populated\n",
    "            elif data.get('value_timedelta') and isinstance(data.get('value_timedelta'), timedelta):\n",
    "                 pass # Already populated\n",
    "        elif isinstance(data, str):\n",
    "             # Allow direct initialization from ISO string\n",
    "             try:\n",
    "                 td = isodate.parse_duration(data)\n",
    "                 return {'value_iso8601': data, 'value_timedelta': td}\n",
    "             except (isodate.ISO8601Error, ValueError) as e:\n",
    "                 logger.warning(f\"Could not parse ISO 8601 duration string '{data}': {e}\")\n",
    "                 return {'value_iso8601': data, 'value_timedelta': None}\n",
    "\n",
    "        return data # Return dict for Pydantic processing\n",
    "\n",
    "    # Optional: Add property to access timedelta easily\n",
    "    @property\n",
    "    def timedelta(self) -> Optional[timedelta]:\n",
    "        return self.value_timedelta\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \\\"\\\"\\\"Return ISO 8601 string representation if available.\\\"\\\"\\\"\n",
    "        # Prefer original string if available, otherwise format timedelta (basic)\n",
    "        if self.value_iso8601:\n",
    "            return self.value_iso8601\n",
    "        elif self.value_timedelta is not None:\n",
    "             try:\n",
    "                 # Attempt basic formatting back (might lose fidelity vs isodate.duration_isoformat)\n",
    "                 return str(self.value_timedelta)\n",
    "             except Exception:\n",
    "                 return \"Invalid Duration Timedelta\"\n",
    "        return \"Invalid/Missing Duration\"\n",
    "\n",
    "\n",
    "class DefinedTerm(BaseModel):\n",
    "    \\\"\\\"\\\"\n",
    "    Represents a term from a defined set based on schema.org/DefinedTerm.\n",
    "    \\\"\\\"\\\"\n",
    "    # Core properties often associated with DefinedTerm\n",
    "    termCode: Optional[str] = Field(None, description=\"A code that identifies this DefinedTerm within a DefinedTermSet.\")\n",
    "    name: Optional[str] = Field(None, description=\"The name of the item.\")\n",
    "    description: Optional[str] = Field(None, description=\"A description of the item.\")\n",
    "    # Allow referencing the set it belongs to, if known (using AnyUrl for flexibility)\n",
    "    inDefinedTermSet: Optional[AnyUrl] = Field(None, description=\"A DefinedTermSet Organization or DataCatalog that contains this term.\")\n",
    "\n",
    "    model_config = {'extra': 'allow'} # Allow potential other properties from schema.org or extensions\n",
    "\n",
    "class Money(BaseModel):\n",
    "     \\\"\\\"\\\"\n",
    "     Represents an amount of money with a currency. Based on schema.org concepts\n",
    "     often used with PriceSpecification or MonetaryAmount.\n",
    "     \\\"\\\"\\\"\n",
    "     # Using 'amount' and 'currency' inspired by common patterns, not a direct schema.org/Money type\n",
    "     amount: Optional[decimal.Decimal] = Field(None, description=\"The amount of money.\")\n",
    "     currency: Optional[constr(pattern=r'^[A-Z]{3}$')] = Field(None, description=\"ISO 4217 Currency Code\") # type: ignore\n",
    "\n",
    "     @field_validator('amount', mode='before')\n",
    "     @classmethod\n",
    "     def clean_amount(cls, v: Any) -> Optional[decimal.Decimal]: # Added type hints\n",
    "         # Indentation Level 2 (Inside Function)\n",
    "         if isinstance(v, (int, float)):\n",
    "             try: # Indentation Level 3\n",
    "                 return decimal.Decimal(v) # Indentation Level 4\n",
    "             except Exception as e: # Indentation Level 3\n",
    "                  logger.error(f\"Error converting {v} to Decimal: {e}\")\n",
    "                  raise ValueError(f\"Cannot convert {v} to Decimal\") # Indentation Level 4\n",
    "         if isinstance(v, str): # Indentation Level 2\n",
    "             try: # Indentation Level 3\n",
    "                 return decimal.Decimal(v.strip()) # Indentation Level 4\n",
    "             except decimal.InvalidOperation: # Indentation Level 3\n",
    "                  raise ValueError(f\"Invalid decimal format for amount: {v}\") # Indentation Level 4\n",
    "         # Allow existing Decimals or None to pass through\n",
    "         if isinstance(v, decimal.Decimal) or v is None: # Indentation Level 2\n",
    "              return v # Indentation Level 3\n",
    "         # Raise error for other unexpected types\n",
    "         raise ValueError(f\"Unexpected type for amount: {type(v)}\") # Indentation Level 2\n",
    "\n",
    "     model_config = {'extra': 'forbid'}\n",
    "\n",
    "# Add other base types/VC-Zeros below if needed\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "908bbb7e-ad9f-461a-83fc-fcf04b579921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "from typing import List, Set, Dict, Optional, NamedTuple, Union, Any, cast # Ensure necessary types for function signature\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import shlex # If used by helper functions, ensure imported\n",
    "import textwrap # If used by helper functions\n",
    "\n",
    "logger = logging.getLogger(__name__) # Define logger for use within function\n",
    "\n",
    "def generate_ontology_models(\n",
    "     analyzed_schema: Dict[str, Dict],\n",
    "     output_base_dir: str,\n",
    "     models_subdir: str = \"models\"\n",
    "     ) -> None: # Added return type hint\n",
    "    \"\"\"\n",
    "    Generates Pydantic model files from analyzed schema info, including base_types.py\n",
    "    and a correctly structured __init__.py.\n",
    "    \"\"\"\n",
    "\n",
    "    classes_info: Dict[rdflib.URIRef, ClassInfo] = analyzed_schema.get(\"classes\", {})\n",
    "    properties_info: Dict[rdflib.URIRef, PropertyInfo] = analyzed_schema.get(\"properties\", {})\n",
    "\n",
    "    if not classes_info: # Check classes_info primarily\n",
    "        logging.error(\"No class information found in analyzed schema. Cannot generate models.\")\n",
    "        return\n",
    "    if not properties_info:\n",
    "         logging.warning(\"No property information found in analyzed schema. Models may lack fields.\")\n",
    "         # Proceed cautiously, or return depending on desired strictness\n",
    "\n",
    "    output_path = pathlib.Path(output_base_dir) / models_subdir\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    # Ensure main __init__.py exists before generating submodules\n",
    "    (output_path / \"__init__.py\").touch()\n",
    "\n",
    "    # Define URIs and names for types handled ONLY in base_types.py\n",
    "    base_type_uris = {\n",
    "        SCHEMA.Quantity, SCHEMA.Distance, SCHEMA.Duration, SCHEMA.DefinedTerm,\n",
    "        # Add SCHEMA.Money if defined in BASE_TYPES_CODE\n",
    "        SCHEMA.Money\n",
    "    }\n",
    "    base_type_names = {map_uri_to_classname(uri) for uri in base_type_uris if uri in classes_info} # Get names only for types actually present\n",
    "\n",
    "    # --- Generate base_types.py ---\n",
    "    base_types_path = output_path / \"base_types.py\"\n",
    "    try:\n",
    "        # Assuming BASE_TYPES_CODE string is defined globally or passed in\n",
    "        global BASE_TYPES_CODE\n",
    "        if 'BASE_TYPES_CODE' not in globals():\n",
    "             raise NameError(\"BASE_TYPES_CODE string not found.\")\n",
    "\n",
    "        with open(base_types_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(BASE_TYPES_CODE)\n",
    "        logging.info(f\"Generated base types file: {base_types_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write base_types.py: {e}\", exc_info=True)\n",
    "        return # Stop if base types cannot be written\n",
    "\n",
    "    all_class_names = {map_uri_to_classname(uri) for uri in classes_info.keys()}\n",
    "    # Add base type names to the known class registry for type hinting purposes\n",
    "    all_class_names.update(base_type_names)\n",
    "\n",
    "    # --- (Optional but recommended) Topological Sort for Generation Order ---\n",
    "    # Using alphabetical as fallback, assuming TYPE_CHECKING handles most cycles\n",
    "    generation_order_uris = sorted(list(classes_info.keys()))\n",
    "    logging.info(\"Generating models in alphabetical order by URI.\")\n",
    "    # Add more robust sorting here if needed based on hierarchy\n",
    "\n",
    "\n",
    "    # --- Generate individual model files, SKIPPING base types ---\n",
    "    generated_files = 0\n",
    "    # **** Store mapping from module name part to main class name ****\n",
    "    module_to_class_map: Dict[str, str] = {}\n",
    "\n",
    "    for class_uri in generation_order_uris:\n",
    "        if class_uri not in classes_info: continue\n",
    "\n",
    "        # **** CORRECTED SKIP LOGIC ****\n",
    "        if class_uri in base_type_uris:\n",
    "            logging.debug(f\"Skipping individual file generation for {class_uri} (defined in base_types.py)\")\n",
    "            continue\n",
    "        # **** END CORRECTION ****\n",
    "\n",
    "        class_info = classes_info[class_uri]\n",
    "        class_name = map_uri_to_classname(class_uri)\n",
    "        module_name_part = map_uri_to_fieldname(class_uri) # snake_case name for file/module\n",
    "\n",
    "        try:\n",
    "            model_code = generate_pydantic_model_code(class_info, properties_info, all_class_names)\n",
    "            file_path = output_path / f\"{module_name_part}.py\"\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(model_code)\n",
    "\n",
    "            # **** Store mapping IF generation succeeded ****\n",
    "            module_to_class_map[module_name_part] = class_name\n",
    "            generated_files += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate code for class {class_name} ({class_uri}): {e}\", exc_info=True) # Add traceback\n",
    "\n",
    "    logging.info(f\"Generated {generated_files} specific Pydantic model files in {output_path}\")\n",
    "\n",
    "\n",
    "    # --- *** CORRECTED __init__.py Generation *** ---\n",
    "    init_py_path = output_path / \"__init__.py\"\n",
    "    try:\n",
    "        with open(init_py_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# flake8: noqa\\n\")\n",
    "            f.write(\"# Auto-generated __init__.py for ontology models\\n\\n\")\n",
    "            f.write(\"import logging\\n\")\n",
    "            f.write(\"import importlib\\n\")\n",
    "            f.write(\"import pkgutil\\n\")\n",
    "            f.write(\"from typing import TYPE_CHECKING # TYPE_CHECKING might be needed by rebuild_all\\n\")\n",
    "            f.write(\"from pydantic import BaseModel # Needed for rebuild_all check\\n\\n\")\n",
    "\n",
    "            f.write(\"logger: logging.Logger = logging.getLogger(__name__)\\n\\n\") # Define early\n",
    "\n",
    "            # 1. Import explicitly from base_types\n",
    "            if base_type_names:\n",
    "                f.write(\"# --- Import Base Types ---\\n\")\n",
    "                f.write(\"try:\\n\")\n",
    "                f.write(f\"    from .base_types import ({', '.join(sorted(list(base_type_names)))})\\n\") # Explicit names\n",
    "                f.write(\"except ImportError:\\n\")\n",
    "                f.write(\"    logger.warning('Could not import base_types')\\n\\n\")\n",
    "\n",
    "            # 2. Import explicitly from all OTHER generated modules\n",
    "            if module_to_class_map:\n",
    "                 f.write(\"# --- Import Generated Models ---\\n\")\n",
    "            generated_class_names = set()\n",
    "            for module_name_part in sorted(module_to_class_map.keys()):\n",
    "                class_name = module_to_class_map[module_name_part]\n",
    "                # Ensure we don't try to re-import base types if map is somehow wrong\n",
    "                if class_name not in base_type_names:\n",
    "                    f.write(f\"try:\\n\")\n",
    "                    f.write(f\"    from .{module_name_part} import {class_name}\\n\")\n",
    "                    generated_class_names.add(class_name)\n",
    "                    f.write(f\"except ImportError:\\n\")\n",
    "                    f.write(f\"    logger.warning(f'Could not import {class_name} from .{module_name_part}')\\n\")\n",
    "\n",
    "            # 3. Define __all__ for cleaner namespace\n",
    "            all_names = sorted(list(base_type_names | generated_class_names))\n",
    "            f.write(\"\\n__all__ = [\\n\")\n",
    "            for name in all_names:\n",
    "                f.write(f'    \"{name}\",\\n') # Use double quotes for names in list\n",
    "            f.write(\"]\\n\\n\")\n",
    "\n",
    "\n",
    "            # 4. Include rebuild_all function and call (with fixed annotation)\n",
    "            f.write(\"# --- Rebuild models to resolve forward references ---\\n\")\n",
    "            f.write(\"def rebuild_all() -> None:\\n\") # Ensure annotation is present\n",
    "            f.write(\"    package_name = __name__\\n\")\n",
    "            f.write(\"    package = importlib.import_module(package_name)\\n\")\n",
    "            f.write(\"    rebuilt_models = set()\\n\")\n",
    "            f.write('    logger.debug(f\"Attempting rebuild in {package_name}\")\\n\\n')\n",
    "            f.write(\"    for _, module_name_part, _ in pkgutil.iter_modules(package.__path__, package_name + '.') :\\n\")\n",
    "            f.write(\"        # Skip rebuild attempt on __init__ itself or base_types if desired\\n\")\n",
    "            f.write(\"        if module_name_part.endswith('.__init__') or module_name_part.endswith('.base_types'):\\n\")\n",
    "            f.write(\"            continue\\n\")\n",
    "            f.write(\"        try:\\n\")\n",
    "            f.write(\"            module = importlib.import_module(module_name_part)\\n\")\n",
    "            f.write(\"            for attribute_name in dir(module):\\n\")\n",
    "            f.write(\"                attribute = getattr(module, attribute_name)\\n\")\n",
    "            f.write(\"                if (isinstance(attribute, type) and\\n\")\n",
    "            f.write(\"                        issubclass(attribute, BaseModel) and\\n\") # Check issubclass safely\n",
    "            f.write(\"                        attribute is not BaseModel and\\n\")\n",
    "            f.write(\"                        hasattr(attribute, 'model_rebuild') and\\n\") # Check if it has the method\n",
    "            f.write(\"                        attribute not in rebuilt_models):\\n\")\n",
    "            f.write(\"                    try:\\n\")\n",
    "            f.write('                        logger.debug(f\"Rebuilding: {attribute.__name__}\")\\n')\n",
    "            f.write(\"                        attribute.model_rebuild(force=True)\\n\")\n",
    "            f.write(\"                        rebuilt_models.add(attribute)\\n\")\n",
    "            f.write(\"                    except Exception as e_rebuild:\\n\")\n",
    "            f.write(\"                        logger.error(f'Error rebuilding model {attribute.__name__} in {module_name_part}: {e_rebuild}', exc_info=False)\\n\")\n",
    "            f.write(\"        except ModuleNotFoundError:\\n\")\n",
    "            f.write(\"            logger.warning(f\\\"Module not found during rebuild: {module_name_part}\\\")\\n\")\n",
    "            f.write(\"        except Exception as e_import:\\n\")\n",
    "            f.write(\"             logger.error(f'Error importing module {module_name_part} during rebuild: {e_import}', exc_info=False)\\n\\n\")\n",
    "\n",
    "            f.write(\"# Run rebuild automatically on import\\n\")\n",
    "            f.write(\"try:\\n\")\n",
    "            f.write(\"    rebuild_all()\\n\")\n",
    "            f.write(\"    logger.info(f'Pydantic models in {__name__} package rebuilt.')\\n\")\n",
    "            f.write(\"except Exception as e_global:\\n\")\n",
    "            f.write(\"    logger.error(f'Global error during model rebuild: {e_global}', exc_info=True)\\n\")\n",
    "\n",
    "        logging.info(f\"Successfully generated __init__.py at {init_py_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write __init__.py: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569e0ac-4986-473f-94d3-07225a2b0b4f",
   "metadata": {},
   "source": [
    "### Execution Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44e54a96-9fe2-4c66-b0bf-c518f3ce2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Attempting to parse schema file: schema.txt (format: turtle)\n",
      "INFO:root:Successfully parsed 8904 triples.\n",
      "INFO:root:Found 628 potential schema.org classes.\n",
      "INFO:root:Found 921 potential schema.org properties.\n",
      "INFO:root:Analyzed 628 classes and 921 properties.\n",
      "INFO:root:Generated base types file: output_ontology/models/base_types.py\n",
      "INFO:root:Generating models in alphabetical order by URI.\n",
      "INFO:root:Generated 625 specific Pydantic model files in output_ontology/models\n",
      "INFO:root:Successfully generated __init__.py at output_ontology/models/__init__.py\n",
      "INFO:root:Pydantic model code generation complete. Check the 'output_ontology/models' directory.\n",
      "INFO:root:Ready for Step 5: Post-Processing & Verification.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: --- Main execution block demonstrating generation ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume schema_graph is loaded from Step 1\n",
    "    # Assume analyzed_schema is created from Step 2\n",
    "    schema_graph = parse_schema_to_graph(SCHEMA_FILE, SCHEMA_FORMAT)\n",
    "    if schema_graph:\n",
    "        analyzed_schema = analyze_schema_graph(schema_graph)\n",
    "        if analyzed_schema and analyzed_schema.get(\"classes\"):\n",
    "            # Step 4: Generate the Pydantic models\n",
    "            generate_ontology_models(analyzed_schema, OUTPUT_DIR, MODELS_SUBDIR)\n",
    "            logging.info(f\"Pydantic model code generation complete. Check the '{OUTPUT_DIR}/{MODELS_SUBDIR}' directory.\")\n",
    "            logging.info(\"Ready for Step 5: Post-Processing & Verification.\")\n",
    "        else:\n",
    "             logging.error(\"Schema analysis did not produce class/property data.\")\n",
    "    else:\n",
    "        logging.error(\"Failed to parse schema graph for generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da93df-0ae8-465d-a42a-405007b6ca90",
   "metadata": {},
   "source": [
    "# Step 5: Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb0952-d819-4edd-ab8d-6d2633709f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: postprocess.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import logging\n",
    "import pathlib\n",
    "import shlex # Use shlex for safer command construction\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Define target directories (adjust paths as needed)\n",
    "TARGET_DIR = pathlib.Path(\"output_ontology\") # Main output directory\n",
    "MODELS_DIR = TARGET_DIR / \"models\" # Specific directory with models\n",
    "TESTS_DIR = TARGET_DIR / \"tests\"  # Directory for tests\n",
    "\n",
    "# Tool Configurations (Ideally read from pyproject.toml or similar)\n",
    "# Example: Fail bandit on medium+ severity, medium+ confidence\n",
    "BANDIT_ARGS = [\"-r\", str(MODELS_DIR), \"-ll\", \"--exit-zero\"] # Exit-zero initially to report, fail later based on output if needed\n",
    "# Example: MyPy needs strict checks\n",
    "MYPY_ARGS = [\"--strict\", str(MODELS_DIR)]\n",
    "# Example: Ruff - fix specific issues, check others\n",
    "RUFF_FIX_ARGS = [\"check\", str(TARGET_DIR), \"--fix\", \"--select\", \"F\", \"--select\", \"E\", \"--select\", \"W\", \"--select\", \"I\"] # Fix F401, E, W, I (Imports etc)\n",
    "RUFF_CHECK_ARGS = [\"check\", str(TARGET_DIR)] # Check everything else\n",
    "\n",
    "def run_command(command: list[str], cwd: str = \".\", check: bool = True, capture: bool = False) -> subprocess.CompletedProcess | None:\n",
    "    \"\"\"Runs a command, logs, checks for errors, optionally captures output.\"\"\"\n",
    "    command_str = shlex.join(command) # Safer than \" \".join\n",
    "    logging.info(f\"Running: {command_str}\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            capture_output=capture,\n",
    "            text=True,\n",
    "            check=check, # Raises CalledProcessError if return code is non-zero\n",
    "            cwd=cwd,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        if capture:\n",
    "            if result.stdout: logging.debug(f\"Stdout:\\n{result.stdout}\")\n",
    "            if result.stderr: logging.debug(f\"Stderr:\\n{result.stderr}\") # Debug level for stderr unless error occurred\n",
    "        logging.info(f\"Command succeeded: {command_str}\")\n",
    "        return result\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Command not found: {command[0]}. Is the tool installed and in PATH?\")\n",
    "        raise # Re-raise to stop the pipeline\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Command failed with exit code {e.returncode}: {command_str}\")\n",
    "        if capture: # Log captured output on error\n",
    "             if e.stdout: logging.error(f\"Failed command stdout:\\n{e.stdout}\")\n",
    "             if e.stderr: logging.error(f\"Failed command stderr:\\n{e.stderr}\")\n",
    "        raise # Re-raise to stop the pipeline\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred running {command_str}: {e}\")\n",
    "        raise # Re-raise to stop the pipeline\n",
    "\n",
    "def run_post_processing_pipeline():\n",
    "    \"\"\"\n",
    "    Executes a robust post-processing and verification pipeline\n",
    "    for the generated ontology models.\n",
    "    Stops immediately if any critical step fails.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Robust Post-Processing & Verification Pipeline ---\")\n",
    "    try:\n",
    "        # Preparation: Ensure directories and necessary files exist\n",
    "        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        (MODELS_DIR / \"py.typed\").touch()\n",
    "        TESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        (TESTS_DIR / \"__init__.py\").touch()\n",
    "        logging.info(\"Directories and marker files ensured.\")\n",
    "\n",
    "        # Stage 1: Formatting (Fail on error)\n",
    "        # Using Ruff format as an example, Black is also fine\n",
    "        logging.info(\"Stage 1: Formatting...\")\n",
    "        run_command([\"ruff\", \"format\", str(TARGET_DIR)], check=True)\n",
    "\n",
    "        # Stage 2: Linting & Auto-Fixing (Fail on error)\n",
    "        # Run fix for specific safe-to-fix categories first\n",
    "        logging.info(\"Stage 2a: Linting & Auto-Fixing (Imports, Syntax, Style)...\")\n",
    "        run_command([\"ruff\", \"check\", str(TARGET_DIR), \"--fix\", \"--select\", \"F\", \"--select\", \"E\", \"--select\", \"W\", \"--select\", \"I\", \"--exit-zero-even-if-fixed\"], check=True)\n",
    "        # Run check for remaining issues (don't auto-fix things like complexity)\n",
    "        logging.info(\"Stage 2b: Linting (Remaining Checks)...\")\n",
    "        run_command([\"ruff\", \"check\", str(TARGET_DIR)], check=True) # Fail if non-fixable errors remain\n",
    "\n",
    "        # Stage 3: Static Type Checking (Fail on error)\n",
    "        logging.info(\"Stage 3: Type Checking...\")\n",
    "        run_command([\"mypy\"] + MYPY_ARGS, check=True)\n",
    "\n",
    "        # Stage 4: Security Scanning (Fail on error - check output if needed)\n",
    "        logging.info(\"Stage 4: Security Scanning...\")\n",
    "        # Run bandit, capture output, decide later if specific findings should fail the build\n",
    "        bandit_result = run_command([\"bandit\"] + BANDIT_ARGS, check=False, capture=True)\n",
    "        # Example: Fail build only if Bandit found issues and exited non-zero (if not using --exit-zero)\n",
    "        # Or parse bandit_result.stdout (JSON format possible) for high-severity issues\n",
    "        # For now, we just log it based on its exit code (using --exit-zero means it won't fail the script here)\n",
    "        if bandit_result and bandit_result.returncode != 0:\n",
    "             logging.warning(\"Bandit found issues, but configured not to fail the build (--exit-zero). Review output.\")\n",
    "             # If not using --exit-zero, the run_command would have raised an error if issues found.\n",
    "\n",
    "        # Stage 5: Testing (Fail on error)\n",
    "        logging.info(\"Stage 5: Testing...\")\n",
    "        logging.warning(\"Ensure tests are present in \" + str(TESTS_DIR))\n",
    "        run_command([\"pytest\", str(TESTS_DIR)], check=True)\n",
    "\n",
    "        logging.info(\"--- Pipeline Completed Successfully ---\")\n",
    "\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError, Exception) as e:\n",
    "        # Error already logged by run_command\n",
    "        logging.critical(\"--- Pipeline Failed ---\")\n",
    "        sys.exit(1) # Exit with non-zero code to signal failure\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_post_processing_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-edtech)",
   "language": "python",
   "name": "edtech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
